{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Anthropic Claude Requirements Discovery\n",
    "**Discovers and validates requirement associations using Claude AI models with configurable similarity thresholds and association probability filtering.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings for Multi-LLM testing\n",
    "# Dependencies: os, sys, json, pathlib, dotenv, datetime, pandas, praxis_sentence_transformer\n",
    "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import from praxis_sentence_transformer package (installed via pip)\n",
    "from praxis_sentence_transformer.neo4j_operations import Neo4jClient\n",
    "from praxis_sentence_transformer.analysis.analyzer import RequirementsAnalyzer\n",
    "from praxis_sentence_transformer.clients.claude import ClaudeRequirementAnalyzer\n",
    "from praxis_sentence_transformer.logger import setup_logging, DebugTimer\n",
    "from praxis_sentence_transformer.visualization import RequirementsVisualizer\n",
    "\n",
    "# Set up logging\n",
    "logger = setup_logging(\"neo4j-notebook\", logging.DEBUG)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Neo4j Client Initialization\n",
    "# Purpose: Initialize Neo4j client and establish database connection for requirement data access\n",
    "# Dependencies: Neo4jClient, logger, sys\n",
    "# Breadcrumbs: Setup -> Database Connection -> Client Initialization\n",
    "\n",
    "neo4j_client = Neo4jClient()\n",
    "\n",
    "# Test connection\n",
    "if not neo4j_client.connect():\n",
    "    logger.error(\"Failed to connect to Neo4j database\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Configuration and Parameter Setup\n",
    "# Purpose: Configure model parameters and create output directories for analysis results\n",
    "# Dependencies: Path, logger\n",
    "# Breadcrumbs: Database Connection -> Configuration -> Parameter Definition\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "alpha = 0.6\n",
    "threshold = 0.4\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"results/neo4j_analysis\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Using model: {model_name}\")\n",
    "logger.info(f\"Alpha: {alpha}\")\n",
    "logger.info(f\"Threshold: {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Requirement Mappings Retrieval\n",
    "# Purpose: Retrieve all requirement mappings from Neo4j database using configured parameters\n",
    "# Dependencies: neo4j_client, json, logger\n",
    "# Breadcrumbs: Configuration -> Data Retrieval -> Mapping Extraction\n",
    "\n",
    "# Get all mappings in one go\n",
    "mappings = neo4j_client.get_all_requirement_mappings(\n",
    "    model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    alpha=0.6,\n",
    "    threshold=0.4\n",
    ")\n",
    "\n",
    "# Log summary statistics at INFO level\n",
    "logger.info(f\"Total source requirements: {len(mappings['mappings'])}\")\n",
    "logger.info(f\"Total relationships: {mappings['metadata']['total_relationships']}\")\n",
    "\n",
    "# Log detailed mapping data at DEBUG level\n",
    "for mapping in mappings['mappings'][:5]:  # First 5 for example\n",
    "    source = mapping['source']\n",
    "    targets = mapping['targets']\n",
    "    logger.info(f\"Source {source['id']} has {len(targets)} matches\")\n",
    "    # Optional: Add more detailed debug logging for each source's targets\n",
    "    for target in targets[:3]:  # Show first 3 targets for each source\n",
    "        logger.info(f\"  - Target: {target['id']}, Similarity: {target['similarity']:.3f}\")\n",
    "        \n",
    "# Create and log first 3 requirements with only first 3 targets each\n",
    "first_three = {\n",
    "    \"metadata\": mappings[\"metadata\"],\n",
    "    \"mappings\": [\n",
    "        {\n",
    "            \"source\": mapping[\"source\"],\n",
    "            \"targets\": mapping[\"targets\"][:3]  # Limit to first 3 targets\n",
    "        }\n",
    "        for mapping in mappings[\"mappings\"][:3]  # Limit to first 3 mappings\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "logger.debug(f\"\\nFirst 3 requirements structure:\\n{json.dumps(first_three, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Claude Client Setup and Mapping Processing\n",
    "# Purpose: Initialize Claude AI analyzer and process requirement mappings for association analysis\n",
    "# Dependencies: os, ClaudeRequirementAnalyzer, DebugTimer, logger\n",
    "# Breadcrumbs: Data Retrieval -> AI Analysis -> Claude Processing Pipeline\n",
    "\n",
    "# Get configuration from environment\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "claude_model = os.getenv(\"CLAUDE_3_5_MODEL\")\n",
    "min_association_prob = float(os.getenv(\"MIN_ASSOCIATION_PROBABILITY\", 0.6))\n",
    "\n",
    "# Validate environment variables\n",
    "if not anthropic_api_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
    "if not claude_model:\n",
    "    raise ValueError(\"CLAUDE_3_5_MODEL not found in environment variables\")\n",
    "\n",
    "logger.debug(\"Retrieved configuration from environment variables\")\n",
    "\n",
    "# Initialize Claude analyzer\n",
    "try:\n",
    "    claude_analyzer = ClaudeRequirementAnalyzer(\n",
    "        api_key=anthropic_api_key,\n",
    "        model_name=claude_model,\n",
    "        min_association_probability=min_association_prob\n",
    "    )\n",
    "    logger.info(f\"Successfully initialized Claude analyzer with model: {claude_model}\")\n",
    "    \n",
    "    # Log the system prompt being used\n",
    "    logger.debug(f\"Using system prompt:\\n{claude_analyzer.system_prompt}\")\n",
    "    \n",
    "    # Process mappings with Claude\n",
    "    logger.info(\"Starting mapping processing with Claude...\")\n",
    "    timer = DebugTimer(\"Processing mappings with Claude\")\n",
    "    timer.start()\n",
    "    \n",
    "    claude_requirements_results_set = claude_analyzer.process_mappings(mappings)\n",
    "    \n",
    "    timer.stop()\n",
    "    logger.info(f\"Processing completed in {timer.duration:.2f} seconds\")\n",
    "    \n",
    "    # Log summary statistics\n",
    "    logger.info(f\"Processed {claude_requirements_results_set.total_target_matches} requirement matches\")\n",
    "    logger.info(f\"Found {claude_requirements_results_set.total_associated_matches} associated matches\")\n",
    "    \n",
    "    # Show detailed debug info about some matches\n",
    "    if claude_requirements_results_set.processed_matches:\n",
    "        logger.info(\"Sample of processed matches:\")\n",
    "        for i, match in enumerate(claude_requirements_results_set.processed_matches[:2]):\n",
    "            logger.debug(f\"\"\"\n",
    "            Match {i+1}:\n",
    "            Source ID: {match.source_id}\n",
    "            Source Content: {match.source_content[:100]}...\n",
    "            Target ID: {match.target_id}\n",
    "            Target Content: {match.target_content[:100]}...\n",
    "            Similarity Score: {match.similarity_score:.3f}\n",
    "            Association Probability: {match.association_probability:.3f}\n",
    "            Is Associated: {match.is_associated}\n",
    "            Explanation: {match.explanation}\n",
    "            \"\"\")\n",
    "        # Print summary of results\n",
    "        logger.info(\"\\nClaude Analysis Summary:\")\n",
    "        logger.info(f\"Total source requirements processed: {claude_requirements_results_set.total_source_requirements}\")\n",
    "        logger.info(f\"Total target matches analyzed: {claude_requirements_results_set.total_target_matches}\")\n",
    "        logger.info(f\"Total associated matches found: {claude_requirements_results_set.total_associated_matches}\")\n",
    "\n",
    "        # Print breakdown by source requirement\n",
    "        logger.info(\"\\nBreakdown by source requirement:\")\n",
    "        source_counts = {}\n",
    "        associated_counts = {}\n",
    "\n",
    "        for match in claude_requirements_results_set.processed_matches:\n",
    "            source_counts[match.source_id] = source_counts.get(match.source_id, 0) + 1\n",
    "            if match.is_associated:\n",
    "                associated_counts[match.source_id] = associated_counts.get(match.source_id, 0) + 1\n",
    "\n",
    "        for source_id, count in source_counts.items():\n",
    "            associated = associated_counts.get(source_id, 0)\n",
    "            logger.info(f\"Source {source_id}:\")\n",
    "            logger.info(f\"  - Total target matches: {count}\")\n",
    "            logger.info(f\"  - Associated matches: {associated}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Claude analysis: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - LLM Requirement Traces Creation\n",
    "# Purpose: Create and store LLM requirement traces in Neo4j database for analysis tracking\n",
    "# Dependencies: neo4j_client, logger\n",
    "# Breadcrumbs: Claude Processing -> Database Storage -> Trace Creation\n",
    "\n",
    "if claude_requirements_results_set.processed_matches:\n",
    "    # Configuration\n",
    "    sentence_transformer_model = model_name\n",
    "    llm_model = claude_model  # Using the Claude model name from environment\n",
    "\n",
    "    logger.info(\"Creating LLM requirement traces...\")\n",
    "    logger.info(f\"Sentence Transformer Model: {sentence_transformer_model}\")\n",
    "    logger.info(f\"LLM Model: {llm_model}\")\n",
    "\n",
    "    # Create traces\n",
    "    success_count, failure_count = neo4j_client.create_llm_traces_from_results(\n",
    "        results_set=claude_requirements_results_set,\n",
    "        model_name=sentence_transformer_model,\n",
    "        llm_model_name=llm_model\n",
    "    )\n",
    "\n",
    "    # Log results\n",
    "    logger.info(f\"Successfully created {success_count} LLM requirement traces\")\n",
    "    if failure_count > 0:\n",
    "        logger.warning(f\"Failed to create {failure_count} traces\")\n",
    "\n",
    "    # Verify creation\n",
    "    verification_query = f\"\"\"\n",
    "    MATCH ()-[r:LLM_REQUIREMENT_TRACE]->()\n",
    "    WHERE r.llm_model_name = '{llm_model}'\n",
    "    RETURN count(r) as count\n",
    "    \"\"\"\n",
    "\n",
    "    with neo4j_client.driver.session(database=neo4j_client.database) as session:\n",
    "        result = session.run(verification_query)\n",
    "        trace_count = result.single()[\"count\"]\n",
    "        logger.info(f\"Total LLM requirement traces in database for {llm_model}: {trace_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Statistical Analysis and Metrics Calculation\n",
    "# Purpose: Perform comprehensive statistical analysis and calculate performance metrics\n",
    "# Dependencies: RequirementsAnalyzer, pandas, json, datetime, logger\n",
    "# Breadcrumbs: Trace Creation -> Performance Analysis -> Statistical Evaluation\n",
    "\n",
    "logger.info(\"Performing statistical analysis of results\")\n",
    "\n",
    "# Initialize analyzer if not already initialized\n",
    "analyzer = RequirementsAnalyzer(\n",
    "    neo4j_client=neo4j_client,\n",
    "    sentence_transformer_model=model_name,\n",
    "    llm_model_name=claude_model,\n",
    "    alpha=alpha,\n",
    "    threshold=threshold\n",
    ")\n",
    "\n",
    "# Calculate basic metrics\n",
    "metrics = analyzer.calculate_metrics()\n",
    "\n",
    "# Print detailed results\n",
    "logger.info(\"\\nDetailed Metrics:\")\n",
    "logger.info(f\"True Positives: {metrics['true_positives']}\")\n",
    "logger.info(f\"False Positives: {metrics['false_positives']}\")\n",
    "logger.info(f\"True Negatives: {metrics['true_negatives']}\")\n",
    "logger.info(f\"False Negatives: {metrics['false_negatives']}\")\n",
    "logger.info(f\"\\nPrecision: {metrics['precision']:.4f}\")\n",
    "logger.info(f\"Recall: {metrics['recall']:.4f}\")\n",
    "logger.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "logger.info(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "logger.info(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Get confusion matrix\n",
    "conf_matrix = analyzer.get_confusion_matrix()\n",
    "logger.info(\"\\nConfusion Matrix:\")\n",
    "logger.info(\"[[TN, FP]\")\n",
    "logger.info(\" [FN, TP]]\")\n",
    "logger.info(f\"\\n{conf_matrix}\")\n",
    "\n",
    "# Get detailed classification information\n",
    "logger.info(\"\\nDetailed Classification Analysis:\")\n",
    "classification_details = analyzer.get_all_classification_details()\n",
    "\n",
    "# True Positives Analysis\n",
    "tp_details = pd.DataFrame(classification_details['true_positives'])\n",
    "if not tp_details.empty:\n",
    "    logger.info(\"\\nTrue Positives Details:\")\n",
    "    logger.info(f\"Total Count: {len(tp_details)}\")\n",
    "    logger.info(\"\\nSample of True Positive matches:\")\n",
    "    logger.info(tp_details.head().to_string())\n",
    "    logger.info(f\"\\nAverage LLM Confidence: {tp_details['llm_confidence'].mean():.4f}\")\n",
    "    logger.info(f\"Average Ground Truth Confidence: {tp_details['ground_truth_confidence'].mean():.4f}\")\n",
    "\n",
    "# False Positives Analysis\n",
    "fp_details = pd.DataFrame(classification_details['false_positives'])\n",
    "if not fp_details.empty:\n",
    "    logger.info(\"\\nFalse Positives Details:\")\n",
    "    logger.info(f\"Total Count: {len(fp_details)}\")\n",
    "    logger.info(\"\\nSample of False Positive matches:\")\n",
    "    logger.info(fp_details.head().to_string())\n",
    "    logger.info(f\"\\nAverage LLM Confidence: {fp_details['llm_confidence'].mean():.4f}\")\n",
    "\n",
    "# False Negatives Analysis\n",
    "fn_details = pd.DataFrame(classification_details['false_negatives'])\n",
    "if not fn_details.empty:\n",
    "    logger.info(\"\\nFalse Negatives Details:\")\n",
    "    logger.info(f\"Total Count: {len(fn_details)}\")\n",
    "    logger.info(\"\\nSample of False Negative matches:\")\n",
    "    logger.info(fn_details.head().to_string())\n",
    "    logger.info(f\"\\nAverage Ground Truth Confidence: {fn_details['ground_truth_confidence'].mean():.4f}\")\n",
    "\n",
    "# True Negatives Analysis\n",
    "tn_details = pd.DataFrame(classification_details['true_negatives'])\n",
    "if not tn_details.empty:\n",
    "    logger.info(\"\\nTrue Negatives Details:\")\n",
    "    logger.info(f\"Total Count: {len(tn_details)}\")\n",
    "    logger.info(\"\\nSample of True Negative pairs:\")\n",
    "    logger.info(tn_details.head().to_string())\n",
    "\n",
    "# Save detailed results to file\n",
    "results_dir = Path(\"results/analysis\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_dir / f\"detailed_analysis_{timestamp}.json\"\n",
    "\n",
    "detailed_results = {\n",
    "    'metrics': metrics,\n",
    "    'confusion_matrix': conf_matrix.tolist(),\n",
    "    'classification_details': classification_details\n",
    "}\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(detailed_results, f, indent=2)\n",
    "\n",
    "logger.info(f\"\\nDetailed analysis results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Results Visualization and Plotting\n",
    "# Purpose: Create comprehensive visualizations for analysis results and performance metrics\n",
    "# Dependencies: RequirementsVisualizer, Path, datetime, pandas, matplotlib\n",
    "# Breadcrumbs: Statistical Evaluation -> Visualization -> Results Presentation\n",
    "\n",
    "# Initialize visualizer with model information\n",
    "visualizer = RequirementsVisualizer(\n",
    "    figsize=(8, 6),  # Reduced figure size\n",
    "    st_model=model_name,\n",
    "    llm_model=claude_model\n",
    ")\n",
    "\n",
    "# Get current timestamp for filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create model-specific results directory\n",
    "st_name = model_name.replace('/', '_').replace('-', '_')\n",
    "llm_name = claude_model.replace('-', '_')\n",
    "results_dir = Path(f\"results/analysis_plots/{llm_name}/{st_name}\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get metrics for Claude model\n",
    "metrics = analyzer.calculate_metrics()\n",
    "\n",
    "# Plot and display confusion matrix\n",
    "plt.figure(figsize=(8, 6))  # Controlled figure size\n",
    "conf_matrix = analyzer.get_confusion_matrix()\n",
    "visualizer.plot_confusion_matrix(\n",
    "    conf_matrix,\n",
    "    title=\"Model Confusion Matrix\",\n",
    "    save_path=str(results_dir / f\"confusion_{timestamp}.png\"),\n",
    "    dpi=100\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Create and display metrics summary table\n",
    "metrics_to_display = [\n",
    "    'precision', 'recall', 'f1_score', \n",
    "    'accuracy', 'balanced_accuracy'\n",
    "]\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': metrics_to_display,\n",
    "    'Value': [f\"{metrics[metric]:.4f}\" for metric in metrics_to_display]\n",
    "})\n",
    "\n",
    "# Set pandas display options for better formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the metrics table\n",
    "display(metrics_df)\n",
    "\n",
    "# Log paths to saved plots\n",
    "logger.info(f\"\\nPlots saved to:\")\n",
    "logger.info(f\"- Confusion Matrix: {results_dir}/confusion_{timestamp}.png\")\n",
    "\n",
    "# Close any remaining matplotlib figures\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Cleanup and Session Termination\n",
    "# Purpose: Close database connections and finalize the analysis session\n",
    "# Dependencies: neo4j_client, logger\n",
    "# Breadcrumbs: Results Presentation -> Session Management -> Resource Cleanup\n",
    "\n",
    "neo4j_client.close()\n",
    "logger.info(\"Analysis complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
