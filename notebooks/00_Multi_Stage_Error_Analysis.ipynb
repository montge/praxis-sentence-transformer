{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Multi-Stage LLM Error Analysis\n",
    "**Calculates error rates and performance metrics across TF-IDF, Claude 2.1, and Claude 3.5 processing stages for requirements analysis pipelines.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings for Multi-LLM testing\n",
    "# Dependencies: numpy, pandas, matplotlib, seaborn, scipy, sklearn, typing, dataclasses\n",
    "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from typing import Dict, Tuple, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ProcessingStageResults:\n",
    "    \"\"\"Store and calculate metrics for each processing stage\"\"\"\n",
    "    tp: int  # True Positives\n",
    "    fp: int  # False Positives\n",
    "    tn: int  # True Negatives\n",
    "    fn: int  # False Negatives\n",
    "    name: str  # Stage identifier\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        return self.tp / (self.tp + self.fp)\n",
    "\n",
    "    @property\n",
    "    def recall(self) -> float:\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "\n",
    "    @property\n",
    "    def fnr(self) -> float:\n",
    "        \"\"\"Calculate False Negative Rate\"\"\"\n",
    "        return self.fn / (self.fn + self.tp)\n",
    "\n",
    "    @property\n",
    "    def fpr(self) -> float:\n",
    "        \"\"\"Calculate False Positive Rate\"\"\"\n",
    "        return self.fp / (self.fp + self.tn)\n",
    "\n",
    "class MultiStageLLMAnalysis:\n",
    "    def __init__(self, \n",
    "                 tfidf_results: Dict[str, int],\n",
    "                 claude21_results: Dict[str, int],\n",
    "                 claude35_results: Dict[str, int]):\n",
    "        \"\"\"\n",
    "        Initialize analysis with results from all processing stages.\n",
    "        \n",
    "        Args:\n",
    "            tfidf_results: Results from TF-IDF + Transformer stage\n",
    "            claude21_results: Results from Claude 2.1\n",
    "            claude35_results: Results from Claude 3.5\n",
    "        \"\"\"\n",
    "        # Initialize each stage with its results\n",
    "        self.stages = {\n",
    "            'tfidf': ProcessingStageResults(**tfidf_results, name='TF-IDF + Transformer'),\n",
    "            'claude21': ProcessingStageResults(**claude21_results, name='Claude 2.1'),\n",
    "            'claude35': ProcessingStageResults(**claude35_results, name='Claude 3.5')\n",
    "        }\n",
    "        \n",
    "        # Calculate combined error rates for different pipelines\n",
    "        self.calculate_combined_metrics()\n",
    "\n",
    "    def calculate_combined_metrics(self):\n",
    "        \"\"\"Calculate error rates for different processing pipelines\"\"\"\n",
    "        # Calculate combined FNR for TF-IDF → Claude 2.1\n",
    "        self.fnr_combined_21 = 1 - (1 - self.stages['tfidf'].fnr) * (1 - self.stages['claude21'].fnr)\n",
    "        \n",
    "        # Calculate combined FNR for TF-IDF → Claude 3.5\n",
    "        self.fnr_combined_35 = 1 - (1 - self.stages['tfidf'].fnr) * (1 - self.stages['claude35'].fnr)\n",
    "\n",
    "    def calculate_confidence_interval(self, \n",
    "                                   stage_key: str,\n",
    "                                   confidence: float = 0.95) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate confidence interval for success rate of a specific stage.\n",
    "        \n",
    "        Args:\n",
    "            stage_key: Key identifying the processing stage\n",
    "            confidence: Confidence level (default: 0.95)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (lower_bound, upper_bound)\n",
    "        \"\"\"\n",
    "        stage = self.stages[stage_key]\n",
    "        z_score = stats.norm.ppf((1 + confidence) / 2)\n",
    "        p_hat = stage.tp / (stage.tp + stage.fn)\n",
    "        n = stage.tp + stage.fn\n",
    "        \n",
    "        margin_of_error = z_score * np.sqrt((p_hat * (1 - p_hat)) / n)\n",
    "        return (p_hat - margin_of_error, p_hat + margin_of_error)\n",
    "\n",
    "    def plot_comparative_metrics(self):\n",
    "        \"\"\"Create a comprehensive comparison of metrics across all stages\"\"\"\n",
    "        metrics = {\n",
    "            'Precision': [stage.precision for stage in self.stages.values()],\n",
    "            'Recall': [stage.recall for stage in self.stages.values()],\n",
    "            'FNR': [stage.fnr for stage in self.stages.values()],\n",
    "            'FPR': [stage.fpr for stage in self.stages.values()]\n",
    "        }\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot precision and recall\n",
    "        x = np.arange(len(self.stages))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0].bar(x - width/2, metrics['Precision'], width, label='Precision')\n",
    "        axes[0].bar(x + width/2, metrics['Recall'], width, label='Recall')\n",
    "        axes[0].set_ylabel('Rate')\n",
    "        axes[0].set_title('Precision and Recall by Processing Stage')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels([stage.name for stage in self.stages.values()])\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Plot error rates\n",
    "        axes[1].bar(x - width/2, metrics['FNR'], width, label='False Negative Rate')\n",
    "        axes[1].bar(x + width/2, metrics['FPR'], width, label='False Positive Rate')\n",
    "        axes[1].set_ylabel('Rate')\n",
    "        axes[1].set_title('Error Rates by Processing Stage')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels([stage.name for stage in self.stages.values()])\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def plot_pipeline_comparison(self):\n",
    "        \"\"\"Compare the performance of different processing pipelines\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        pipeline_data = {\n",
    "            'TF-IDF → Claude 2.1': self.fnr_combined_21,\n",
    "            'TF-IDF → Claude 3.5': self.fnr_combined_35\n",
    "        }\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar(pipeline_data.keys(), pipeline_data.values())\n",
    "        ax.set_ylabel('Combined False Negative Rate')\n",
    "        ax.set_title('Pipeline Performance Comparison')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.1%}',\n",
    "                   ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def calculate_buffer_requirements(self, \n",
    "                                   cost_ratio: float,\n",
    "                                   confidence: float = 0.95) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate required buffers for different pipeline configurations.\n",
    "        \n",
    "        Args:\n",
    "            cost_ratio: Cost ratio of missing requirement vs review\n",
    "            confidence: Confidence level\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of pipeline names and their required buffers\n",
    "        \"\"\"\n",
    "        buffers = {}\n",
    "        \n",
    "        # Calculate confidence intervals for each LLM stage\n",
    "        _, margin_21 = self.calculate_confidence_interval('claude21', confidence)\n",
    "        _, margin_35 = self.calculate_confidence_interval('claude35', confidence)\n",
    "        \n",
    "        # Calculate buffers for each pipeline\n",
    "        buffers['TF-IDF → Claude 2.1'] = self.fnr_combined_21 * cost_ratio * (1 + margin_21)\n",
    "        buffers['TF-IDF → Claude 3.5'] = self.fnr_combined_35 * cost_ratio * (1 + margin_35)\n",
    "        \n",
    "        return buffers\n",
    "\n",
    "def main():\n",
    "    # Sample data for all stages\n",
    "    tfidf_results = {\n",
    "        'tp': 318,\n",
    "        'fp': 10156,\n",
    "        'tn': 41183,\n",
    "        'fn': 43\n",
    "    }\n",
    "    \n",
    "    claude21_results = {\n",
    "        'tp': 108,\n",
    "        'fp': 443,\n",
    "        'tn': 50896,\n",
    "        'fn': 253\n",
    "    }\n",
    "    \n",
    "    claude35_results = {\n",
    "        'tp': 152,\n",
    "        'fp': 566,\n",
    "        'tn': 50773,\n",
    "        'fn': 209\n",
    "    }\n",
    "    \n",
    "    # Initialize analysis\n",
    "    analysis = MultiStageLLMAnalysis(tfidf_results, claude21_results, claude35_results)\n",
    "    \n",
    "    # Generate comparative visualizations\n",
    "    metrics_fig = analysis.plot_comparative_metrics()\n",
    "    metrics_fig.savefig('stage_comparison.png')\n",
    "    \n",
    "    pipeline_fig = analysis.plot_pipeline_comparison()\n",
    "    pipeline_fig.savefig('pipeline_comparison.png')\n",
    "    \n",
    "    # Calculate and print buffer requirements for different cost ratios\n",
    "    cost_ratios = [5, 10, 15, 20]\n",
    "    print(\"\\nBuffer Requirements at Different Cost Ratios:\")\n",
    "    print(\"-\" * 50)\n",
    "    for ratio in cost_ratios:\n",
    "        buffers = analysis.calculate_buffer_requirements(ratio)\n",
    "        print(f\"\\nCost Ratio: {ratio}x\")\n",
    "        for pipeline, buffer in buffers.items():\n",
    "            print(f\"{pipeline}: {buffer:.1%}\")\n",
    "\n",
    "    # Print detailed metrics for each stage\n",
    "    print(\"\\nDetailed Stage Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, stage in analysis.stages.items():\n",
    "        print(f\"\\n{stage.name}:\")\n",
    "        print(f\"Precision: {stage.precision:.3f}\")\n",
    "        print(f\"Recall: {stage.recall:.3f}\")\n",
    "        print(f\"False Negative Rate: {stage.fnr:.3f}\")\n",
    "        print(f\"False Positive Rate: {stage.fpr:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
