{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Sentence Transformer Requirements Matching\n",
    "**Processes requirement documents using sentence transformer models to identify and score semantic similarities between source and target requirements.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings for Multi-LLM testing\n",
    "# Dependencies: os, sys, logging, pathlib, xml, collections, typing, torch, matplotlib, sentence_transformers, tqdm, dotenv, praxis_sentence_transformer\n",
    "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import from the praxis-sentence-transformer package (installed via pip)\n",
    "try:\n",
    "    from praxis_sentence_transformer import (\n",
    "        setup_logging, \n",
    "        handle_exception, \n",
    "        DebugTimer\n",
    "    )\n",
    "except ImportError as e:\n",
    "    print(f\"Failed to import praxis_sentence_transformer: {str(e)}\")\n",
    "    print(\"Please install the package using pip install praxis-sentence-transformer\")\n",
    "    raise\n",
    "\n",
    "# Set up logging\n",
    "logger = setup_logging(\"sentence-transformer-notebook\", logging.DEBUG)\n",
    "\n",
    "class RequirementsLoader:\n",
    "    \"\"\"Handles loading and parsing of requirements from XML files\"\"\"\n",
    "    \n",
    "    @handle_exception\n",
    "    def parse_requirements(self, file_path: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Parse requirements from XML file\n",
    "        \n",
    "        Parameters:\n",
    "            file_path (str): Path to the requirements XML file\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[str, str]]: List of tuples containing requirement IDs and descriptions\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Parsing requirements from {file_path}\")\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        requirements = []\n",
    "        \n",
    "        for artifact in root.findall('.//artifact'):\n",
    "            req_id = artifact.find('id')\n",
    "            req_desc = artifact.find('content')\n",
    "            \n",
    "            if req_id is not None and req_desc is not None:\n",
    "                requirements.append((req_id.text.strip(), req_desc.text.strip()))\n",
    "                \n",
    "        logger.info(f\"Successfully parsed {len(requirements)} requirements\")\n",
    "        return requirements\n",
    "    \n",
    "    @handle_exception\n",
    "    def parse_answer_set(self, file_path: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Parse the answer set from XML file\n",
    "        \n",
    "        Parameters:\n",
    "            file_path (str): Path to the answer set XML file\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[str, str]]: List of tuples containing source and target requirement IDs\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Parsing answer set from {file_path}\")\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        mappings = []\n",
    "        \n",
    "        for link in root.findall('.//link'):\n",
    "            source = link.find('source_artifact_id')\n",
    "            target = link.find('target_artifact_id')\n",
    "            \n",
    "            if source is not None and target is not None:\n",
    "                mappings.append((source.text.strip(), target.text.strip()))\n",
    "        \n",
    "        logger.info(f\"Successfully parsed {len(mappings)} reference mappings\")\n",
    "        return mappings\n",
    "\n",
    "class SentenceTransformerAnalyzer:\n",
    "    \"\"\"Analyzes requirements using sentence transformers\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with a specific model\n",
    "        \n",
    "        Parameters:\n",
    "            model_name (str): Name of the sentence transformer model to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.loader = RequirementsLoader()\n",
    "        \n",
    "    @handle_exception\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the sentence transformer model\"\"\"\n",
    "        logger.info(f\"Initializing sentence transformer model: {self.model_name}\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.to(torch.device(\"cuda\"))\n",
    "            logger.info(\"Model moved to CUDA\")\n",
    "    \n",
    "    @handle_exception\n",
    "    def analyze_requirements(self, \n",
    "                           source_file: str,\n",
    "                           target_file: str,\n",
    "                           threshold: float = 0.5) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        \"\"\"\n",
    "        Analyze requirements and find similarities using mean pooling and cosine similarity\n",
    "        \n",
    "        Parameters:\n",
    "            source_file (str): Path to source requirements XML file\n",
    "            target_file (str): Path to target requirements XML file\n",
    "            threshold (float): Similarity threshold for matching (default: 0.5)\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, List[Tuple[str, float]]]: Dictionary mapping source IDs to list of (target_id, similarity_score)\n",
    "        \"\"\"\n",
    "        with DebugTimer(logger, \"Requirements Analysis\"):\n",
    "            source_reqs = self.loader.parse_requirements(source_file)\n",
    "            target_reqs = self.loader.parse_requirements(target_file)\n",
    "            \n",
    "            logger.info(f\"Analyzing {len(source_reqs)} source and {len(target_reqs)} target requirements\")\n",
    "            \n",
    "            source_texts = [desc for _, desc in source_reqs]\n",
    "            target_texts = [desc for _, desc in target_reqs]\n",
    "            \n",
    "            with DebugTimer(logger, \"Encoding source texts\"):\n",
    "                source_embeddings = self.model.encode(\n",
    "                    source_texts, \n",
    "                    convert_to_tensor=True,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "            \n",
    "            with DebugTimer(logger, \"Encoding target texts\"):\n",
    "                target_embeddings = self.model.encode(\n",
    "                    target_texts, \n",
    "                    convert_to_tensor=True,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "            \n",
    "            similarities = util.pytorch_cos_sim(source_embeddings, target_embeddings)\n",
    "            \n",
    "            mappings = defaultdict(list)\n",
    "            for i, (source_id, _) in enumerate(source_reqs):\n",
    "                for j, (target_id, _) in enumerate(target_reqs):\n",
    "                    similarity = similarities[i][j].item()\n",
    "                    if similarity >= threshold:\n",
    "                        mappings[source_id].append((target_id, similarity))\n",
    "                \n",
    "                if source_id in mappings:\n",
    "                    mappings[source_id] = sorted(mappings[source_id], key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            logger.info(f\"Found {len(mappings)} source requirements with matches above threshold {threshold}\")\n",
    "            return dict(mappings)\n",
    "\n",
    "    @handle_exception\n",
    "    def evaluate_results(self, \n",
    "                        calculated_mapping: Dict[str, List[Tuple[str, float]]], \n",
    "                        answer_set_file: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the calculated mappings against the reference answer set and generate confusion matrix\n",
    "        \n",
    "        Parameters:\n",
    "            calculated_mapping (Dict[str, List[Tuple[str, float]]]): Calculated requirement mappings\n",
    "            answer_set_file (str): Path to the answer set XML file\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        reference_mappings = set(self.loader.parse_answer_set(answer_set_file))\n",
    "        \n",
    "        calculated_set = set()\n",
    "        for source_id, matches in calculated_mapping.items():\n",
    "            for target_id, _ in matches:\n",
    "                calculated_set.add((source_id, target_id))\n",
    "        \n",
    "        true_positives = len(reference_mappings.intersection(calculated_set))\n",
    "        false_positives = len(calculated_set - reference_mappings)\n",
    "        false_negatives = len(reference_mappings - calculated_set)\n",
    "        \n",
    "        all_source_ids = {pair[0] for pair in reference_mappings.union(calculated_set)}\n",
    "        all_target_ids = {pair[1] for pair in reference_mappings.union(calculated_set)}\n",
    "        all_possible_pairs = {(s, t) for s in all_source_ids for t in all_target_ids}\n",
    "        true_negatives = len(all_possible_pairs - reference_mappings - calculated_set)\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "        balanced_accuracy = (\n",
    "            true_positives / (true_positives + false_negatives) +\n",
    "            true_negatives / (true_negatives + false_positives)\n",
    "        ) / 2\n",
    "        \n",
    "        confusion_matrix = f\"\"\"\n",
    "Confusion Matrix:\n",
    "                 Predicted Positive | Predicted Negative\n",
    "Actual Positive |        {true_positives:^10d} |       {false_negatives:^10d}\n",
    "Actual Negative |        {false_positives:^10d} |       {true_negatives:^10d}\n",
    "\"\"\"\n",
    "        \n",
    "        results = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'accuracy': accuracy,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'confusion_matrix': confusion_matrix,\n",
    "            'true_positives': true_positives,\n",
    "            'false_positives': false_positives,\n",
    "            'true_negatives': true_negatives,\n",
    "            'false_negatives': false_negatives\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Evaluation Results: {results}\")\n",
    "        logger.info(confusion_matrix)\n",
    "        return results\n",
    "\n",
    "    @handle_exception\n",
    "    def find_optimal_threshold(self, \n",
    "                             source_file: str,\n",
    "                             target_file: str,\n",
    "                             answer_set_file: str,\n",
    "                             threshold_range: List[float] = None) -> Dict[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Find optimal threshold by testing different values\n",
    "        \n",
    "        Parameters:\n",
    "            source_file (str): Path to source requirements file\n",
    "            target_file (str): Path to target requirements file\n",
    "            answer_set_file (str): Path to answer set file\n",
    "            threshold_range (List[float]): List of thresholds to test (default: [0.05 to 0.6 by 0.05])\n",
    "            \n",
    "        Returns:\n",
    "            Dict[float, Dict[str, float]]: Dictionary mapping thresholds to their evaluation metrics\n",
    "        \"\"\"\n",
    "        if threshold_range is None:\n",
    "            threshold_range = [round(x * 0.05, 2) for x in range(1, 13)]  # 0.05 to 0.60\n",
    "            \n",
    "        results = {}\n",
    "        for threshold in threshold_range:\n",
    "            logger.info(f\"Testing threshold: {threshold}\")\n",
    "            mapping = self.analyze_requirements(source_file, target_file, threshold=threshold)\n",
    "            evaluation = self.evaluate_results(mapping, answer_set_file)\n",
    "            \n",
    "            fnr = evaluation['false_negatives'] / (evaluation['true_positives'] + evaluation['false_negatives'])\n",
    "            evaluation['false_negative_rate'] = fnr\n",
    "            \n",
    "            results[threshold] = evaluation\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def plot_threshold_metrics(self, threshold_results: Dict[float, Dict[str, float]], save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot threshold analysis metrics\n",
    "        \n",
    "        Parameters:\n",
    "            threshold_results (Dict[float, Dict[str, float]]): Results from threshold analysis\n",
    "            save_path (str): Optional path to save the plots\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        from datetime import datetime\n",
    "        import os\n",
    "        \n",
    "        # Create timestamp and format save path\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if save_path:\n",
    "            # Extract model name from the full path\n",
    "            model_name = self.model_name.split('/')[-1]\n",
    "            # Create directory structure: results/model_name/timestamp/\n",
    "            save_dir = os.path.join('results', model_name, timestamp)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, 'threshold_analysis.png')\n",
    "\n",
    "        # Rest of the plotting code remains the same\n",
    "        thresholds = sorted(threshold_results.keys())\n",
    "        metrics = {\n",
    "            'precision': [threshold_results[t]['precision'] for t in thresholds],\n",
    "            'recall': [threshold_results[t]['recall'] for t in thresholds],\n",
    "            'f1_score': [threshold_results[t]['f1_score'] for t in thresholds],\n",
    "            'accuracy': [(threshold_results[t]['true_positives'] + threshold_results[t]['true_negatives']) / \n",
    "                        (threshold_results[t]['true_positives'] + threshold_results[t]['true_negatives'] + \n",
    "                        threshold_results[t]['false_positives'] + threshold_results[t]['false_negatives']) \n",
    "                        for t in thresholds],\n",
    "            'balanced_accuracy': [\n",
    "                (threshold_results[t]['true_positives'] / (threshold_results[t]['true_positives'] + threshold_results[t]['false_negatives']) +\n",
    "                threshold_results[t]['true_negatives'] / (threshold_results[t]['true_negatives'] + threshold_results[t]['false_positives'])) / 2\n",
    "                for t in thresholds\n",
    "            ],\n",
    "            'false_negative_rate': [threshold_results[t]['false_negative_rate'] for t in thresholds]\n",
    "        }\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 18))\n",
    "        \n",
    "        # Plot 1: Precision-Recall vs Threshold\n",
    "        ax1.plot(thresholds, metrics['precision'], 'b-', label='Precision')\n",
    "        ax1.plot(thresholds, metrics['recall'], 'r-', label='Recall')\n",
    "        ax1.set_xlabel('Threshold')\n",
    "        ax1.set_ylabel('Score')\n",
    "        ax1.set_title('Precision and Recall vs Threshold')\n",
    "        ax1.grid(True)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot 2: All Metrics vs Threshold\n",
    "        for metric_name in ['precision', 'recall', 'f1_score', 'accuracy', 'balanced_accuracy']:\n",
    "            ax2.plot(thresholds, metrics[metric_name], label=metric_name.replace('_', ' ').title())\n",
    "        ax2.set_xlabel('Threshold')\n",
    "        ax2.set_ylabel('Score')\n",
    "        ax2.set_title('All Metrics vs Threshold')\n",
    "        ax2.grid(True)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Plot 3: False Negative Rate vs Threshold\n",
    "        ax3.plot(thresholds, metrics['false_negative_rate'], 'r-', label='False Negative Rate')\n",
    "        ax3.set_xlabel('Threshold')\n",
    "        ax3.set_ylabel('Rate')\n",
    "        ax3.set_title('False Negative Rate vs Threshold')\n",
    "        ax3.grid(True)\n",
    "        ax3.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            logger.info(f\"Plots saved to {save_path}\")\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    @handle_exception\n",
    "    def analyze_requirements_bidirectional(self,\n",
    "                                         source_file: str,\n",
    "                                         target_file: str,\n",
    "                                         threshold: float = 0.5) -> Tuple[Dict[str, List[Tuple[str, float]]], Dict[str, List[Tuple[str, float]]]]:\n",
    "        \"\"\"\n",
    "        Analyze requirements in both directions (source->target and target->source)\n",
    "        \n",
    "        Parameters:\n",
    "            source_file (str): Path to source requirements XML file\n",
    "            target_file (str): Path to target requirements XML file\n",
    "            threshold (float): Similarity threshold for matching\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: (source_to_target_mappings, target_to_source_mappings)\n",
    "        \"\"\"\n",
    "        with DebugTimer(logger, \"Bidirectional Requirements Analysis\"):\n",
    "            source_reqs = self.loader.parse_requirements(source_file)\n",
    "            target_reqs = self.loader.parse_requirements(target_file)\n",
    "            \n",
    "            logger.info(f\"Analyzing {len(source_reqs)} source and {len(target_reqs)} target requirements bidirectionally\")\n",
    "            \n",
    "            source_texts = [desc for _, desc in source_reqs]\n",
    "            target_texts = [desc for _, desc in target_reqs]\n",
    "            \n",
    "            with DebugTimer(logger, \"Encoding texts\"):\n",
    "                source_embeddings = self.model.encode(\n",
    "                    source_texts, \n",
    "                    convert_to_tensor=True,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "                target_embeddings = self.model.encode(\n",
    "                    target_texts, \n",
    "                    convert_to_tensor=True,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "            \n",
    "            similarities = util.pytorch_cos_sim(source_embeddings, target_embeddings)\n",
    "            \n",
    "            # Source to target mappings\n",
    "            source_to_target = defaultdict(list)\n",
    "            for i, (source_id, _) in enumerate(source_reqs):\n",
    "                for j, (target_id, _) in enumerate(target_reqs):\n",
    "                    similarity = similarities[i][j].item()\n",
    "                    if similarity >= threshold:\n",
    "                        source_to_target[source_id].append((target_id, similarity))\n",
    "                \n",
    "                if source_id in source_to_target:\n",
    "                    source_to_target[source_id] = sorted(source_to_target[source_id], key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Target to source mappings\n",
    "            target_to_source = defaultdict(list)\n",
    "            for j, (target_id, _) in enumerate(target_reqs):\n",
    "                for i, (source_id, _) in enumerate(source_reqs):\n",
    "                    similarity = similarities[i][j].item()\n",
    "                    if similarity >= threshold:\n",
    "                        target_to_source[target_id].append((source_id, similarity))\n",
    "                \n",
    "                if target_id in target_to_source:\n",
    "                    target_to_source[target_id] = sorted(target_to_source[target_id], key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            logger.info(f\"Found {len(source_to_target)} source->target and {len(target_to_source)} target->source mappings above threshold {threshold}\")\n",
    "            return dict(source_to_target), dict(target_to_source)\n",
    "\n",
    "    @handle_exception\n",
    "    def find_optimal_threshold_bidirectional(self,\n",
    "                                           source_file: str,\n",
    "                                           target_file: str,\n",
    "                                           answer_set_file: str,\n",
    "                                           threshold_range: List[float] = None) -> Dict[str, Dict[float, Dict[str, float]]]:\n",
    "        \"\"\"\n",
    "        Find optimal threshold by testing different values in both directions\n",
    "        \n",
    "        Parameters:\n",
    "            source_file (str): Path to source requirements file\n",
    "            target_file (str): Path to target requirements file\n",
    "            answer_set_file (str): Path to answer set file\n",
    "            threshold_range (List[float]): List of thresholds to test\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Dict[float, Dict[str, float]]]: Dictionary with results for both directions\n",
    "        \"\"\"\n",
    "        if threshold_range is None:\n",
    "            threshold_range = [round(x * 0.05, 2) for x in range(1, 13)]  # 0.05 to 0.60\n",
    "            \n",
    "        results = {\n",
    "            'source_to_target': {},\n",
    "            'target_to_source': {}\n",
    "        }\n",
    "        \n",
    "        for threshold in threshold_range:\n",
    "            logger.info(f\"Testing threshold: {threshold}\")\n",
    "            s2t_mapping, t2s_mapping = self.analyze_requirements_bidirectional(\n",
    "                source_file, target_file, threshold=threshold\n",
    "            )\n",
    "            \n",
    "            # Evaluate source to target\n",
    "            s2t_evaluation = self.evaluate_results(s2t_mapping, answer_set_file)\n",
    "            s2t_evaluation['false_negative_rate'] = s2t_evaluation['false_negatives'] / (\n",
    "                s2t_evaluation['true_positives'] + s2t_evaluation['false_negatives']\n",
    "            )\n",
    "            results['source_to_target'][threshold] = s2t_evaluation\n",
    "            \n",
    "            # Evaluate target to source\n",
    "            t2s_evaluation = self.evaluate_results(t2s_mapping, answer_set_file)\n",
    "            t2s_evaluation['false_negative_rate'] = t2s_evaluation['false_negatives'] / (\n",
    "                t2s_evaluation['true_positives'] + t2s_evaluation['false_negatives']\n",
    "            )\n",
    "            results['target_to_source'][threshold] = t2s_evaluation\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def plot_bidirectional_metrics(self, threshold_results: Dict[str, Dict[float, Dict[str, float]]], save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot threshold analysis metrics for bidirectional analysis\n",
    "        \n",
    "        Parameters:\n",
    "            threshold_results (Dict): Results from bidirectional threshold analysis\n",
    "            save_path (str): Optional path to save the plots\n",
    "        \"\"\"\n",
    "        # Similar to existing plot_threshold_metrics but with two sets of plots\n",
    "        # Implementation details omitted for brevity - can provide if needed\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer\n",
    "    analyzer = SentenceTransformerAnalyzer()\n",
    "    analyzer.initialize()\n",
    "    \n",
    "    # File paths\n",
    "    source_file = \"datasets/CM1/CM1-sourceArtifacts.xml\"\n",
    "    target_file = \"datasets/CM1/CM1-targetArtifacts.xml\"\n",
    "    answer_set_file = \"datasets/CM1/CM1-answerSet.xml\"\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    threshold_results = analyzer.find_optimal_threshold(\n",
    "        source_file, \n",
    "        target_file, \n",
    "        answer_set_file,\n",
    "        threshold_range=[0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]\n",
    "    )\n",
    "    \n",
    "    # Create visualizations with default save path\n",
    "    analyzer.plot_threshold_metrics(threshold_results, save_path=\"threshold_analysis.png\")\n",
    "    \n",
    "    # Print results for each threshold\n",
    "    print(\"\\nThreshold Analysis:\")\n",
    "    print(\"Threshold | Precision | Recall | F1-Score | Accuracy | Bal Acc | FN Rate\")\n",
    "    print(\"-\" * 75)\n",
    "    for threshold, metrics in sorted(threshold_results.items()):\n",
    "        print(f\"{threshold:^9.2f} | {metrics['precision']:^9.3f} | {metrics['recall']:^6.3f} | \"\n",
    "              f\"{metrics['f1_score']:^8.3f} | {metrics['accuracy']:^8.3f} | \"\n",
    "              f\"{metrics['balanced_accuracy']:^7.3f} | {metrics['false_negative_rate']:^7.3f}\")\n",
    "    \n",
    "    # Find threshold with best F1 score\n",
    "    best_threshold = max(threshold_results.items(), key=lambda x: x[1]['f1_score'])[0]\n",
    "    \n",
    "    # Run final analysis with best threshold\n",
    "    print(f\"\\nRunning final analysis with best threshold: {best_threshold}\")\n",
    "    mapping = analyzer.analyze_requirements(source_file, target_file, threshold=best_threshold)\n",
    "    evaluation = analyzer.evaluate_results(mapping, answer_set_file)\n",
    "    \n",
    "    print(\"\\nBest Threshold Results:\")\n",
    "    print(f\"Precision: {evaluation['precision']:.3f}\")\n",
    "    print(f\"Recall: {evaluation['recall']:.3f}\")\n",
    "    print(f\"F1-Score: {evaluation['f1_score']:.3f}\")\n",
    "    print(f\"Accuracy: {evaluation['accuracy']:.3f}\")\n",
    "    print(f\"Balanced Accuracy: {evaluation['balanced_accuracy']:.3f}\")\n",
    "    print(evaluation['confusion_matrix'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
