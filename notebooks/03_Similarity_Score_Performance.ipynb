{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Similarity Score Performance Analysis\n",
        "**Evaluates machine learning models on similarity scores from sentence transformers to optimize requirement matching accuracy and feature selection.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [0] - Setup and Imports\n",
        "# Purpose: Import all required libraries and configure environment settings for Multi-LLM testing\n",
        "# Dependencies: os, io, sys, pathlib, dotenv, pandas, numpy, sklearn, matplotlib, seaborn, tqdm, praxis_sentence_transformer\n",
        "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
        "\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, fbeta_score, roc_auc_score, confusion_matrix,\n",
        "    roc_curve, auc, precision_recall_curve, precision_score, recall_score\n",
        ")\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Import project modules (installed via pip)\n",
        "from praxis_sentence_transformer import (\n",
        "    setup_logging,\n",
        "    handle_exception,\n",
        "    DebugTimer,\n",
        "    Neo4jClient\n",
        ")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set up logging with appropriate format\n",
        "logger = setup_logging(\"similarity-score-analysis-notebook\")\n",
        "\n",
        "try:\n",
        "    # Log the initialization of the notebook\n",
        "    logger.info(\"Initializing Similarity Score Analysis Notebook\")\n",
        "    \n",
        "    # Verify environment variables are loaded\n",
        "    required_env_vars = [\n",
        "        'NEO4J_URI', \n",
        "        'NEO4J_USER', \n",
        "        'NEO4J_PASSWORD',\n",
        "        'PROJECT_NAME'  # Add PROJECT_NAME to required variables\n",
        "    ]\n",
        "    missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
        "    \n",
        "    if missing_vars:\n",
        "        logger.error(f\"Missing required environment variables: {missing_vars}\")\n",
        "        raise EnvironmentError(f\"Missing required environment variables: {missing_vars}\")\n",
        "    else:\n",
        "        logger.debug(\"All required environment variables loaded successfully\")\n",
        "        logger.info(f\"Working with project: {os.getenv('PROJECT_NAME')}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to initialize notebook\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [1] - Neo4j Connection and Data Loading\n",
        "# Purpose: Establish database connection and load similarity data for analysis\n",
        "# Dependencies: Neo4jClient, logger, os, pandas\n",
        "# Breadcrumbs: Setup -> Database Connection -> Data Retrieval\n",
        "\n",
        "def check_schema():\n",
        "    \"\"\"Check Neo4j schema and relationship types\"\"\"\n",
        "    logger.debug(\"Initializing Neo4j schema check\")\n",
        "    try:\n",
        "        neo4j_client = Neo4jClient(\n",
        "            uri=os.getenv('NEO4J_URI'),\n",
        "            username=os.getenv('NEO4J_USER'),\n",
        "            password=os.getenv('NEO4J_PASSWORD')\n",
        "        )\n",
        "        \n",
        "        # Modified query to get schema information\n",
        "        query = \"\"\"\n",
        "        CALL apoc.meta.schema()\n",
        "        YIELD value\n",
        "        RETURN value\n",
        "        \"\"\"\n",
        "        \n",
        "        with neo4j_client.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            schema = result.single()['value']\n",
        "            \n",
        "            # Log the raw schema for debugging\n",
        "            logger.debug(f\"Raw schema structure: {schema}\")\n",
        "            \n",
        "            # Get relationship counts for current project\n",
        "            project_name = os.getenv('PROJECT_NAME')\n",
        "            count_query = \"\"\"\n",
        "            MATCH (p:Project {name: $project_name})<-[:CONTAINS]-(d:Document)-[:CONTAINS]->(r:Requirement)\n",
        "            WITH r\n",
        "            MATCH (r)-[rel:SIMILAR_TO]->(other:Requirement)\n",
        "            WHERE rel.project = $project_name\n",
        "            RETURN count(rel) as similar_count\n",
        "            \"\"\"\n",
        "            \n",
        "            similar_count = session.run(count_query, project_name=project_name).single()['similar_count']\n",
        "            \n",
        "            logger.info(\"Database Schema Statistics:\")\n",
        "            logger.info(f\"Number of SIMILAR_TO relationships in project {project_name}: {similar_count}\")\n",
        "            \n",
        "            if 'SIMILAR_TO' in schema:\n",
        "                logger.debug(f\"SIMILAR_TO properties: {schema['SIMILAR_TO']['properties'].keys()}\")\n",
        "            \n",
        "            return schema\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error checking Neo4j schema\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "    finally:\n",
        "        if 'neo4j_client' in locals():\n",
        "            neo4j_client.close()\n",
        "            logger.debug(\"Successfully closed Neo4j connection\")\n",
        "\n",
        "def get_similarity_data():\n",
        "    \"\"\"Retrieve similarity data for the current project\"\"\"\n",
        "    logger.debug(\"Initializing similarity data retrieval\")\n",
        "    try:\n",
        "        neo4j_client = Neo4jClient(\n",
        "            uri=os.getenv('NEO4J_URI'),\n",
        "            username=os.getenv('NEO4J_USER'),\n",
        "            password=os.getenv('NEO4J_PASSWORD')\n",
        "        )\n",
        "        \n",
        "        project_name = os.getenv('PROJECT_NAME')\n",
        "        query = \"\"\"\n",
        "        MATCH (r1:Requirement)-[s:SIMILAR_TO]->(r2:Requirement)\n",
        "        WHERE s.project = $project_name\n",
        "        OPTIONAL MATCH (r1)-[g:GROUND_TRUTH]->(r2)\n",
        "        WHERE g.project = $project_name\n",
        "        RETURN \n",
        "            r1.id as source_id,\n",
        "            r2.id as target_id,\n",
        "            s.similarity as similarity_score,\n",
        "            s.model as model_name,\n",
        "            CASE WHEN g IS NOT NULL THEN 1 ELSE 0 END as is_related\n",
        "        \"\"\"\n",
        "        \n",
        "        with neo4j_client.driver.session() as session:\n",
        "            result = session.run(query, project_name=project_name)\n",
        "            records = [dict(record) for record in result]\n",
        "            \n",
        "            # Create a DataFrame with all possible model scores initialized to None\n",
        "            data = pd.DataFrame(records)\n",
        "            \n",
        "            if len(data) > 0:\n",
        "                # Pivot the data to create separate columns for each model\n",
        "                model_scores = data.pivot(\n",
        "                    index=['source_id', 'target_id', 'is_related'],\n",
        "                    columns='model_name',\n",
        "                    values='similarity_score'\n",
        "                ).reset_index()\n",
        "                \n",
        "                # Rename columns to match expected format\n",
        "                model_mapping = {\n",
        "                    'model1': 'model1_score',\n",
        "                    'model2': 'model2_score',\n",
        "                    'model3': 'model3_score',\n",
        "                    'model4': 'model4_score',\n",
        "                    'model5': 'model5_score',\n",
        "                    'model6': 'model6_score',\n",
        "                    'model7': 'model7_score',\n",
        "                    'model8': 'model8_score',\n",
        "                    'tfidf': 'tfidf_score'\n",
        "                }\n",
        "                model_scores.rename(columns=model_mapping, inplace=True)\n",
        "                \n",
        "                logger.info(\"\\nDataset Statistics:\")\n",
        "                logger.info(f\"Total pairs: {len(model_scores)}\")\n",
        "                logger.info(f\"Related pairs: {model_scores['is_related'].sum()}\")\n",
        "                logger.info(f\"Unrelated pairs: {len(model_scores) - model_scores['is_related'].sum()}\")\n",
        "                \n",
        "                if model_scores.isnull().values.any():\n",
        "                    logger.warning(\"Missing values found in the dataset\")\n",
        "                else:\n",
        "                    logger.debug(\"No missing values found in the dataset\")\n",
        "                    \n",
        "                logger.info(\"\\nFirst 5 rows of dataset:\")\n",
        "                logger.info(f\"\\n{model_scores.head()}\")\n",
        "                logger.info(\"\\nLast 5 rows of dataset:\")\n",
        "                logger.info(f\"\\n{model_scores.tail()}\")\n",
        "                logger.info(\"\\nDataset Info:\")\n",
        "                logger.info(f\"\\n{model_scores.info()}\")\n",
        "                \n",
        "                return model_scores\n",
        "            else:\n",
        "                logger.warning(\"No data found for the specified project\")\n",
        "                return pd.DataFrame()\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error retrieving similarity data\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "    finally:\n",
        "        if 'neo4j_client' in locals():\n",
        "            neo4j_client.close()\n",
        "            logger.debug(\"Successfully closed Neo4j connection\")\n",
        "\n",
        "try:\n",
        "    schema = check_schema()\n",
        "    similarity_data = get_similarity_data()\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to initialize data\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [2] - Data Preparation and Model Training\n",
        "# Purpose: Prepare balanced dataset and train Random Forest classifier for similarity prediction\n",
        "# Dependencies: pandas, sklearn, logger, os\n",
        "# Breadcrumbs: Data Retrieval -> Data Preparation -> Model Training\n",
        "\n",
        "def prepare_and_train_model(data):\n",
        "    \"\"\"Prepare balanced dataset and train Random Forest model\"\"\"\n",
        "    logger.info(f\"Starting data preparation and model training for project: {os.getenv('PROJECT_NAME')}\")\n",
        "    try:\n",
        "        # Prepare balanced dataset\n",
        "        logger.debug(\"Preparing balanced dataset\")\n",
        "        positive_samples = data[data['is_related'] == 1]\n",
        "        negative_samples = data[data['is_related'] == 0].sample(n=len(positive_samples), random_state=42)\n",
        "        balanced_data = pd.concat([positive_samples, negative_samples])\n",
        "        \n",
        "        logger.debug(f\"Created balanced dataset with {len(balanced_data)} total samples\")\n",
        "        \n",
        "        # Prepare features and target\n",
        "        feature_columns = [col for col in balanced_data.columns \n",
        "                         if col not in ['source_id', 'target_id', 'is_related']]\n",
        "        X = balanced_data[feature_columns]\n",
        "        y = balanced_data['is_related']\n",
        "        \n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "        logger.debug(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
        "        \n",
        "        # Initialize and train model\n",
        "        logger.debug(\"Initializing RandomForestClassifier\")\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=None,\n",
        "            min_samples_split=2,\n",
        "            min_samples_leaf=1,\n",
        "            random_state=42\n",
        "        )\n",
        "        \n",
        "        logger.debug(\"Training RandomForestClassifier\")\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Evaluate model\n",
        "        logger.debug(\"Evaluating model performance\")\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        classification_rep = classification_report(y_test, y_pred)\n",
        "        f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred)\n",
        "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "        \n",
        "        # Log results\n",
        "        logger.info(\"\\nClassification Report:\")\n",
        "        logger.info(f\"\\n{classification_rep}\")\n",
        "        logger.info(f\"F2 Score: {f2:.3f}\")\n",
        "        logger.info(f\"ROC AUC Score: {f2:.3f}\")\n",
        "        logger.info(\"Confusion Matrix:\")\n",
        "        logger.info(f\"\\n{conf_matrix}\")\n",
        "        \n",
        "        # Feature importance analysis\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': feature_columns,\n",
        "            'importance': model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        logger.info(\"\\nFeature Importance:\")\n",
        "        logger.info(f\"\\n{feature_importance}\")\n",
        "        \n",
        "        return model, feature_importance, X_test, y_test, y_pred\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error in model preparation and training\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    model, feature_importance, X_test, y_test, y_pred = prepare_and_train_model(similarity_data)\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to prepare and train model\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [3] - Visualization Functions\n",
        "# Purpose: Create comprehensive visualizations for model performance analysis and results interpretation\n",
        "# Dependencies: matplotlib, seaborn, sklearn.metrics, numpy, pandas, datetime\n",
        "# Breadcrumbs: Model Training -> Performance Analysis -> Visualization Generation\n",
        "\n",
        "def create_visualizations(model, feature_importance, X_test, y_test, y_pred):\n",
        "    \"\"\"Create and save visualizations for model analysis\"\"\"\n",
        "    project_name = os.getenv('PROJECT_NAME')\n",
        "    logger.info(f\"Creating visualizations for project: {project_name}\")\n",
        "    \n",
        "    try:\n",
        "        # Set up the figure with subplots\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "        fig.suptitle(f'Model Analysis Visualizations - Project: {project_name}', fontsize=16)\n",
        "        \n",
        "        # 1. Confusion Matrix Heatmap\n",
        "        logger.debug(\"Creating confusion matrix heatmap\")\n",
        "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(\n",
        "            conf_matrix, \n",
        "            annot=True, \n",
        "            fmt='d', \n",
        "            cmap='Blues',\n",
        "            xticklabels=['Not Related', 'Related'],\n",
        "            yticklabels=['Not Related', 'Related'],\n",
        "            ax=ax1\n",
        "        )\n",
        "        ax1.set_title('Confusion Matrix')\n",
        "        ax1.set_xlabel('Predicted')\n",
        "        ax1.set_ylabel('Actual')\n",
        "        \n",
        "        # 2. Feature Importance Plot\n",
        "        logger.debug(\"Creating feature importance plot\")\n",
        "        feature_importance_plot = feature_importance.plot(\n",
        "            kind='barh',\n",
        "            x='feature',\n",
        "            y='importance',\n",
        "            ax=ax2,\n",
        "            color='skyblue'\n",
        "        )\n",
        "        ax2.set_title('Feature Importance')\n",
        "        ax2.set_xlabel('Importance Score')\n",
        "        plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
        "        \n",
        "        # 3. ROC Curve\n",
        "        logger.debug(\"Creating ROC curve\")\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        ax3.plot(\n",
        "            fpr, \n",
        "            tpr, \n",
        "            color='darkorange',\n",
        "            lw=2, \n",
        "            label=f'ROC curve (AUC = {roc_auc:.2f})'\n",
        "        )\n",
        "        ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        ax3.set_xlim([0.0, 1.0])\n",
        "        ax3.set_ylim([0.0, 1.05])\n",
        "        ax3.set_xlabel('False Positive Rate')\n",
        "        ax3.set_ylabel('True Positive Rate')\n",
        "        ax3.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
        "        ax3.legend(loc=\"lower right\")\n",
        "        \n",
        "        # 4. Precision-Recall Curve\n",
        "        logger.debug(\"Creating precision-recall curve\")\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "        pr_auc = auc(recall, precision)\n",
        "        \n",
        "        ax4.plot(\n",
        "            recall, \n",
        "            precision, \n",
        "            color='green',\n",
        "            lw=2, \n",
        "            label=f'PR curve (AUC = {pr_auc:.2f})'\n",
        "        )\n",
        "        ax4.set_xlim([0.0, 1.0])\n",
        "        ax4.set_ylim([0.0, 1.05])\n",
        "        ax4.set_xlabel('Recall')\n",
        "        ax4.set_ylabel('Precision')\n",
        "        ax4.set_title('Precision-Recall Curve')\n",
        "        ax4.legend(loc=\"lower left\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save the plot\n",
        "        try:\n",
        "            plot_filename = f\"model_analysis_{project_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "            plt.savefig(plot_filename)\n",
        "            logger.info(f\"Saved visualization plot to {plot_filename}\")\n",
        "        except Exception as save_error:\n",
        "            logger.warning(f\"Could not save plot to file: {str(save_error)}\")\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "        # Additional Analysis: Model Performance at Different Thresholds\n",
        "        logger.debug(\"Calculating threshold metrics\")\n",
        "        thresholds = np.arange(0.1, 1.0, 0.1)\n",
        "        threshold_metrics = []\n",
        "        \n",
        "        for threshold in thresholds:\n",
        "            y_pred_threshold = (y_prob >= threshold).astype(int)\n",
        "            f2 = fbeta_score(y_test, y_pred_threshold, beta=2)\n",
        "            precision = precision_score(y_test, y_pred_threshold)\n",
        "            recall = recall_score(y_test, y_pred_threshold)\n",
        "            \n",
        "            threshold_metrics.append({\n",
        "                'threshold': threshold,\n",
        "                'f2_score': f2,\n",
        "                'precision': precision,\n",
        "                'recall': recall\n",
        "            })\n",
        "        \n",
        "        threshold_df = pd.DataFrame(threshold_metrics)\n",
        "        logger.info(\"\\nThreshold Analysis:\")\n",
        "        logger.info(f\"\\n{threshold_df}\")\n",
        "        \n",
        "        # Plot threshold analysis\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        threshold_df.plot(x='threshold', y=['f2_score', 'precision', 'recall'], ax=ax)\n",
        "        plt.title(f'Model Metrics vs Threshold - Project: {project_name}')\n",
        "        plt.xlabel('Threshold')\n",
        "        plt.ylabel('Score')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        \n",
        "        # Save the threshold analysis plot\n",
        "        try:\n",
        "            threshold_plot_filename = f\"threshold_analysis_{project_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "            plt.savefig(threshold_plot_filename)\n",
        "            logger.info(f\"Saved threshold analysis plot to {threshold_plot_filename}\")\n",
        "        except Exception as save_error:\n",
        "            logger.warning(f\"Could not save threshold analysis plot to file: {str(save_error)}\")\n",
        "            \n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating visualizations\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    create_visualizations(model, feature_importance, X_test, y_test, y_pred)\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to create visualizations\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [4] - Dataset Distribution Analysis\n",
        "# Purpose: Analyze distribution of related and unrelated requirement pairs for understanding data balance\n",
        "# Dependencies: pandas, logger\n",
        "# Breadcrumbs: Visualization Generation -> Dataset Analysis -> Distribution Statistics\n",
        "\n",
        "try:\n",
        "    # Method 1: Using sum()\n",
        "    related_count = df['is_related'].sum()\n",
        "    logger.info(\"Dataset Distribution Analysis:\")\n",
        "    logger.info(f\"Number of related pairs: {related_count}\")\n",
        "\n",
        "    # Method 2: Using value_counts() to see both related and unrelated counts\n",
        "    distribution = df['is_related'].value_counts()\n",
        "    logger.info(\"Distribution of related/unrelated pairs:\")\n",
        "    for label, count in distribution.items():\n",
        "        logger.info(f\"Class {label}: {count} pairs\")\n",
        "\n",
        "    # Method 3: Using value_counts(normalize=True) to see percentages\n",
        "    percentage_dist = df['is_related'].value_counts(normalize=True) * 100\n",
        "    logger.info(\"Percentage distribution:\")\n",
        "    for label, percentage in percentage_dist.items():\n",
        "        logger.info(f\"Class {label}: {percentage:.2f}%\")\n",
        "\n",
        "    # Additional statistics\n",
        "    total_pairs = len(df)\n",
        "    logger.info(f\"Total number of pairs analyzed: {total_pairs}\")\n",
        "    logger.info(f\"Imbalance ratio (unrelated:related): {(total_pairs - related_count)/related_count:.2f}:1\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to analyze dataset distribution\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [5] - Feature Importance Analysis\n",
        "# Purpose: Visualize and analyze feature importance from trained model for understanding model behavior\n",
        "# Dependencies: pandas, matplotlib, seaborn\n",
        "# Breadcrumbs: Dataset Analysis -> Feature Analysis -> Model Interpretation\n",
        "\n",
        "def plot_feature_importance(rf, feature_names):\n",
        "    \"\"\"Plot feature importance\"\"\"\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': rf.feature_importances_\n",
        "    }).sort_values('importance', ascending=True)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))  # Made figure slightly larger to accommodate more features\n",
        "    sns.barplot(data=importance_df, x='importance', y='feature')\n",
        "    plt.title('Feature Importance in Predicting Related Requirements')\n",
        "    plt.xlabel('Importance Score')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "feature_names = [\n",
        "    'Model 1 (multi-qa-mpnet-dot)', \n",
        "    'Model 2 (all-mpnet)', \n",
        "    'Model 3 (MiniLM)', \n",
        "    'Model 4 (distilroberta)',\n",
        "    'Model 5 (multi-qa-distilbert-cos)',\n",
        "    'Model 6 (multi-qa-MiniLM-L6-cos)',\n",
        "    'Model 7 (stsb-bert-large)',\n",
        "    'Model 8 (stsb-bert-base)',\n",
        "    'TF-IDF'\n",
        "]\n",
        "plot_feature_importance(rf, feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [6] - Threshold Analysis\n",
        "# Purpose: Analyze model performance at different probability thresholds including F2 score optimization\n",
        "# Dependencies: numpy, pandas, sklearn.metrics, matplotlib, pyplot\n",
        "# Breadcrumbs: Model Interpretation -> Threshold Optimization -> Performance Analysis\n",
        "def analyze_thresholds(rf, X_test, y_test):\n",
        "    \"\"\"Analyze different probability thresholds including F2 score\"\"\"\n",
        "    logger.debug(\"Starting threshold analysis\")\n",
        "    try:\n",
        "        y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "        results = []\n",
        "        \n",
        "        for threshold in np.arange(0.1, 1.0, 0.1):\n",
        "            y_pred = (y_prob >= threshold).astype(int)\n",
        "            report = classification_report(y_test, y_pred, output_dict=True)\n",
        "            f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "            results.append({\n",
        "                'threshold': threshold,\n",
        "                'precision': report['1']['precision'],\n",
        "                'recall': report['1']['recall'],\n",
        "                'f1': report['1']['f1-score'],\n",
        "                'f2': f2\n",
        "            })\n",
        "            logger.debug(f\"Analyzed threshold {threshold:.1f}: F2={f2:.3f}\")\n",
        "        \n",
        "        results_df = pd.DataFrame(results)\n",
        "        \n",
        "        # Plot metrics vs threshold\n",
        "        logger.debug(\"Generating threshold analysis plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for metric in ['precision', 'recall', 'f1', 'f2']:\n",
        "            plt.plot(results_df['threshold'], results_df[metric], label=metric)\n",
        "        plt.xlabel('Threshold')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Metrics vs Classification Threshold')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        # Log best results for each metric\n",
        "        logger.info(\"Best Thresholds for Each Metric:\")\n",
        "        best_f1 = results_df.loc[results_df['f1'].idxmax()]\n",
        "        best_f2 = results_df.loc[results_df['f2'].idxmax()]\n",
        "        best_precision = results_df.loc[results_df['precision'].idxmax()]\n",
        "        best_recall = results_df.loc[results_df['recall'].idxmax()]\n",
        "        \n",
        "        logger.info(f\"Best F1 Score (threshold={best_f1['threshold']:.3f}):\")\n",
        "        logger.info(f\"  Precision: {best_f1['precision']:.3f}\")\n",
        "        logger.info(f\"  Recall: {best_f1['recall']:.3f}\")\n",
        "        logger.info(f\"  F1: {best_f1['f1']:.3f}\")\n",
        "        logger.info(f\"  F2: {best_f1['f2']:.3f}\")\n",
        "        \n",
        "        logger.info(f\"Best F2 Score (threshold={best_f2['threshold']:.3f}):\")\n",
        "        logger.info(f\"  Precision: {best_f2['precision']:.3f}\")\n",
        "        logger.info(f\"  Recall: {best_f2['recall']:.3f}\")\n",
        "        logger.info(f\"  F1: {best_f2['f1']:.3f}\")\n",
        "        logger.info(f\"  F2: {best_f2['f2']:.3f}\")\n",
        "        \n",
        "        logger.info(f\"Best Precision (threshold={best_precision['threshold']:.3f}):\")\n",
        "        logger.info(f\"  Precision: {best_precision['precision']:.3f}\")\n",
        "        logger.info(f\"  Recall: {best_precision['recall']:.3f}\")\n",
        "        logger.info(f\"  F1: {best_precision['f1']:.3f}\")\n",
        "        logger.info(f\"  F2: {best_precision['f2']:.3f}\")\n",
        "        \n",
        "        logger.info(f\"Best Recall (threshold={best_recall['threshold']:.3f}):\")\n",
        "        logger.info(f\"  Precision: {best_recall['precision']:.3f}\")\n",
        "        logger.info(f\"  Recall: {best_recall['recall']:.3f}\")\n",
        "        logger.info(f\"  F1: {best_recall['f1']:.3f}\")\n",
        "        logger.info(f\"  F2: {best_recall['f2']:.3f}\")\n",
        "        \n",
        "        logger.debug(\"Threshold analysis completed successfully\")\n",
        "        return results_df\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error during threshold analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting threshold analysis for model performance\")\n",
        "    threshold_results = analyze_thresholds(rf, X_test, y_test)\n",
        "    logger.debug(\"Threshold analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete threshold analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [7] - Get Requirement Texts\n",
        "# Purpose: Retrieve requirement text content from Neo4j database for detailed analysis\n",
        "# Dependencies: Neo4jClient, logger, os\n",
        "# Breadcrumbs: Performance Analysis -> Data Retrieval -> Text Analysis\n",
        "def get_requirement_texts(source_ids, target_ids):\n",
        "    \"\"\"Get requirement texts from Neo4j for given IDs\"\"\"\n",
        "    logger.debug(\"Fetching requirement texts from Neo4j\")\n",
        "    try:\n",
        "        neo4j_client = Neo4jClient(\n",
        "            uri=os.getenv('NEO4J_URI'),\n",
        "            username=os.getenv('NEO4J_USER'),\n",
        "            password=os.getenv('NEO4J_PASSWORD')\n",
        "        )\n",
        "        \n",
        "        # Query to get source requirement texts\n",
        "        query = \"\"\"\n",
        "        MATCH (s:Requirement {type: 'SOURCE'})\n",
        "        WHERE s.id IN $source_ids\n",
        "        RETURN s.id as source_id, s.content as source_text\n",
        "        \"\"\"\n",
        "        with neo4j_client.driver.session() as session:\n",
        "            result = session.run(query, source_ids=source_ids)\n",
        "            source_texts = {record['source_id']: record['source_text'] for record in result}\n",
        "        \n",
        "        # Query to get target requirement texts\n",
        "        query = \"\"\"\n",
        "        MATCH (t:Requirement {type: 'TARGET'})\n",
        "        WHERE t.id IN $target_ids\n",
        "        RETURN t.id as target_id, t.content as target_text\n",
        "        \"\"\"\n",
        "        with neo4j_client.driver.session() as session:\n",
        "            result = session.run(query, target_ids=target_ids)\n",
        "            target_texts = {record['target_id']: record['target_text'] for record in result}\n",
        "        \n",
        "        neo4j_client.close()\n",
        "        logger.debug(f\"Successfully retrieved texts for {len(source_texts)} source and {len(target_texts)} target requirements\")\n",
        "        return source_texts, target_texts\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Failed to retrieve requirement texts\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [8] - Analyze False Negatives\n",
        "# Purpose: Identify and analyze cases where model failed to detect true positive relationships\n",
        "# Dependencies: pandas, numpy, logger, matplotlib\n",
        "# Breadcrumbs: Text Analysis -> Error Analysis -> False Negative Detection\n",
        "def analyze_false_negatives(rf, X_full, y_full, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Analyze all cases where model predicted negative but actual was positive\n",
        "    Args:\n",
        "        rf: Random Forest model\n",
        "        X_full: Full feature set\n",
        "        y_full: Full ground truth labels\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting false negative analysis with threshold {threshold}\")\n",
        "    try:\n",
        "        # Get predictions using the specified threshold\n",
        "        y_prob = rf.predict_proba(X_full)[:, 1]\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        \n",
        "        # Find false negative indices\n",
        "        fn_indices = np.where((y_pred == 0) & (y_full == 1))[0]\n",
        "        \n",
        "        # Create DataFrame with false negatives\n",
        "        fn_data = pd.DataFrame({\n",
        "            'Actual': y_full.iloc[fn_indices],\n",
        "            'Predicted': y_pred[fn_indices],\n",
        "            'Probability': y_prob[fn_indices],\n",
        "            'Source ID': df.iloc[fn_indices]['source_id'],\n",
        "            'Target ID': df.iloc[fn_indices]['target_id'],\n",
        "        })\n",
        "        \n",
        "        # Add model scores\n",
        "        for i, model in enumerate(['model1_score', 'model2_score', 'model3_score', \n",
        "                                 'model4_score', 'model5_score', 'model6_score',\n",
        "                                 'model7_score', 'model8_score', 'tfidf_score']):\n",
        "            fn_data[f'Model {i+1}'] = df.iloc[fn_indices][model]\n",
        "        \n",
        "        # Sort by probability\n",
        "        fn_data = fn_data.sort_values('Probability', ascending=False)\n",
        "        \n",
        "        # Log statistics\n",
        "        logger.info(\"\\nFalse Negative Analysis Results:\")\n",
        "        logger.info(f\"Total false negatives: {len(fn_indices)}\")\n",
        "        logger.info(f\"False negative rate: {len(fn_indices)/len(y_full):.2%}\")\n",
        "        \n",
        "        # Log probability distribution\n",
        "        logger.info(\"\\nProbability Distribution of False Negatives:\")\n",
        "        prob_ranges = [(0.0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5)]\n",
        "        for low, high in prob_ranges:\n",
        "            count = ((fn_data['Probability'] >= low) & (fn_data['Probability'] < high)).sum()\n",
        "            logger.info(f\"Probability {low:.1f}-{high:.1f}: {count} cases ({count/len(fn_indices):.1%})\")\n",
        "        \n",
        "        # Log model score statistics\n",
        "        logger.info(\"\\nModel Score Statistics for False Negatives:\")\n",
        "        for i in range(1, 10):\n",
        "            scores = fn_data[f'Model {i}']\n",
        "            logger.info(f\"Model {i}:\")\n",
        "            logger.info(f\"  Mean: {scores.mean():.3f}\")\n",
        "            logger.info(f\"  Std: {scores.std():.3f}\")\n",
        "            logger.info(f\"  Min: {scores.min():.3f}\")\n",
        "            logger.info(f\"  Max: {scores.max():.3f}\")\n",
        "        \n",
        "        # Plot probability distribution\n",
        "        logger.debug(\"Generating probability distribution plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(fn_data['Probability'], bins=20)\n",
        "        plt.xlabel('Prediction Probability')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of False Negative Probabilities')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        logger.debug(\"False negative analysis completed successfully\")\n",
        "        return fn_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error during false negative analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting false negative analysis\")\n",
        "    fn_analysis = analyze_false_negatives(rf, X_full, y_full, threshold=0.3)\n",
        "    \n",
        "    # Get requirement texts for example cases\n",
        "    source_texts, target_texts = get_requirement_texts(\n",
        "        fn_analysis['Source ID'].head().tolist(),\n",
        "        fn_analysis['Target ID'].head().tolist()\n",
        "    )\n",
        "    \n",
        "    # Log example cases\n",
        "    logger.info(\"Example False Negative Cases (Top 5 by probability):\")\n",
        "    for idx, row in fn_analysis.head().iterrows():\n",
        "        logger.info(f\"Case {idx+1}:\")\n",
        "        logger.info(f\"Source ID: {row['Source ID']}\")\n",
        "        logger.info(f\"Target ID: {row['Target ID']}\")\n",
        "        logger.info(f\"Prediction Probability: {row['Probability']:.3f}\")\n",
        "        for i in range(1, 10):\n",
        "            logger.info(f\"Model {i} Score: {row[f'Model {i}']:.3f}\")\n",
        "        \n",
        "        # Log requirement texts at DEBUG level\n",
        "        logger.debug(\"Requirement Texts:\")\n",
        "        logger.debug(f\"Source Text: {source_texts.get(row['Source ID'], 'Not found')}\")\n",
        "        logger.debug(f\"Target Text: {target_texts.get(row['Target ID'], 'Not found')}\")\n",
        "    \n",
        "    logger.debug(\"False negative analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete false negative analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [9] - Analyze False Positives\n",
        "# Purpose: Identify and analyze cases where model incorrectly predicted positive relationships\n",
        "# Dependencies: pandas, numpy, logger, matplotlib\n",
        "# Breadcrumbs: False Negative Detection -> Error Analysis -> False Positive Detection\n",
        "def analyze_false_positives(rf, X_full, y_full, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Analyze all cases where model predicted positive but actual was negative\n",
        "    Args:\n",
        "        rf: Random Forest model\n",
        "        X_full: Full feature set\n",
        "        y_full: Full ground truth labels\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting false positive analysis with threshold {threshold}\")\n",
        "    try:\n",
        "        # Get predictions using the specified threshold\n",
        "        y_prob = rf.predict_proba(X_full)[:, 1]\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        \n",
        "        # Find false positive indices\n",
        "        fp_indices = np.where((y_pred == 1) & (y_full == 0))[0]\n",
        "        \n",
        "        # Create DataFrame with false positives\n",
        "        fp_data = pd.DataFrame({\n",
        "            'Actual': y_full.iloc[fp_indices],\n",
        "            'Predicted': y_pred[fp_indices],\n",
        "            'Probability': y_prob[fp_indices],\n",
        "            'Source ID': df.iloc[fp_indices]['source_id'],\n",
        "            'Target ID': df.iloc[fp_indices]['target_id'],\n",
        "        })\n",
        "        \n",
        "        # Add model scores\n",
        "        for i, model in enumerate(['model1_score', 'model2_score', 'model3_score', \n",
        "                                 'model4_score', 'model5_score', 'model6_score',\n",
        "                                 'model7_score', 'model8_score', 'tfidf_score']):\n",
        "            fp_data[f'Model {i+1}'] = df.iloc[fp_indices][model]\n",
        "        \n",
        "        # Sort by probability\n",
        "        fp_data = fp_data.sort_values('Probability', ascending=False)\n",
        "        \n",
        "        # Log statistics\n",
        "        logger.info(\"False Positive Analysis Results:\")\n",
        "        logger.info(f\"Total false positives: {len(fp_indices)}\")\n",
        "        logger.info(f\"False positive rate: {len(fp_indices)/len(y_full):.2%}\")\n",
        "        \n",
        "        # Log probability distribution\n",
        "        logger.info(\"Probability Distribution of False Positives:\")\n",
        "        prob_ranges = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
        "        for low, high in prob_ranges:\n",
        "            count = ((fp_data['Probability'] >= low) & (fp_data['Probability'] < high)).sum()\n",
        "            logger.info(f\"Probability {low:.1f}-{high:.1f}: {count} cases ({count/len(fp_indices):.1%})\")\n",
        "        \n",
        "        # Log model score statistics\n",
        "        logger.info(\"Model Score Statistics for False Positives:\")\n",
        "        for i in range(1, 10):\n",
        "            scores = fp_data[f'Model {i}']\n",
        "            logger.info(f\"Model {i}:\")\n",
        "            logger.info(f\"  Mean: {scores.mean():.3f}\")\n",
        "            logger.info(f\"  Std: {scores.std():.3f}\")\n",
        "            logger.info(f\"  Min: {scores.min():.3f}\")\n",
        "            logger.info(f\"  Max: {scores.max():.3f}\")\n",
        "        \n",
        "        # Plot probability distribution\n",
        "        logger.debug(\"Generating probability distribution plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(fp_data['Probability'], bins=20)\n",
        "        plt.xlabel('Prediction Probability')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of False Positive Probabilities')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        logger.debug(\"False positive analysis completed successfully\")\n",
        "        return fp_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error during false positive analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting false positive analysis\")\n",
        "    fp_analysis = analyze_false_positives(rf, X_full, y_full, threshold=0.3)\n",
        "    \n",
        "    # Get requirement texts for example cases\n",
        "    source_texts, target_texts = get_requirement_texts(\n",
        "        fp_analysis['Source ID'].head().tolist(),\n",
        "        fp_analysis['Target ID'].head().tolist()\n",
        "    )\n",
        "    \n",
        "    # Log example cases\n",
        "    logger.info(\"Example False Positive Cases (Top 5 by probability):\")\n",
        "    for idx, row in fp_analysis.head().iterrows():\n",
        "        logger.info(f\"Case {idx+1}:\")\n",
        "        logger.info(f\"Source ID: {row['Source ID']}\")\n",
        "        logger.info(f\"Target ID: {row['Target ID']}\")\n",
        "        logger.info(f\"Prediction Probability: {row['Probability']:.3f}\")\n",
        "        for i in range(1, 10):\n",
        "            logger.info(f\"Model {i} Score: {row[f'Model {i}']:.3f}\")\n",
        "            \n",
        "        # Log requirement texts at DEBUG level\n",
        "        logger.debug(\"Requirement Texts:\")\n",
        "        logger.debug(f\"Source Text: {source_texts.get(row['Source ID'], 'Not found')}\")\n",
        "        logger.debug(f\"Target Text: {target_texts.get(row['Target ID'], 'Not found')}\")\n",
        "    \n",
        "    logger.debug(\"False positive analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete false positive analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [10] - Analyze True Positives\n",
        "# Purpose: Analyze cases where model correctly identified positive relationships for validation\n",
        "# Dependencies: pandas, numpy, logger, matplotlib\n",
        "# Breadcrumbs: False Positive Detection -> Validation Analysis -> True Positive Analysis\n",
        "def analyze_true_positives(rf, X_full, y_full, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Analyze all cases where model correctly predicted positive matches\n",
        "    Args:\n",
        "        rf: Random Forest model\n",
        "        X_full: Full feature set\n",
        "        y_full: Full ground truth labels\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting true positive analysis with threshold {threshold}\")\n",
        "    try:\n",
        "        # Get predictions using the specified threshold\n",
        "        y_prob = rf.predict_proba(X_full)[:, 1]\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        \n",
        "        # Find true positive indices\n",
        "        tp_indices = np.where((y_pred == 1) & (y_full == 1))[0]\n",
        "        \n",
        "        # Create DataFrame with true positives\n",
        "        tp_data = pd.DataFrame({\n",
        "            'Actual': y_full.iloc[tp_indices],\n",
        "            'Predicted': y_pred[tp_indices],\n",
        "            'Probability': y_prob[tp_indices],\n",
        "            'Source ID': df.iloc[tp_indices]['source_id'],\n",
        "            'Target ID': df.iloc[tp_indices]['target_id'],\n",
        "        })\n",
        "        \n",
        "        # Add model scores\n",
        "        for i, model in enumerate(['model1_score', 'model2_score', 'model3_score', \n",
        "                                 'model4_score', 'model5_score', 'model6_score',\n",
        "                                 'model7_score', 'model8_score', 'tfidf_score']):\n",
        "            tp_data[f'Model {i+1}'] = df.iloc[tp_indices][model]\n",
        "        \n",
        "        # Sort by probability\n",
        "        tp_data = tp_data.sort_values('Probability', ascending=False)\n",
        "        \n",
        "        # Log statistics\n",
        "        logger.info(\"True Positive Analysis Results:\")\n",
        "        logger.info(f\"Total true positives: {len(tp_indices)}\")\n",
        "        logger.info(f\"True positive rate: {len(tp_indices)/len(y_full):.2%}\")\n",
        "        \n",
        "        # Log probability distribution\n",
        "        logger.info(\"Probability Distribution of True Positives:\")\n",
        "        prob_ranges = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
        "        for low, high in prob_ranges:\n",
        "            count = ((tp_data['Probability'] >= low) & (tp_data['Probability'] < high)).sum()\n",
        "            logger.info(f\"Probability {low:.1f}-{high:.1f}: {count} cases ({count/len(tp_indices):.1%})\")\n",
        "        \n",
        "        # Log model score statistics\n",
        "        logger.info(\"Model Score Statistics for True Positives:\")\n",
        "        for i in range(1, 10):\n",
        "            scores = tp_data[f'Model {i}']\n",
        "            logger.info(f\"Model {i}:\")\n",
        "            logger.info(f\"  Mean: {scores.mean():.3f}\")\n",
        "            logger.info(f\"  Std: {scores.std():.3f}\")\n",
        "            logger.info(f\"  Min: {scores.min():.3f}\")\n",
        "            logger.info(f\"  Max: {scores.max():.3f}\")\n",
        "        \n",
        "        # Plot probability distribution\n",
        "        logger.debug(\"Generating probability distribution plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(tp_data['Probability'], bins=20)\n",
        "        plt.xlabel('Prediction Probability')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of True Positive Probabilities')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        logger.debug(\"True positive analysis completed successfully\")\n",
        "        return tp_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error during true positive analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting true positive analysis\")\n",
        "    tp_analysis = analyze_true_positives(rf, X_full, y_full, threshold=0.3)\n",
        "    \n",
        "    # Get requirement texts for example cases\n",
        "    source_texts, target_texts = get_requirement_texts(\n",
        "        tp_analysis['Source ID'].head().tolist(),\n",
        "        tp_analysis['Target ID'].head().tolist()\n",
        "    )\n",
        "    \n",
        "    # Log example cases\n",
        "    logger.info(\"Example True Positive Cases (Top 5 by probability):\")\n",
        "    for idx, row in tp_analysis.head().iterrows():\n",
        "        logger.info(f\"Case {idx+1}:\")\n",
        "        logger.info(f\"Source ID: {row['Source ID']}\")\n",
        "        logger.info(f\"Target ID: {row['Target ID']}\")\n",
        "        logger.info(f\"Prediction Probability: {row['Probability']:.3f}\")\n",
        "        for i in range(1, 10):\n",
        "            logger.info(f\"Model {i} Score: {row[f'Model {i}']:.3f}\")\n",
        "            \n",
        "        # Log requirement texts at DEBUG level\n",
        "        logger.debug(\"Requirement Texts:\")\n",
        "        logger.debug(f\"Source Text: {source_texts.get(row['Source ID'], 'Not found')}\")\n",
        "        logger.debug(f\"Target Text: {target_texts.get(row['Target ID'], 'Not found')}\")\n",
        "    \n",
        "    logger.debug(\"True positive analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete true positive analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [11] - Analyze True Negatives\n",
        "# Purpose: Analyze cases where model correctly identified negative relationships for completeness\n",
        "# Dependencies: pandas, numpy, logger, matplotlib\n",
        "# Breadcrumbs: True Positive Analysis -> Validation Analysis -> True Negative Analysis\n",
        "def analyze_true_negatives(rf, X_full, y_full, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Analyze all cases where model correctly predicted negative matches\n",
        "    Args:\n",
        "        rf: Random Forest model\n",
        "        X_full: Full feature set\n",
        "        y_full: Full ground truth labels\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting true negative analysis with threshold {threshold}\")\n",
        "    try:\n",
        "        # Get predictions using the specified threshold\n",
        "        y_prob = rf.predict_proba(X_full)[:, 1]\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        \n",
        "        # Find true negative indices\n",
        "        tn_indices = np.where((y_pred == 0) & (y_full == 0))[0]\n",
        "        \n",
        "        # Create DataFrame with true negatives\n",
        "        tn_data = pd.DataFrame({\n",
        "            'Actual': y_full.iloc[tn_indices],\n",
        "            'Predicted': y_pred[tn_indices],\n",
        "            'Probability': y_prob[tn_indices],\n",
        "            'Source ID': df.iloc[tn_indices]['source_id'],\n",
        "            'Target ID': df.iloc[tn_indices]['target_id'],\n",
        "        })\n",
        "        \n",
        "        # Add model scores\n",
        "        for i, model in enumerate(['model1_score', 'model2_score', 'model3_score', \n",
        "                                 'model4_score', 'model5_score', 'model6_score',\n",
        "                                 'model7_score', 'model8_score', 'tfidf_score']):\n",
        "            tn_data[f'Model {i+1}'] = df.iloc[tn_indices][model]\n",
        "        \n",
        "        # Sort by probability\n",
        "        tn_data = tn_data.sort_values('Probability', ascending=False)\n",
        "        \n",
        "        # Log statistics\n",
        "        logger.info(\"\\nTrue Negative Analysis Results:\")\n",
        "        logger.info(f\"Total true negatives: {len(tn_indices)}\")\n",
        "        logger.info(f\"True negative rate: {len(tn_indices)/len(y_full):.2%}\")\n",
        "        \n",
        "        # Log probability distribution\n",
        "        logger.info(\"\\nProbability Distribution of True Negatives:\")\n",
        "        prob_ranges = [(0.0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5)]\n",
        "        for low, high in prob_ranges:\n",
        "            count = ((tn_data['Probability'] >= low) & (tn_data['Probability'] < high)).sum()\n",
        "            logger.info(f\"Probability {low:.1f}-{high:.1f}: {count} cases ({count/len(tn_indices):.1%})\")\n",
        "        \n",
        "        # Log model score statistics\n",
        "        logger.info(\"\\nModel Score Statistics for True Negatives:\")\n",
        "        for i in range(1, 10):\n",
        "            scores = tn_data[f'Model {i}']\n",
        "            logger.info(f\"Model {i}:\")\n",
        "            logger.info(f\"  Mean: {scores.mean():.3f}\")\n",
        "            logger.info(f\"  Std: {scores.std():.3f}\")\n",
        "            logger.info(f\"  Min: {scores.min():.3f}\")\n",
        "            logger.info(f\"  Max: {scores.max():.3f}\")\n",
        "        \n",
        "        # Plot probability distribution\n",
        "        logger.debug(\"Generating probability distribution plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(tn_data['Probability'], bins=20)\n",
        "        plt.xlabel('Prediction Probability')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of True Negative Probabilities')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        logger.debug(\"True negative analysis completed successfully\")\n",
        "        return tn_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error during true negative analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting true negative analysis\")\n",
        "    tn_analysis = analyze_true_negatives(rf, X_full, y_full, threshold=0.3)\n",
        "    \n",
        "    # Get requirement texts for example cases\n",
        "    source_texts, target_texts = get_requirement_texts(\n",
        "        tn_analysis['Source ID'].head().tolist(),\n",
        "        tn_analysis['Target ID'].head().tolist()\n",
        "    )\n",
        "    \n",
        "    # Log example cases\n",
        "    logger.info(\"\\nExample True Negative Cases (Top 5 by probability):\")\n",
        "    for idx, row in tn_analysis.head().iterrows():\n",
        "        logger.info(f\"\\nCase {idx+1}:\")\n",
        "        logger.info(f\"Source ID: {row['Source ID']}\")\n",
        "        logger.info(f\"Target ID: {row['Target ID']}\")\n",
        "        logger.info(f\"Prediction Probability: {row['Probability']:.3f}\")\n",
        "        for i in range(1, 10):\n",
        "            logger.info(f\"Model {i} Score: {row[f'Model {i}']:.3f}\")\n",
        "            \n",
        "        # Log requirement texts at DEBUG level\n",
        "        logger.debug(\"\\nRequirement Texts:\")\n",
        "        logger.debug(f\"Source Text: {source_texts.get(row['Source ID'], 'Not found')}\")\n",
        "        logger.debug(f\"Target Text: {target_texts.get(row['Target ID'], 'Not found')}\")\n",
        "    \n",
        "    logger.debug(\"True negative analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete true negative analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [12] - Impact Analysis on Project Effort Estimation\n",
        "# Purpose: Evaluate complexity metrics and effort estimation impact based on prediction categories\n",
        "# Dependencies: pandas, numpy, logger\n",
        "# Breadcrumbs: True Negative Analysis -> Business Impact -> Effort Estimation\n",
        "def analyze_requirement_complexity(df_analysis, category):\n",
        "    \"\"\"\n",
        "    Analyze complexity metrics for requirements in different prediction categories\n",
        "    (TP, FP, FN, or TN) to understand potential impact on effort estimation\n",
        "    \n",
        "    Args:\n",
        "        df_analysis: DataFrame containing the analysis results\n",
        "        category: String indicating which category we're analyzing ('TP', 'FP', 'FN', or 'TN')\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting complexity analysis for {category} requirements\")\n",
        "    try:\n",
        "        # Get requirement texts for analysis\n",
        "        source_texts, target_texts = get_requirement_texts(\n",
        "            df_analysis['Source ID'].tolist(),\n",
        "            df_analysis['Target ID'].tolist()\n",
        "        )\n",
        "        \n",
        "        # Add texts to dataframe\n",
        "        df_analysis['Source Text'] = df_analysis['Source ID'].map(source_texts)\n",
        "        df_analysis['Target Text'] = df_analysis['Target ID'].map(target_texts)\n",
        "        \n",
        "        # Calculate complexity metrics\n",
        "        metrics = {\n",
        "            'avg_text_length': {\n",
        "                'source': df_analysis['Source Text'].str.len().mean(),\n",
        "                'target': df_analysis['Target Text'].str.len().mean()\n",
        "            },\n",
        "            'requirement_count': len(df_analysis),\n",
        "            'avg_similarity_scores': {\n",
        "                'model1': df_analysis['Model 1'].mean(),\n",
        "                'model2': df_analysis['Model 2'].mean(),\n",
        "                'tfidf': df_analysis['Model 9'].mean()  # TFIDF is Model 9\n",
        "            },\n",
        "            'similarity_score_variance': {\n",
        "                'model1': df_analysis['Model 1'].var(),\n",
        "                'model2': df_analysis['Model 2'].var(),\n",
        "                'tfidf': df_analysis['Model 9'].var()\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Log analysis results\n",
        "        logger.info(f\"=== {category} Impact Analysis ===\")\n",
        "        logger.info(f\"Number of Requirements: {metrics['requirement_count']}\")\n",
        "        \n",
        "        logger.info(\"Text Length Analysis:\")\n",
        "        logger.info(f\"Average Source Text Length: {metrics['avg_text_length']['source']:.1f} characters\")\n",
        "        logger.info(f\"Average Target Text Length: {metrics['avg_text_length']['target']:.1f} characters\")\n",
        "        \n",
        "        logger.info(\"Similarity Score Analysis:\")\n",
        "        logger.info(\"Average Scores:\")\n",
        "        logger.info(f\"  Model 1: {metrics['avg_similarity_scores']['model1']:.3f}\")\n",
        "        logger.info(f\"  Model 2: {metrics['avg_similarity_scores']['model2']:.3f}\")\n",
        "        logger.info(f\"  TF-IDF: {metrics['avg_similarity_scores']['tfidf']:.3f}\")\n",
        "        \n",
        "        logger.info(\"Score Variance:\")\n",
        "        logger.info(f\"  Model 1: {metrics['similarity_score_variance']['model1']:.3f}\")\n",
        "        logger.info(f\"  Model 2: {metrics['similarity_score_variance']['model2']:.3f}\")\n",
        "        logger.info(f\"  TF-IDF: {metrics['similarity_score_variance']['tfidf']:.3f}\")\n",
        "        \n",
        "        # Log example requirements at DEBUG level\n",
        "        logger.debug(\"Example Requirements (first 3):\")\n",
        "        for idx, row in df_analysis.head(3).iterrows():\n",
        "            logger.debug(f\"Requirement Pair {idx+1}:\")\n",
        "            logger.debug(f\"Source ID: {row['Source ID']}\")\n",
        "            logger.debug(f\"Source Text: {row['Source Text']}\")\n",
        "            logger.debug(f\"Target ID: {row['Target ID']}\")\n",
        "            logger.debug(f\"Target Text: {row['Target Text']}\")\n",
        "            logger.debug(f\"Similarity Scores:\")\n",
        "            logger.debug(f\"  Model 1: {row['Model 1']:.3f}\")\n",
        "            logger.debug(f\"  Model 2: {row['Model 2']:.3f}\")\n",
        "            logger.debug(f\"  TF-IDF: {row['Model 9']:.3f}\")\n",
        "        \n",
        "        return metrics\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during complexity analysis for {category}\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting impact analysis on project effort estimation\")\n",
        "    \n",
        "    # Analyze true positives\n",
        "    logger.debug(\"Analyzing true positive cases\")\n",
        "    tp_metrics = analyze_requirement_complexity(tp_analysis, \"True Positives\")\n",
        "    \n",
        "    # Analyze false positives\n",
        "    logger.debug(\"Analyzing false positive cases\")\n",
        "    fp_metrics = analyze_requirement_complexity(fp_analysis, \"False Positives\")\n",
        "    \n",
        "    # Analyze false negatives\n",
        "    logger.debug(\"Analyzing false negative cases\")\n",
        "    fn_metrics = analyze_requirement_complexity(fn_analysis, \"False Negatives\")\n",
        "    \n",
        "    # Calculate and log comparative metrics\n",
        "    logger.info(\"Comparative Analysis:\")\n",
        "    \n",
        "    # Compare text lengths\n",
        "    logger.info(\"Average Text Length Comparison:\")\n",
        "    categories = [\"True Positives\", \"False Positives\", \"False Negatives\"]\n",
        "    metrics = [tp_metrics, fp_metrics, fn_metrics]\n",
        "    \n",
        "    for cat, met in zip(categories, metrics):\n",
        "        logger.info(f\"{cat}:\")\n",
        "        logger.info(f\"  Source Text: {met['avg_text_length']['source']:.1f} characters\")\n",
        "        logger.info(f\"  Target Text: {met['avg_text_length']['target']:.1f} characters\")\n",
        "    \n",
        "    # Compare similarity scores\n",
        "    logger.info(\"Average Similarity Score Comparison:\")\n",
        "    for cat, met in zip(categories, metrics):\n",
        "        logger.info(f\"{cat}:\")\n",
        "        logger.info(f\"  Model 1: {met['avg_similarity_scores']['model1']:.3f}\")\n",
        "        logger.info(f\"  Model 2: {met['avg_similarity_scores']['model2']:.3f}\")\n",
        "        logger.info(f\"  TF-IDF: {met['avg_similarity_scores']['tfidf']:.3f}\")\n",
        "    \n",
        "    logger.debug(\"Impact analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete impact analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [13] - Scatter Plot Analysis\n",
        "# Purpose: Create comprehensive scatter plots and density visualizations for model comparison\n",
        "# Dependencies: matplotlib, seaborn, numpy, pandas, logger\n",
        "# Breadcrumbs: Effort Estimation -> Visualization -> Scatter Plot Analysis\n",
        "def create_scatter_plots():\n",
        "    \"\"\"\n",
        "    Create scatter plots showing the distribution of TP, FP, FN, and TN cases\n",
        "    with TP and FN on top layers for better visibility\n",
        "    \"\"\"\n",
        "    logger.debug(\"Starting scatter plot analysis\")\n",
        "    try:\n",
        "        # Prepare data for each category - order determines layer position (last items on top)\n",
        "        categories = {\n",
        "            'True Negative': (tn_analysis, 'blue', 'x'),\n",
        "            'False Positive': (fp_analysis, 'grey', '^'),\n",
        "            'False Negative': (fn_analysis, 'red', 's'),\n",
        "            'True Positive': (tp_analysis, 'green', 'o')\n",
        "        }\n",
        "        \n",
        "        # Create subplots for different model combinations\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
        "        fig.suptitle('Similarity Score Distribution by Prediction Category', fontsize=16)\n",
        "        \n",
        "        # Model combinations to plot - using correct column names\n",
        "        plot_configs = [\n",
        "            {\n",
        "                'x': 'Model 1',\n",
        "                'y': 'Model 2',\n",
        "                'title': 'Model1 vs Model2',\n",
        "                'pos': (0, 0)\n",
        "            },\n",
        "            {\n",
        "                'x': 'Model 1',\n",
        "                'y': 'Model 9',  # TFIDF is Model 9\n",
        "                'title': 'Model1 vs TF-IDF',\n",
        "                'pos': (0, 1)\n",
        "            },\n",
        "            {\n",
        "                'x': 'Model 2',\n",
        "                'y': 'Model 9',  # TFIDF is Model 9\n",
        "                'title': 'Model2 vs TF-IDF',\n",
        "                'pos': (1, 0)\n",
        "            },\n",
        "            {\n",
        "                'x': 'Model 3',\n",
        "                'y': 'Model 4',\n",
        "                'title': 'Model3 vs Model4',\n",
        "                'pos': (1, 1)\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        logger.debug(\"Creating scatter plots for model comparisons\")\n",
        "        # Create each subplot\n",
        "        for config in plot_configs:\n",
        "            logger.debug(f\"Creating plot for {config['title']}\")\n",
        "            ax = axes[config['pos'][0], config['pos'][1]]\n",
        "            \n",
        "            # Plot each category (order matters for layering)\n",
        "            for category, (data, color, marker) in categories.items():\n",
        "                # Check for valid data points\n",
        "                valid_mask = ~(np.isnan(data[config['x']]) | np.isnan(data[config['y']]))\n",
        "                if valid_mask.sum() == 0:\n",
        "                    logger.warning(f\"No valid data points for {category} in {config['title']}\")\n",
        "                    continue\n",
        "                \n",
        "                valid_data = data[valid_mask]\n",
        "                logger.debug(f\"Adding {len(valid_data)} {category} data points\")\n",
        "                \n",
        "                ax.scatter(\n",
        "                    valid_data[config['x']],\n",
        "                    valid_data[config['y']],\n",
        "                    c=color,\n",
        "                    marker=marker,\n",
        "                    label=category,\n",
        "                    alpha=0.6,\n",
        "                    s=50  # marker size\n",
        "                )\n",
        "                \n",
        "                # Log statistics for this category and model combination\n",
        "                logger.info(f\"Statistics for {category} in {config['title']}:\")\n",
        "                logger.info(f\"  Number of points: {len(valid_data)}\")\n",
        "                logger.info(f\"  {config['x']} mean: {valid_data[config['x']].mean():.3f}\")\n",
        "                logger.info(f\"  {config['y']} mean: {valid_data[config['y']].mean():.3f}\")\n",
        "            \n",
        "            # Customize plot appearance\n",
        "            ax.set_xlabel(config['x'])\n",
        "            ax.set_ylabel(config['y'])\n",
        "            ax.set_title(config['title'])\n",
        "            ax.grid(True, linestyle='--', alpha=0.7)\n",
        "            ax.legend()\n",
        "            \n",
        "            # Add correlation coefficient to plot\n",
        "            for category, (data, _, _) in categories.items():\n",
        "                valid_mask = ~(np.isnan(data[config['x']]) | np.isnan(data[config['y']]))\n",
        "                if valid_mask.sum() > 1:  # Need at least 2 points for correlation\n",
        "                    valid_data = data[valid_mask]\n",
        "                    correlation = valid_data[config['x']].corr(valid_data[config['y']])\n",
        "                    logger.info(f\"Correlation for {category} between {config['x']} and {config['y']}: {correlation:.3f}\")\n",
        "                else:\n",
        "                    logger.warning(f\"Insufficient data for correlation calculation in {category}\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        logger.debug(\"Scatter plots created successfully\")\n",
        "        \n",
        "        # Save plot if needed\n",
        "        try:\n",
        "            plot_path = \"similarity_scatter_plots.png\"\n",
        "            plt.savefig(plot_path)\n",
        "            logger.info(f\"Scatter plots saved to {plot_path}\")\n",
        "        except Exception as save_error:\n",
        "            logger.warning(f\"Could not save plot to file: {str(save_error)}\")\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating scatter plots\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "def create_density_plots():\n",
        "    \"\"\"\n",
        "    Create density plots for each model's score distribution\n",
        "    \"\"\"\n",
        "    logger.debug(\"Starting density plot analysis\")\n",
        "    try:\n",
        "        fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
        "        fig.suptitle('Score Distribution Density by Model and Category', fontsize=16)\n",
        "        \n",
        "        model_names = [\n",
        "            'Model 1', 'Model 2', 'Model 3',\n",
        "            'Model 4', 'Model 5', 'Model 6',\n",
        "            'Model 7', 'Model 8', 'Model 9'  # Model 9 is TFIDF\n",
        "        ]\n",
        "        \n",
        "        categories = {\n",
        "            'True Positive': (tp_analysis, 'green'),\n",
        "            'False Positive': (fp_analysis, 'grey'),\n",
        "            'False Negative': (fn_analysis, 'red'),\n",
        "            'True Negative': (tn_analysis, 'blue')\n",
        "        }\n",
        "        \n",
        "        for idx, model in enumerate(model_names):\n",
        "            logger.debug(f\"Creating density plot for {model}\")\n",
        "            ax = axes[idx // 3, idx % 3]\n",
        "            \n",
        "            for category, (data, color) in categories.items():\n",
        "                # Filter out invalid values and check for variance\n",
        "                valid_data = data[model].dropna()\n",
        "                if len(valid_data) < 2:\n",
        "                    logger.warning(f\"Insufficient data for {category} in {model}\")\n",
        "                    continue\n",
        "                    \n",
        "                if valid_data.var() == 0:\n",
        "                    logger.warning(f\"Zero variance in {category} for {model}\")\n",
        "                    continue\n",
        "                \n",
        "                try:\n",
        "                    sns.kdeplot(\n",
        "                        data=valid_data,\n",
        "                        ax=ax,\n",
        "                        label=category,\n",
        "                        color=color,\n",
        "                        warn_singular=False  # Suppress singular matrix warning\n",
        "                    )\n",
        "                    \n",
        "                    # Log statistics for this model and category\n",
        "                    logger.info(f\"Statistics for {category} in {model}:\")\n",
        "                    logger.info(f\"  Mean: {valid_data.mean():.3f}\")\n",
        "                    logger.info(f\"  Std: {valid_data.std():.3f}\")\n",
        "                    logger.info(f\"  Min: {valid_data.min():.3f}\")\n",
        "                    logger.info(f\"  Max: {valid_data.max():.3f}\")\n",
        "                except Exception as plot_error:\n",
        "                    logger.warning(f\"Could not create density plot for {category} in {model}: {str(plot_error)}\")\n",
        "            \n",
        "            ax.set_title(f\"{model} Score Distribution\")\n",
        "            ax.grid(True, linestyle='--', alpha=0.7)\n",
        "            ax.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        logger.debug(\"Density plots created successfully\")\n",
        "        \n",
        "        # Save plot if needed\n",
        "        try:\n",
        "            plot_path = \"similarity_density_plots.png\"\n",
        "            plt.savefig(plot_path)\n",
        "            logger.info(f\"Density plots saved to {plot_path}\")\n",
        "        except Exception as save_error:\n",
        "            logger.warning(f\"Could not save plot to file: {str(save_error)}\")\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating density plots\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting visualization analysis\")\n",
        "    logger.info(\"Creating scatter plots...\")\n",
        "    create_scatter_plots()\n",
        "    logger.info(\"Creating density plots...\")\n",
        "    create_density_plots()\n",
        "    logger.info(\"Visualization analysis completed\")\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete visualization analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [14] - Score Transformation Analysis\n",
        "# Purpose: Apply and analyze various mathematical transformations on similarity scores\n",
        "# Dependencies: numpy, pandas, matplotlib, logger\n",
        "# Breadcrumbs: Scatter Plot Analysis -> Data Transformation -> Score Transformation\n",
        "def apply_transformations(scores):\n",
        "    \"\"\"\n",
        "    Apply different transformations to similarity scores\n",
        "    Args:\n",
        "        scores: numpy array of similarity scores\n",
        "    Returns:\n",
        "        dict: Dictionary of transformed scores\n",
        "    \"\"\"\n",
        "    logger.debug(\"Applying score transformations\")\n",
        "    try:\n",
        "        transformations = {\n",
        "            'log': np.log1p(scores),  # log1p to handle zeros\n",
        "            'exp': np.exp(scores) - 1,  # subtract 1 to maintain 0 baseline\n",
        "            'squared': np.square(scores),\n",
        "            'cubic': np.power(scores, 3),\n",
        "            'sqrt': np.sqrt(scores),\n",
        "            'sigmoid': 1 / (1 + np.exp(-10 * (scores - 0.5)))  # scaled sigmoid\n",
        "        }\n",
        "        \n",
        "        # Log transformation statistics\n",
        "        for name, transformed in transformations.items():\n",
        "            logger.debug(f\"{name.capitalize()} transformation stats:\")\n",
        "            logger.debug(f\"  Mean: {transformed.mean():.3f}\")\n",
        "            logger.debug(f\"  Std: {transformed.std():.3f}\")\n",
        "            logger.debug(f\"  Range: [{transformed.min():.3f}, {transformed.max():.3f}]\")\n",
        "            \n",
        "        return transformations\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error applying transformations\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "def plot_transformed_scores():\n",
        "    \"\"\"\n",
        "    Plot original vs transformed scores for different categories\n",
        "    \"\"\"\n",
        "    logger.debug(\"Starting transformed scores visualization\")\n",
        "    try:\n",
        "        # Combine all datasets with their categories\n",
        "        data_sources = {\n",
        "            'True Positive': tp_analysis['Model 1'],\n",
        "            'False Positive': fp_analysis['Model 1'],\n",
        "            'False Negative': fn_analysis['Model 1'],\n",
        "            'True Negative': tn_analysis['Model 1']\n",
        "        }\n",
        "        \n",
        "        all_data = []\n",
        "        for category, scores in data_sources.items():\n",
        "            valid_scores = scores.dropna()\n",
        "            if len(valid_scores) > 0:\n",
        "                df = pd.DataFrame({'score': valid_scores, 'category': category})\n",
        "                all_data.append(df)\n",
        "            else:\n",
        "                logger.warning(f\"No valid scores for {category}\")\n",
        "        \n",
        "        all_data = pd.concat(all_data)\n",
        "        logger.info(f\"Total samples for transformation analysis: {len(all_data)}\")\n",
        "        \n",
        "        # Create subplots for each transformation\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 15))\n",
        "        fig.suptitle('Score Transformations by Category', fontsize=16)\n",
        "        \n",
        "        # Color mapping for categories\n",
        "        colors = {\n",
        "            'True Positive': 'green',\n",
        "            'False Positive': 'grey',\n",
        "            'False Negative': 'red',\n",
        "            'True Negative': 'blue'\n",
        "        }\n",
        "        \n",
        "        # Plot each transformation\n",
        "        transformations = apply_transformations(all_data['score'].values)\n",
        "        for (name, transformed), ax in zip(transformations.items(), axes.flat):\n",
        "            logger.debug(f\"Creating plot for {name} transformation\")\n",
        "            \n",
        "            for category in colors.keys():\n",
        "                category_mask = all_data['category'] == category\n",
        "                if category_mask.sum() > 0:\n",
        "                    original = all_data.loc[category_mask, 'score']\n",
        "                    transformed_scores = transformed[category_mask]\n",
        "                    \n",
        "                    ax.scatter(\n",
        "                        original,\n",
        "                        transformed_scores,\n",
        "                        c=colors[category],\n",
        "                        label=category,\n",
        "                        alpha=0.6,\n",
        "                        s=50\n",
        "                    )\n",
        "                    \n",
        "                    # Log correlation between original and transformed scores\n",
        "                    correlation = np.corrcoef(original, transformed_scores)[0, 1]\n",
        "                    logger.info(f\"Correlation for {category} with {name} transformation: {correlation:.3f}\")\n",
        "            \n",
        "            ax.set_xlabel('Original Score')\n",
        "            ax.set_ylabel(f'{name.capitalize()} Score')\n",
        "            ax.set_title(f'{name.capitalize()} Transformation')\n",
        "            ax.grid(True, linestyle='--', alpha=0.7)\n",
        "            ax.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        logger.debug(\"Transformation plots created successfully\")\n",
        "        \n",
        "        # Save plot if needed\n",
        "        try:\n",
        "            plot_path = \"score_transformations.png\"\n",
        "            plt.savefig(plot_path)\n",
        "            logger.info(f\"Transformation plots saved to {plot_path}\")\n",
        "        except Exception as save_error:\n",
        "            logger.warning(f\"Could not save plot to file: {str(save_error)}\")\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating transformation plots\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting score transformation analysis\")\n",
        "    plot_transformed_scores()\n",
        "    logger.info(\"Score transformation analysis completed\")\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete transformation analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [15] - Log Scale Model Comparison\n",
        "# Purpose: Create logarithmic scale visualizations for enhanced model comparison and analysis\n",
        "# Dependencies: matplotlib, seaborn, numpy, pandas, logger\n",
        "# Breadcrumbs: Score Transformation -> Advanced Visualization -> Log Scale Analysis\n",
        "def create_log_scale_comparisons():\n",
        "    \"\"\"\n",
        "    Create log-scale visualizations comparing different models\n",
        "    \"\"\"\n",
        "    logger.debug(\"Starting log scale comparison analysis\")\n",
        "    try:\n",
        "        # Create figure with multiple subplots\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "        fig.suptitle('Model Comparisons with Different Scale Transformations', fontsize=16)\n",
        "        \n",
        "        categories = {\n",
        "            'True Negative': (tn_analysis, 'blue', 'x'),\n",
        "            'False Positive': (fp_analysis, 'grey', '^'),\n",
        "            'False Negative': (fn_analysis, 'red', 's'),\n",
        "            'True Positive': (tp_analysis, 'green', 'o')\n",
        "        }\n",
        "        \n",
        "        logger.debug(\"Creating log10 scale comparison plot\")\n",
        "        # Plot 1: Log scale scatter\n",
        "        for category, (data, color, marker) in categories.items():\n",
        "            # Add validation for data\n",
        "            if len(data) == 0:\n",
        "                logger.warning(f\"No data available for {category}\")\n",
        "                continue\n",
        "                \n",
        "            valid_mask = (data['Model 1'] > 0) & (data['Model 2'] > 0)\n",
        "            if not valid_mask.any():\n",
        "                logger.warning(f\"No valid positive scores for {category}\")\n",
        "                continue\n",
        "                \n",
        "            valid_data = data[valid_mask]\n",
        "            \n",
        "            ax1.scatter(\n",
        "                np.log10(valid_data['Model 1']),\n",
        "                np.log10(valid_data['Model 2']),\n",
        "                c=color,\n",
        "                marker=marker,\n",
        "                label=category,\n",
        "                alpha=0.6\n",
        "            )\n",
        "            logger.info(f\"Log10 scale statistics for {category}:\")\n",
        "            logger.info(f\"  Points plotted: {len(valid_data)}\")\n",
        "            logger.info(f\"  Model 1 log10 range: [{np.log10(valid_data['Model 1']).min():.3f}, {np.log10(valid_data['Model 1']).max():.3f}]\")\n",
        "            logger.info(f\"  Model 2 log10 range: [{np.log10(valid_data['Model 2']).min():.3f}, {np.log10(valid_data['Model 2']).max():.3f}]\")\n",
        "            \n",
        "        ax1.set_title('Log10 Scale Comparison')\n",
        "        ax1.set_xlabel('Log10(Model 1 Score)')\n",
        "        ax1.set_ylabel('Log10(Model 2 Score)')\n",
        "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax1.legend()\n",
        "\n",
        "        logger.debug(\"Creating natural log scale comparison plot\")\n",
        "        # Plot 2: Natural log scale scatter\n",
        "        for category, (data, color, marker) in categories.items():\n",
        "            valid_mask = (data['Model 1'] > 0) & (data['Model 2'] > 0)\n",
        "            if not valid_mask.any():\n",
        "                continue\n",
        "                \n",
        "            valid_data = data[valid_mask]\n",
        "            \n",
        "            ax2.scatter(\n",
        "                np.log(valid_data['Model 1']),\n",
        "                np.log(valid_data['Model 2']),\n",
        "                c=color,\n",
        "                marker=marker,\n",
        "                label=category,\n",
        "                alpha=0.6\n",
        "            )\n",
        "            logger.info(f\"Natural log scale statistics for {category}:\")\n",
        "            logger.info(f\"  Points plotted: {len(valid_data)}\")\n",
        "            logger.info(f\"  Model 1 ln range: [{np.log(valid_data['Model 1']).min():.3f}, {np.log(valid_data['Model 1']).max():.3f}]\")\n",
        "            logger.info(f\"  Model 2 ln range: [{np.log(valid_data['Model 2']).min():.3f}, {np.log(valid_data['Model 2']).max():.3f}]\")\n",
        "            \n",
        "        ax2.set_title('Natural Log Scale Comparison')\n",
        "        ax2.set_xlabel('ln(Model 1 Score)')\n",
        "        ax2.set_ylabel('ln(Model 2 Score)')\n",
        "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax2.legend()\n",
        "\n",
        "        logger.debug(\"Creating density plots\")\n",
        "        # Plot 3: Log-scaled density plot for Model 1\n",
        "        for category, (data, color, _) in categories.items():\n",
        "            valid_data = data[data['Model 1'] > 0]\n",
        "            if len(valid_data) > 0:\n",
        "                try:\n",
        "                    sns.kdeplot(\n",
        "                        data=np.log10(valid_data['Model 1']),\n",
        "                        ax=ax3,\n",
        "                        label=category,\n",
        "                        color=color\n",
        "                    )\n",
        "                    logger.debug(f\"Created density plot for {category} Model 1\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Could not create density plot for {category} Model 1: {str(e)}\")\n",
        "\n",
        "        ax3.set_title('Log10 Score Distribution - Model 1')\n",
        "        ax3.set_xlabel('Log10(Score)')\n",
        "        ax3.set_ylabel('Density')\n",
        "        ax3.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax3.legend()\n",
        "\n",
        "        # Plot 4: Log-scaled density plot for Model 2\n",
        "        for category, (data, color, _) in categories.items():\n",
        "            valid_data = data[data['Model 2'] > 0]\n",
        "            if len(valid_data) > 0:\n",
        "                try:\n",
        "                    sns.kdeplot(\n",
        "                        data=np.log10(valid_data['Model 2']),\n",
        "                        ax=ax4,\n",
        "                        label=category,\n",
        "                        color=color\n",
        "                    )\n",
        "                    logger.debug(f\"Created density plot for {category} Model 2\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Could not create density plot for {category} Model 2: {str(e)}\")\n",
        "\n",
        "        ax4.set_title('Log10 Score Distribution - Model 2')\n",
        "        ax4.set_xlabel('Log10(Score)')\n",
        "        ax4.set_ylabel('Density')\n",
        "        ax4.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax4.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        logger.debug(\"All plots created successfully\")\n",
        "        \n",
        "        # Save plot if needed\n",
        "        try:\n",
        "            plot_path = \"log_scale_comparisons.png\"\n",
        "            plt.savefig(plot_path)\n",
        "            logger.info(f\"Log scale comparison plots saved to {plot_path}\")\n",
        "        except Exception as save_error:\n",
        "            logger.warning(f\"Could not save plot to file: {str(save_error)}\")\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating log scale comparisons\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting log scale model comparison analysis\")\n",
        "    create_log_scale_comparisons()\n",
        "    logger.info(\"Log scale model comparison analysis completed\")\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete log scale comparison analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}