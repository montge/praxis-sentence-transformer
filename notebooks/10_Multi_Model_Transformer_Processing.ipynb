{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Multi-Model Sentence Transformer Processing\n",
    "**Processes multiple sentence transformer models with TF-IDF comparison and stores similarity results in Neo4j database for document hierarchy analysis.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings for Multi-LLM testing\n",
    "# Dependencies: os, gc, sys, pathlib, logging, dotenv, matplotlib, seaborn, datetime, json, torch, numpy, typing, tqdm, spacy, praxis_sentence_transformer\n",
    "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import all required functionality from the main package (installed via pip)\n",
    "try:\n",
    "    from praxis_sentence_transformer import (\n",
    "        # Core\n",
    "        setup_logging, \n",
    "        handle_exception, \n",
    "        DebugTimer,\n",
    "        Neo4jClient,\n",
    "        \n",
    "        # Data models\n",
    "        Project,\n",
    "        Document,\n",
    "        Section,\n",
    "        Requirement,\n",
    "        DocumentHierarchyLoader,\n",
    "        RequirementsLoader,\n",
    "        RequirementsPreprocessor,\n",
    "        \n",
    "        # Neo4j\n",
    "        RequirementsTraceabilityGraph,\n",
    "        RequirementNode,\n",
    "        \n",
    "        # Utils\n",
    "        initialize_cuda,\n",
    "        cleanup_cuda,\n",
    "        cleanup_resources,\n",
    "        create_results_directory,\n",
    "    )\n",
    "    \n",
    "    # Import SentenceTransformerAnalyzer directly from analyzers package\n",
    "    from praxis_sentence_transformer.analyzers import SentenceTransformerAnalyzer\n",
    "    \n",
    "except ImportError as e:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.error(f\"Failed to import praxis_sentence_transformer: {str(e)}\")\n",
    "    logger.info(\"Please install the package using pip install praxis-sentence-transformer\")\n",
    "    raise\n",
    "\n",
    "# Ensure both English and Italian spaCy models are downloaded\n",
    "try:\n",
    "    nlp_en = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print(\"Downloading English language model...\")\n",
    "    download('en_core_web_sm')\n",
    "    nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    nlp_it = spacy.load('it_core_news_sm')\n",
    "except OSError:\n",
    "    print(\"Downloading Italian language model...\")\n",
    "    download('it_core_news_sm')\n",
    "    nlp_it = spacy.load('it_core_news_sm')\n",
    "\n",
    "# Set up logging\n",
    "logger = setup_logging(\"sentence-transformer-notebook\")\n",
    "logger.info(\"Environment setup completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Path Verification\n",
    "# Purpose: Verify project paths are correctly configured for module imports (No longer needed with pip install)\n",
    "# Dependencies: None (using pip installed package)\n",
    "# Breadcrumbs: Environment Configuration -> Path Verification -> Module Access\n",
    "\n",
    "# Path verification is no longer needed since we're using pip-installed package\n",
    "print(\"Using praxis-sentence-transformer installed via pip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Document Loading and Model Processing Pipeline\n",
    "# Purpose: Initialize database connections, load documents, and process multiple transformer models\n",
    "# Dependencies: neo4j_client, document_loaders, analyzers, graph, cuda utilities\n",
    "# Breadcrumbs: Path Verification -> Document Processing -> Multi-Model Analysis Pipeline\n",
    "\n",
    "try:\n",
    "    # Log project info\n",
    "    project_name = os.getenv('PROJECT_NAME')\n",
    "    logger.info(f\"Processing project: {project_name}\")\n",
    "    \n",
    "    # Initialize CUDA\n",
    "    device, cuda_available = initialize_cuda()\n",
    "    logger.info(f\"Using device: {device} (CUDA available: {cuda_available})\")\n",
    "    \n",
    "    # Initialize Neo4j client\n",
    "    neo4j_client = Neo4jClient(\n",
    "        uri=os.getenv('NEO4J_URI'),\n",
    "        username=os.getenv('NEO4J_USER'),\n",
    "        password=os.getenv('NEO4J_PASSWORD'),\n",
    "        database='neo4j'\n",
    "    )\n",
    "    logger.info(f\"Using Neo4j database: {neo4j_client.database}\")\n",
    "    \n",
    "    # Set up fresh constraints first\n",
    "    logger.info(\"Setting up fresh constraints...\")\n",
    "    neo4j_client.setup_constraints()\n",
    "    \n",
    "    # Initialize document loader\n",
    "    doc_loader = DocumentHierarchyLoader(neo4j_client=neo4j_client)\n",
    "    \n",
    "    # Create project if it doesn't exist\n",
    "    logger.info(f\"Creating/verifying project: {project_name}\")\n",
    "    doc_loader.create_project()\n",
    "    \n",
    "    # Clean database before starting\n",
    "    logger.info(f\"Cleaning {project_name} database before analysis...\")\n",
    "    neo4j_client.cleanup_project(project_name)\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = create_results_directory(\n",
    "        model_name=os.getenv('MODEL_LIST', '[]').strip('[]').split(',')[0].strip().strip('\\\"\\''),\n",
    "        dataset_name=project_name\n",
    "    )\n",
    "    \n",
    "    # Load answer set first\n",
    "    logger.info(\"Loading answer set...\")\n",
    "    requirements_loader = RequirementsLoader(neo4j_client=neo4j_client)\n",
    "    answer_set = requirements_loader.parse_answer_set(os.getenv('ANSWER_FILE'))\n",
    "    logger.info(f\"Loaded {len(answer_set)} reference mappings from answer set\")\n",
    "    \n",
    "    # Load and validate requirements from files\n",
    "    source_reqs = requirements_loader.parse_requirements(os.getenv('SOURCE_FILE'))\n",
    "    target_reqs = requirements_loader.parse_requirements(os.getenv('TARGET_FILE'))\n",
    "    requirements_loader.validate_requirements(source_reqs, target_reqs, answer_set)\n",
    "    \n",
    "    # Load and store documents\n",
    "    logger.info(\"Loading document hierarchy...\")\n",
    "    source_doc, target_doc = doc_loader.load_and_store_documents(\n",
    "        source_file=os.getenv('SOURCE_FILE'),\n",
    "        target_file=os.getenv('TARGET_FILE')\n",
    "    )\n",
    "    \n",
    "    # Create ground truth links\n",
    "    doc_loader.create_ground_truth_links(answer_set)\n",
    "    \n",
    "    # Get model list from environment\n",
    "    model_list = eval(os.getenv('MODEL_LIST', '[\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"]'))\n",
    "    logger.info(f\"Processing {len(model_list)} models: {model_list}\")\n",
    "    \n",
    "    # First compute TF-IDF similarities using first model's analyzer\n",
    "    logger.info(\"Computing TF-IDF similarities...\")\n",
    "    analyzer = SentenceTransformerAnalyzer(\n",
    "        model_name=model_list[0],\n",
    "        alpha=0.3,\n",
    "        device=device\n",
    "    )\n",
    "    analyzer.initialize()\n",
    "    \n",
    "    # Initialize graph with analyzer\n",
    "    logger.info(\"Initializing graph...\")\n",
    "    graph = RequirementsTraceabilityGraph(\n",
    "        analyzer=analyzer, \n",
    "        alpha=0.3,\n",
    "        project_name=project_name\n",
    "    )\n",
    "    \n",
    "    # Compute TF-IDF similarities first\n",
    "    logger.info(\"Computing and storing TF-IDF similarities...\")\n",
    "    graph.compute_tfidf_similarities()\n",
    "    \n",
    "    # Process each sentence transformer model\n",
    "    for model_name in model_list:\n",
    "        logger.info(f\"\\nProcessing model: {model_name}\")\n",
    "        try:\n",
    "            # Initialize analyzer for current model\n",
    "            analyzer = SentenceTransformerAnalyzer(\n",
    "                model_name=model_name,\n",
    "                alpha=0.3,\n",
    "                device=device\n",
    "            )\n",
    "            analyzer.initialize()\n",
    "            \n",
    "            # Update graph with new analyzer\n",
    "            graph.analyzer = analyzer\n",
    "            \n",
    "            # Compute sentence transformer similarities\n",
    "            logger.info(f\"Computing similarities for {model_name}...\")\n",
    "            graph.compute_sentence_transformer_similarities()\n",
    "            \n",
    "            # Log metrics for current model\n",
    "            graph.log_database_metrics()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing model {model_name}: {str(e)}\")\n",
    "            logger.exception(\"Detailed error trace:\")\n",
    "            continue\n",
    "        finally:\n",
    "            cleanup_cuda()\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in processing: {str(e)}\")\n",
    "    cleanup_resources()\n",
    "    if 'neo4j_client' in locals():\n",
    "        neo4j_client.close()\n",
    "    sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
