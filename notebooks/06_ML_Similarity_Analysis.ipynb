{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Machine Learning Similarity Score Analysis\n",
        "**Trains Random Forest classifiers on sentence transformer similarity scores with feature importance analysis and performance visualization.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [0] - Setup and Imports\n",
        "# Purpose: Import all required libraries and configure environment settings for Multi-LLM testing\n",
        "# Dependencies: os, io, sys, pathlib, dotenv, pandas, numpy, datetime, sklearn, matplotlib, seaborn, tqdm, praxis_sentence_transformer\n",
        "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
        "\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    fbeta_score, \n",
        "    roc_auc_score, \n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve,\n",
        "    precision_score,\n",
        "    recall_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Import project modules (installed via pip)\n",
        "from praxis_sentence_transformer import (\n",
        "    setup_logging,\n",
        "    handle_exception,\n",
        "    DebugTimer,\n",
        "    Neo4jClient,\n",
        "    create_results_directory\n",
        ")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set up logging\n",
        "logger = setup_logging(\"similarity-score-analysis-notebook\")\n",
        "\n",
        "try:\n",
        "    # Log initialization\n",
        "    logger.info(\"Initializing Similarity Score Analysis Notebook\")\n",
        "    \n",
        "    # Verify environment variables\n",
        "    required_env_vars = [\n",
        "        'NEO4J_URI', \n",
        "        'NEO4J_USER', \n",
        "        'NEO4J_PASSWORD',\n",
        "        'PROJECT_NAME'\n",
        "    ]\n",
        "    missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
        "    \n",
        "    if missing_vars:\n",
        "        logger.error(f\"Missing required environment variables: {missing_vars}\")\n",
        "        raise EnvironmentError(f\"Missing required environment variables: {missing_vars}\")\n",
        "    \n",
        "    # Validate PROJECT_NAME\n",
        "    project_name = os.getenv('PROJECT_NAME')\n",
        "    if not project_name:\n",
        "        raise ValueError(\"PROJECT_NAME environment variable is empty\")\n",
        "    \n",
        "    # Initialize Neo4j client and verify project\n",
        "    neo4j_client = Neo4jClient(\n",
        "        uri=os.getenv('NEO4J_URI'),\n",
        "        username=os.getenv('NEO4J_USER'),\n",
        "        password=os.getenv('NEO4J_PASSWORD')\n",
        "    )\n",
        "    \n",
        "    # Verify project exists\n",
        "    if not neo4j_client.verify_project_exists(project_name):\n",
        "        raise ValueError(f\"Project '{project_name}' not found in Neo4j database\")\n",
        "    \n",
        "    logger.debug(\"All required environment variables loaded successfully\")\n",
        "        \n",
        "    try:\n",
        "        # Create results directory structure\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        results = create_results_directory(\n",
        "            model_name=\"similarity-analysis\",\n",
        "            dataset_name=project_name\n",
        "        )\n",
        "        logger.info(f\"Created results directory structure\")\n",
        "        logger.debug(f\"Results will be saved with timestamp: {timestamp}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(\"Failed to create results directory\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to initialize notebook\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    if 'neo4j_client' in locals():\n",
        "        neo4j_client.close()\n",
        "        logger.debug(\"Neo4j connection closed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [1] - Neo4j Connection and Data Loading\n",
        "# Purpose: Establish database connection and load similarity data for comprehensive analysis\n",
        "# Dependencies: Neo4jClient, logger, os, pandas, handle_exception\n",
        "# Breadcrumbs: Environment Configuration -> Database Connection -> Data Retrieval\n",
        "\n",
        "def create_neo4j_client():\n",
        "    \"\"\"\n",
        "    Create a Neo4j client with connection parameters from environment variables\n",
        "    Returns:\n",
        "        Neo4jClient: Configured Neo4j client instance\n",
        "    \"\"\"\n",
        "    return Neo4jClient(\n",
        "        uri=os.getenv('NEO4J_URI'),\n",
        "        username=os.getenv('NEO4J_USER'),\n",
        "        password=os.getenv('NEO4J_PASSWORD')\n",
        "    )\n",
        "\n",
        "def check_schema(neo4j_client):\n",
        "    \"\"\"\n",
        "    Check Neo4j schema and relationship types based on project hierarchy\n",
        "    Args:\n",
        "        neo4j_client: Neo4jClient instance\n",
        "    Returns:\n",
        "        dict: Schema information from Neo4j\n",
        "    \"\"\"\n",
        "    logger.debug(\"Initializing Neo4j schema check\")\n",
        "    try:\n",
        "        project_name = os.getenv('PROJECT_NAME')\n",
        "        \n",
        "        # Count query with proper collection and relationship counting\n",
        "        count_query = \"\"\"\n",
        "        MATCH (p:Project {name: $project_name})\n",
        "        WITH p\n",
        "        MATCH (p)-[:CONTAINS*1..]->(r1:Requirement)\n",
        "        WHERE r1.type = 'SOURCE'\n",
        "        WITH p, collect(DISTINCT r1) as source_reqs\n",
        "        MATCH (p)-[:CONTAINS*1..]->(r2:Requirement)\n",
        "        WHERE r2.type = 'TARGET'\n",
        "        WITH source_reqs, collect(DISTINCT r2) as target_reqs\n",
        "        OPTIONAL MATCH (r1)-[rel:SIMILAR_TO]->(r2)\n",
        "        WHERE r1 IN source_reqs \n",
        "        AND r2 IN target_reqs\n",
        "        AND rel.project = $project_name\n",
        "        RETURN \n",
        "            size(source_reqs) as source_req_count,\n",
        "            size(target_reqs) as target_req_count,\n",
        "            count(rel) as similar_count,\n",
        "            size(source_reqs) * size(target_reqs) as potential_total_pairs\n",
        "        \"\"\"\n",
        "        \n",
        "        # Schema query with similar pattern\n",
        "        schema_query = \"\"\"\n",
        "        MATCH (p:Project {name: $project_name})\n",
        "        WITH p\n",
        "        MATCH (p)-[:CONTAINS*1..]->(r1:Requirement)\n",
        "        WHERE r1.type = 'SOURCE'\n",
        "        WITH p, collect(DISTINCT r1) as source_reqs\n",
        "        MATCH (p)-[:CONTAINS*1..]->(r2:Requirement)\n",
        "        WHERE r2.type = 'TARGET'\n",
        "        WITH source_reqs, collect(DISTINCT r2) as target_reqs\n",
        "        MATCH (r1)-[rel:SIMILAR_TO]->(r2)\n",
        "        WHERE r1 IN source_reqs \n",
        "        AND r2 IN target_reqs\n",
        "        AND rel.project = $project_name\n",
        "        WITH DISTINCT labels(r1) as req_labels,\n",
        "             collect(DISTINCT type(rel)) as rel_types,\n",
        "             collect(DISTINCT keys(rel)) as rel_properties\n",
        "        RETURN {\n",
        "            requirement_labels: req_labels,\n",
        "            relationship_types: rel_types,\n",
        "            relationship_properties: rel_properties[0]\n",
        "        } as value\n",
        "        \"\"\"\n",
        "        \n",
        "        with neo4j_client.driver.session() as session:\n",
        "            # Get counts\n",
        "            count_result = session.run(count_query, project_name=project_name)\n",
        "            counts = count_result.single()\n",
        "            \n",
        "            if counts:\n",
        "                logger.info(\"Database Schema Statistics:\")\n",
        "                logger.info(f\"Project: {project_name}\")\n",
        "                logger.info(f\"Number of source requirements: {counts['source_req_count']}\")\n",
        "                logger.info(f\"Number of target requirements: {counts['target_req_count']}\")\n",
        "                logger.info(f\"Number of SIMILAR_TO relationships: {counts['similar_count']}\")\n",
        "                logger.info(f\"Potential total pairs: {counts['potential_total_pairs']}\")\n",
        "            else:\n",
        "                logger.warning(\"No relationships found in the database\")\n",
        "            \n",
        "            # Get schema\n",
        "            schema_result = session.run(schema_query, project_name=project_name)\n",
        "            schema_record = schema_result.single()\n",
        "            \n",
        "            if not schema_record:\n",
        "                logger.warning(f\"No schema information found for project: {project_name}\")\n",
        "                schema = {}\n",
        "            else:\n",
        "                schema = schema_record['value']\n",
        "                logger.debug(\"Schema details:\")\n",
        "                logger.debug(f\"Requirement labels: {schema.get('requirement_labels', [])}\")\n",
        "                logger.debug(f\"Relationship types: {schema.get('relationship_types', [])}\")\n",
        "                logger.debug(f\"Relationship properties: {schema.get('relationship_properties', [])}\")\n",
        "            \n",
        "            return schema\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error checking Neo4j schema\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "def get_similarity_data(neo4j_client):\n",
        "    \"\"\"\n",
        "    Retrieve similarity data for the current project including model scores\n",
        "    \n",
        "    Args:\n",
        "        neo4j_client: Neo4jClient instance\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame containing similarity scores and ground truth\n",
        "    \"\"\"\n",
        "    logger.debug(\"Initializing similarity data retrieval\")\n",
        "    try:\n",
        "        project_name = os.getenv('PROJECT_NAME')\n",
        "        \n",
        "        # Query to get all valid requirement pairs and their scores\n",
        "        query = \"\"\"\n",
        "        MATCH (p:Project {name: $project_name})\n",
        "        WITH p\n",
        "        MATCH (p)-[:CONTAINS*1..]->(r1:Requirement)\n",
        "        WHERE r1.type = 'SOURCE'\n",
        "        WITH p, collect(DISTINCT r1) as source_reqs\n",
        "        MATCH (p)-[:CONTAINS*1..]->(r2:Requirement)\n",
        "        WHERE r2.type = 'TARGET'\n",
        "        WITH source_reqs, collect(DISTINCT r2) as target_reqs\n",
        "        UNWIND source_reqs as r1\n",
        "        UNWIND target_reqs as r2\n",
        "        OPTIONAL MATCH (r1)-[s:SIMILAR_TO]->(r2)\n",
        "        WHERE s.project = $project_name\n",
        "        OPTIONAL MATCH (r1)-[g:GROUND_TRUTH]->(r2)\n",
        "        WHERE g.project = $project_name\n",
        "        WITH \n",
        "            r1.id as source_id,\n",
        "            r2.id as target_id,\n",
        "            s.similarity as similarity_score,\n",
        "            s.model as model_name,\n",
        "            CASE WHEN g IS NOT NULL THEN 1 ELSE 0 END as is_related\n",
        "        WHERE similarity_score IS NOT NULL OR is_related = 1\n",
        "        RETURN *\n",
        "        \"\"\"\n",
        "        \n",
        "        with neo4j_client.driver.session() as session:\n",
        "            result = session.run(query, project_name=project_name)\n",
        "            records = [dict(record) for record in result]\n",
        "            \n",
        "            if not records:\n",
        "                logger.warning(f\"No data found for project: {project_name}\")\n",
        "                return pd.DataFrame()\n",
        "            \n",
        "            # Create initial DataFrame\n",
        "            data = pd.DataFrame(records)\n",
        "            \n",
        "            # Pivot the data to create separate columns for each model\n",
        "            model_scores = data.pivot(\n",
        "                index=['source_id', 'target_id', 'is_related'],\n",
        "                columns='model_name',\n",
        "                values='similarity_score'\n",
        "            ).reset_index()\n",
        "            \n",
        "            # Log dataset statistics\n",
        "            logger.info(\"\\nDataset Statistics:\")\n",
        "            logger.info(f\"Total pairs: {len(model_scores)}\")\n",
        "            logger.info(f\"Related pairs: {model_scores['is_related'].sum()}\")\n",
        "            logger.info(f\"Unrelated pairs: {len(model_scores) - model_scores['is_related'].sum()}\")\n",
        "            \n",
        "            # Check for missing values\n",
        "            if model_scores.isnull().values.any():\n",
        "                logger.warning(\"Missing values found in the dataset\")\n",
        "                logger.debug(f\"Missing value counts:\\n{model_scores.isnull().sum()}\")\n",
        "            else:\n",
        "                logger.debug(\"No missing values found in the dataset\")\n",
        "            \n",
        "            # Log sample data\n",
        "            logger.info(\"\\nFirst 5 rows of dataset:\")\n",
        "            logger.info(f\"\\n{model_scores.head()}\")\n",
        "            logger.info(\"\\nLast 5 rows of dataset:\")\n",
        "            logger.info(f\"\\n{model_scores.tail()}\")\n",
        "            logger.info(\"\\nDataset Info:\")\n",
        "            logger.info(f\"\\n{model_scores.info()}\")\n",
        "            \n",
        "            return model_scores\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error retrieving similarity data\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    # Create a single Neo4j client instance to be used throughout the notebook\n",
        "    neo4j_client = create_neo4j_client()\n",
        "    logger.info(\"Created Neo4j client for notebook session\")\n",
        "    \n",
        "    # Use the client for schema check and data retrieval\n",
        "    schema = check_schema(neo4j_client)\n",
        "    similarity_data = get_similarity_data(neo4j_client)\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to initialize data\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [2] - Data Preparation and Model Training\n",
        "# Purpose: Prepare balanced dataset and train Random Forest classifier for similarity score analysis\n",
        "# Dependencies: pandas, sklearn, logger, RandomForestClassifier, train_test_split\n",
        "# Breadcrumbs: Data Retrieval -> Data Preparation -> Model Training\n",
        "\n",
        "def prepare_and_train_model(data):\n",
        "    \"\"\"\n",
        "    Prepare balanced dataset and train Random Forest model\n",
        "    \n",
        "    Args:\n",
        "        data: DataFrame containing similarity scores and labels\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (trained model, feature importance DataFrame, X_test, y_test, predictions, X_full, y_full)\n",
        "    \"\"\"\n",
        "    logger.info(f\"Starting data preparation and model training for project: {os.getenv('PROJECT_NAME')}\")\n",
        "    try:\n",
        "        # Prepare features and target\n",
        "        # Only use numerical similarity score columns\n",
        "        feature_columns = [col for col in data.columns \n",
        "                         if col not in ['source_id', 'source_content', 'target_id', \n",
        "                                      'target_content', 'is_related', 'model_name']]\n",
        "        \n",
        "        # Create full dataset\n",
        "        X_full = data[feature_columns]\n",
        "        y_full = data['is_related']\n",
        "        \n",
        "        # Prepare balanced dataset\n",
        "        logger.debug(\"Preparing balanced dataset\")\n",
        "        positive_samples = data[data['is_related'] == 1]\n",
        "        negative_samples = data[data['is_related'] == 0].sample(n=len(positive_samples), random_state=42)\n",
        "        balanced_data = pd.concat([positive_samples, negative_samples])\n",
        "        \n",
        "        logger.debug(f\"Created balanced dataset with {len(balanced_data)} total samples\")\n",
        "        \n",
        "        # Prepare features and target for balanced data\n",
        "        X = balanced_data[feature_columns]\n",
        "        y = balanced_data['is_related']\n",
        "        \n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "        logger.debug(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
        "        \n",
        "        # Initialize and train model\n",
        "        logger.debug(\"Initializing RandomForestClassifier\")\n",
        "        rf_model = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=None,\n",
        "            min_samples_split=2,\n",
        "            min_samples_leaf=1,\n",
        "            random_state=42\n",
        "        )\n",
        "        \n",
        "        logger.debug(\"Training RandomForestClassifier\")\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        \n",
        "        # Evaluate model\n",
        "        logger.debug(\"Evaluating model performance\")\n",
        "        y_pred = rf_model.predict(X_test)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        classification_rep = classification_report(y_test, y_pred)\n",
        "        f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred)\n",
        "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "        \n",
        "        # Log results\n",
        "        logger.info(\"\\nClassification Report:\")\n",
        "        logger.info(f\"\\n{classification_rep}\")\n",
        "        logger.info(f\"F2 Score: {f2:.3f}\")\n",
        "        logger.info(f\"ROC AUC Score: {roc_auc:.3f}\")\n",
        "        logger.info(\"Confusion Matrix:\")\n",
        "        logger.info(f\"\\n{conf_matrix}\")\n",
        "        \n",
        "        # Feature importance analysis\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': feature_columns,\n",
        "            'importance': rf_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        logger.info(\"\\nFeature Importance:\")\n",
        "        logger.info(f\"\\n{feature_importance}\")\n",
        "        \n",
        "        return rf_model, feature_importance, X_test, y_test, y_pred, X_full, y_full\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error in model preparation and training\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    # Create a single Neo4j client instance to be used throughout the notebook\n",
        "    rf_model, feature_importance, X_test, y_test, y_pred, X_full, y_full = prepare_and_train_model(similarity_data)\n",
        "    logger.info(\"Model training completed successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to prepare and train model\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [3] - Visualization Functions\n",
        "# Purpose: Create comprehensive visualizations for model performance analysis and feature importance\n",
        "# Dependencies: matplotlib, seaborn, sklearn.metrics, confusion_matrix, roc_curve, precision_recall_curve\n",
        "# Breadcrumbs: Model Training -> Performance Analysis -> Visualization Generation\n",
        "\n",
        "def create_visualizations(model, feature_importance, X_test, y_test, y_pred):\n",
        "    \"\"\"Create and save visualizations for model analysis\"\"\"\n",
        "    project_name = os.getenv('PROJECT_NAME')\n",
        "    logger.info(f\"Creating visualizations for project: {project_name}\")\n",
        "    \n",
        "    try:\n",
        "        # Set up the figure with subplots\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "        fig.suptitle(f'Model Analysis Visualizations - Project: {project_name}', fontsize=16)\n",
        "        \n",
        "        # 1. Confusion Matrix Heatmap\n",
        "        logger.debug(\"Creating confusion matrix heatmap\")\n",
        "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(\n",
        "            conf_matrix, \n",
        "            annot=True, \n",
        "            fmt='d', \n",
        "            cmap='Blues',\n",
        "            xticklabels=['Not Related', 'Related'],\n",
        "            yticklabels=['Not Related', 'Related'],\n",
        "            ax=ax1\n",
        "        )\n",
        "        ax1.set_title('Confusion Matrix')\n",
        "        ax1.set_xlabel('Predicted')\n",
        "        ax1.set_ylabel('Actual')\n",
        "        \n",
        "        # 2. Feature Importance Plot\n",
        "        logger.debug(\"Creating feature importance plot\")\n",
        "        feature_importance_plot = feature_importance.plot(\n",
        "            kind='barh',\n",
        "            x='feature',\n",
        "            y='importance',\n",
        "            ax=ax2,\n",
        "            color='skyblue'\n",
        "        )\n",
        "        ax2.set_title('Feature Importance')\n",
        "        ax2.set_xlabel('Importance Score')\n",
        "        plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
        "        \n",
        "        # 3. ROC Curve\n",
        "        logger.debug(\"Creating ROC curve\")\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        ax3.plot(\n",
        "            fpr, \n",
        "            tpr, \n",
        "            color='darkorange',\n",
        "            lw=2, \n",
        "            label=f'ROC curve (AUC = {roc_auc:.2f})'\n",
        "        )\n",
        "        ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        ax3.set_xlim([0.0, 1.0])\n",
        "        ax3.set_ylim([0.0, 1.05])\n",
        "        ax3.set_xlabel('False Positive Rate')\n",
        "        ax3.set_ylabel('True Positive Rate')\n",
        "        ax3.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
        "        ax3.legend(loc=\"lower right\")\n",
        "        \n",
        "        # 4. Precision-Recall Curve\n",
        "        logger.debug(\"Creating precision-recall curve\")\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "        pr_auc = auc(recall, precision)\n",
        "        \n",
        "        ax4.plot(\n",
        "            recall, \n",
        "            precision, \n",
        "            color='green',\n",
        "            lw=2, \n",
        "            label=f'PR curve (AUC = {pr_auc:.2f})'\n",
        "        )\n",
        "        ax4.set_xlim([0.0, 1.0])\n",
        "        ax4.set_ylim([0.0, 1.05])\n",
        "        ax4.set_xlabel('Recall')\n",
        "        ax4.set_ylabel('Precision')\n",
        "        ax4.set_title('Precision-Recall Curve')\n",
        "        ax4.legend(loc=\"lower left\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save the plot\n",
        "        try:\n",
        "            plot_filename = f\"model_analysis_{project_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "            plot_path = results['visualizations'] / plot_filename\n",
        "            plt.savefig(plot_path)\n",
        "            logger.info(f\"Saved visualization plot to {plot_path}\")\n",
        "        except Exception as save_error:\n",
        "            logger.warning(f\"Could not save plot to file: {str(save_error)}\")\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating visualizations\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    create_visualizations(rf_model, feature_importance, X_test, y_test, y_pred)\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to create visualizations\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [4] - Dataset Distribution Analysis\n",
        "# Purpose: Analyze distribution of related and unrelated requirement pairs for data balance assessment\n",
        "# Dependencies: pandas, logger, similarity_data\n",
        "# Breadcrumbs: Visualization Generation -> Data Analysis -> Distribution Statistics\n",
        "\n",
        "try:\n",
        "    # Method 1: Using sum()\n",
        "    related_count = similarity_data['is_related'].sum()\n",
        "    logger.info(\"Dataset Distribution Analysis:\")\n",
        "    logger.info(f\"Number of related pairs: {related_count}\")\n",
        "\n",
        "    # Method 2: Using value_counts() to see both related and unrelated counts\n",
        "    distribution = similarity_data['is_related'].value_counts()\n",
        "    logger.info(\"Distribution of related/unrelated pairs:\")\n",
        "    for label, count in distribution.items():\n",
        "        logger.info(f\"Class {label}: {count} pairs\")\n",
        "\n",
        "    # Method 3: Using value_counts(normalize=True) to see percentages\n",
        "    percentage_dist = similarity_data['is_related'].value_counts(normalize=True) * 100\n",
        "    logger.info(\"Percentage distribution:\")\n",
        "    for label, percentage in percentage_dist.items():\n",
        "        logger.info(f\"Class {label}: {percentage:.2f}%\")\n",
        "\n",
        "    # Additional statistics\n",
        "    total_pairs = len(similarity_data)\n",
        "    logger.info(f\"Total number of pairs analyzed: {total_pairs}\")\n",
        "    logger.info(f\"Imbalance ratio (unrelated:related): {(total_pairs - related_count)/related_count:.2f}:1\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to analyze dataset distribution\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [5] - Feature Importance Analysis\n",
        "# Purpose: Analyze and visualize feature importance for model selection and weight optimization\n",
        "# Dependencies: pandas, sklearn, RandomForestClassifier, seaborn, matplotlib\n",
        "# Breadcrumbs: Distribution Statistics -> Feature Analysis -> Model Interpretation\n",
        "\n",
        "def analyze_feature_importance(X_train, y_train, model_names, project_name):\n",
        "    \"\"\"\n",
        "    Analyze and visualize feature importance using Random Forest Classifier\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : pandas.DataFrame\n",
        "        Training features\n",
        "    y_train : pandas.Series\n",
        "        Training labels\n",
        "    model_names : list\n",
        "        List of model names used as features\n",
        "    project_name : str\n",
        "        Name of the project being analyzed\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Feature importance scores sorted in descending order\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting feature importance analysis for project: {project_name}\")\n",
        "    \n",
        "    try:\n",
        "        # Initialize and train Random Forest Classifier\n",
        "        rf_classifier = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            random_state=42,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "        logger.debug(\"Training Random Forest Classifier\")\n",
        "        rf_classifier.fit(X_train, y_train)\n",
        "        \n",
        "        # Calculate feature importance\n",
        "        importance_scores = pd.DataFrame({\n",
        "            'Feature': model_names,\n",
        "            'Importance': rf_classifier.feature_importances_\n",
        "        })\n",
        "        importance_scores = importance_scores.sort_values('Importance', ascending=False)\n",
        "        \n",
        "        logger.info(f\"\\nFeature Importance Rankings for {project_name}:\")\n",
        "        for idx, row in importance_scores.iterrows():\n",
        "            logger.info(f\"{row['Feature']}: {row['Importance']:.4f}\")\n",
        "        \n",
        "        # Create visualization\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.barplot(\n",
        "            data=importance_scores,\n",
        "            x='Importance',\n",
        "            y='Feature',\n",
        "            palette='viridis'\n",
        "        )\n",
        "        plt.title(f'Feature Importance Analysis - {project_name}')\n",
        "        plt.xlabel('Importance Score')\n",
        "        plt.ylabel('Model')\n",
        "        \n",
        "        # Add value labels to the bars\n",
        "        for i, v in enumerate(importance_scores['Importance']):\n",
        "            plt.text(v, i, f'{v:.4f}', va='center')\n",
        "        \n",
        "        # Adjust layout and save plot\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save the plot with timestamp in results directory\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        plot_filename = f\"feature_importance_{project_name.lower()}_{timestamp}.png\"\n",
        "        plot_path = results['visualizations'] + \"/\" + plot_filename\n",
        "        plt.savefig(plot_path)\n",
        "        logger.info(f\"Feature importance plot saved to {plot_path}\")\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "        return importance_scores\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error in feature importance analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    project_name = os.getenv('PROJECT_NAME')\n",
        "    logger.info(f\"Starting feature importance analysis for {project_name}\")\n",
        "        \n",
        "    # Get model names (excluding 'is_related' and other non-model columns)\n",
        "    model_columns = [col for col in similarity_data.columns if col not in ['source_id', 'target_id', 'is_related']]\n",
        "    \n",
        "    # Prepare data\n",
        "    X = similarity_data[model_columns]\n",
        "    y = similarity_data['is_related']\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, \n",
        "        test_size=0.2, \n",
        "        random_state=42, \n",
        "        stratify=y\n",
        "    )\n",
        "    \n",
        "    # Run analysis\n",
        "    importance_results = analyze_feature_importance(\n",
        "        X_train, \n",
        "        y_train, \n",
        "        model_columns,\n",
        "        project_name\n",
        "    )\n",
        "    \n",
        "    logger.info(\"Feature importance analysis completed successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete feature importance analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [6] - Threshold Analysis\n",
        "# Purpose: Analyze model performance across different similarity thresholds for optimal threshold selection\n",
        "# Dependencies: numpy, sklearn.metrics, pandas, tqdm, matplotlib\n",
        "# Breadcrumbs: Feature Analysis -> Threshold Optimization -> Performance Evaluation\n",
        "\n",
        "def analyze_threshold_performance(data, thresholds=None):\n",
        "    \"\"\"\n",
        "    Analyze model performance across different thresholds\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas.DataFrame\n",
        "        DataFrame containing similarity scores and ground truth\n",
        "    thresholds : list, optional\n",
        "        List of thresholds to evaluate (default: np.arange(0.1, 1.0, 0.1))\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing performance metrics for each threshold and model\n",
        "    \"\"\"\n",
        "    logger.debug(\"Starting threshold performance analysis\")\n",
        "    \n",
        "    try:\n",
        "        if thresholds is None:\n",
        "            thresholds = np.arange(0.1, 1.0, 0.1)\n",
        "            \n",
        "        # Get only numeric columns (excluding metadata and non-numeric columns)\n",
        "        numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "        model_columns = [col for col in numeric_columns \n",
        "                        if col not in ['source_id', 'target_id', 'is_related']]\n",
        "        \n",
        "        logger.debug(f\"Analyzing thresholds for models: {model_columns}\")\n",
        "        \n",
        "        results = {model: {} for model in model_columns}\n",
        "        \n",
        "        for model in model_columns:\n",
        "            logger.debug(f\"Analyzing thresholds for {model}\")\n",
        "            \n",
        "            # Ensure we have numeric data\n",
        "            model_data = data[model].astype(float)\n",
        "            ground_truth = data['is_related'].astype(int)\n",
        "            \n",
        "            for threshold in thresholds:\n",
        "                # Make predictions using threshold\n",
        "                predictions = (model_data >= threshold).astype(int)\n",
        "                \n",
        "                # Calculate metrics\n",
        "                metrics = {\n",
        "                    'precision': precision_score(ground_truth, predictions),\n",
        "                    'recall': recall_score(ground_truth, predictions),\n",
        "                    'f1': fbeta_score(ground_truth, predictions, beta=1),\n",
        "                    'f2': fbeta_score(ground_truth, predictions, beta=2),\n",
        "                    'confusion_matrix': confusion_matrix(ground_truth, predictions)\n",
        "                }\n",
        "                \n",
        "                # Calculate additional metrics from confusion matrix\n",
        "                tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
        "                metrics.update({\n",
        "                    'true_positives': tp,\n",
        "                    'false_positives': fp,\n",
        "                    'true_negatives': tn,\n",
        "                    'false_negatives': fn,\n",
        "                    'false_negative_rate': fn / (fn + tp) if (fn + tp) > 0 else 0,\n",
        "                    'false_positive_rate': fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "                })\n",
        "                \n",
        "                results[model][threshold] = metrics\n",
        "                \n",
        "                logger.debug(f\"{model} @ {threshold:.2f}: \"\n",
        "                           f\"F1={metrics['f1']:.3f}, \"\n",
        "                           f\"F2={metrics['f2']:.3f}, \"\n",
        "                           f\"FNR={metrics['false_negative_rate']:.3f}\")\n",
        "                \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error in threshold performance analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "def plot_threshold_analysis(results, save_path=None):\n",
        "    \"\"\"\n",
        "    Create visualizations for threshold analysis results\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : dict\n",
        "        Results from threshold analysis\n",
        "    save_path : str, optional\n",
        "        Path to save the visualization\n",
        "    \"\"\"\n",
        "    logger.debug(\"Creating threshold analysis visualizations\")\n",
        "    \n",
        "    try:\n",
        "        models = list(results.keys())\n",
        "        thresholds = sorted(list(results[models[0]].keys()))\n",
        "        \n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "        fig.suptitle('Threshold Analysis Results', fontsize=16)\n",
        "        \n",
        "        # Plot 1: Precision-Recall curves\n",
        "        for model in models:\n",
        "            precision = [results[model][t]['precision'] for t in thresholds]\n",
        "            recall = [results[model][t]['recall'] for t in thresholds]\n",
        "            axes[0, 0].plot(recall, precision, marker='o', label=model)\n",
        "            \n",
        "        axes[0, 0].set_title('Precision-Recall Curve')\n",
        "        axes[0, 0].set_xlabel('Recall')\n",
        "        axes[0, 0].set_ylabel('Precision')\n",
        "        axes[0, 0].grid(True)\n",
        "        axes[0, 0].legend()\n",
        "        \n",
        "        # Plot 2: F1 and F2 scores vs threshold\n",
        "        for model in models:\n",
        "            f1_scores = [results[model][t]['f1'] for t in thresholds]\n",
        "            f2_scores = [results[model][t]['f2'] for t in thresholds]\n",
        "            axes[0, 1].plot(thresholds, f1_scores, marker='o', label=f'{model} (F1)')\n",
        "            axes[0, 1].plot(thresholds, f2_scores, marker='s', label=f'{model} (F2)')\n",
        "            \n",
        "        axes[0, 1].set_title('F1 and F2 Scores vs Threshold')\n",
        "        axes[0, 1].set_xlabel('Threshold')\n",
        "        axes[0, 1].set_ylabel('Score')\n",
        "        axes[0, 1].grid(True)\n",
        "        axes[0, 1].legend()\n",
        "        \n",
        "        # Plot 3: False Negative Rate vs threshold\n",
        "        for model in models:\n",
        "            fnr = [results[model][t]['false_negative_rate'] for t in thresholds]\n",
        "            axes[1, 0].plot(thresholds, fnr, marker='o', label=model)\n",
        "            \n",
        "        axes[1, 0].set_title('False Negative Rate vs Threshold')\n",
        "        axes[1, 0].set_xlabel('Threshold')\n",
        "        axes[1, 0].set_ylabel('False Negative Rate')\n",
        "        axes[1, 0].grid(True)\n",
        "        axes[1, 0].legend()\n",
        "        \n",
        "        # Plot 4: ROC curve\n",
        "        for model in models:\n",
        "            fpr = [results[model][t]['false_positive_rate'] for t in thresholds]\n",
        "            tpr = [1 - results[model][t]['false_negative_rate'] for t in thresholds]\n",
        "            axes[1, 1].plot(fpr, tpr, marker='o', label=model)\n",
        "            \n",
        "        axes[1, 1].plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
        "        axes[1, 1].set_title('ROC Curve')\n",
        "        axes[1, 1].set_xlabel('False Positive Rate')\n",
        "        axes[1, 1].set_ylabel('True Positive Rate')\n",
        "        axes[1, 1].grid(True)\n",
        "        axes[1, 1].legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "            logger.info(f\"Threshold analysis plots saved to {save_path}\")\n",
        "            \n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating threshold analysis visualizations\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting threshold analysis\")\n",
        "    \n",
        "    # Define thresholds to analyze\n",
        "    thresholds = np.arange(0.1, 1.0, 0.1)\n",
        "    \n",
        "    # Run threshold analysis\n",
        "    threshold_results = analyze_threshold_performance(similarity_data, thresholds)\n",
        "    \n",
        "    # Create visualizations\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    plot_filename = f\"threshold_analysis_{project_name.lower()}_{timestamp}.png\"\n",
        "    plot_path = results['visualizations'] + \"/\" + plot_filename\n",
        "    \n",
        "    plot_threshold_analysis(threshold_results, save_path=plot_path)\n",
        "    \n",
        "    # Log best thresholds for each model\n",
        "    logger.info(\"\\nBest thresholds by F1 score:\")\n",
        "    for model in threshold_results.keys():\n",
        "        best_threshold = max(threshold_results[model].items(),\n",
        "                           key=lambda x: x[1]['f1'])[0]\n",
        "        best_metrics = threshold_results[model][best_threshold]\n",
        "        \n",
        "        logger.info(f\"\\n{model}:\")\n",
        "        logger.info(f\"Best threshold: {best_threshold:.2f}\")\n",
        "        logger.info(f\"F1 score: {best_metrics['f1']:.3f}\")\n",
        "        logger.info(f\"F2 score: {best_metrics['f2']:.3f}\")\n",
        "        logger.info(f\"Precision: {best_metrics['precision']:.3f}\")\n",
        "        logger.info(f\"Recall: {best_metrics['recall']:.3f}\")\n",
        "        logger.info(f\"False Negative Rate: {best_metrics['false_negative_rate']:.3f}\")\n",
        "        \n",
        "    logger.info(\"Threshold analysis completed successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete threshold analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [7] - Requirement Text Retrieval\n",
        "# Purpose: Retrieve requirement text content from Neo4j database for detailed analysis\n",
        "# Dependencies: Neo4jClient, logger, handle_exception\n",
        "# Breadcrumbs: Performance Evaluation -> Text Retrieval -> Content Analysis\n",
        "\n",
        "def get_requirement_texts(source_ids, target_ids, neo4j_client):\n",
        "    \"\"\"\n",
        "    Get requirement texts from Neo4j for given IDs\n",
        "    \n",
        "    Args:\n",
        "        source_ids (list): List of source requirement IDs\n",
        "        target_ids (list): List of target requirement IDs\n",
        "        neo4j_client (Neo4jClient): Existing Neo4j client instance\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (source_texts dict, target_texts dict)\n",
        "    \"\"\"\n",
        "    logger.debug(\"Fetching requirement texts from Neo4j\")\n",
        "    try:\n",
        "        # Query to get source requirement texts\n",
        "        query = \"\"\"\n",
        "        MATCH (s:Requirement {type: 'SOURCE'})\n",
        "        WHERE s.id IN $source_ids\n",
        "        RETURN s.id as source_id, s.content as source_text\n",
        "        \"\"\"\n",
        "        with neo4j_client.driver.session() as session:\n",
        "            result = session.run(query, source_ids=source_ids)\n",
        "            source_texts = {record['source_id']: record['source_text'] for record in result}\n",
        "        \n",
        "        # Query to get target requirement texts\n",
        "        query = \"\"\"\n",
        "        MATCH (t:Requirement {type: 'TARGET'})\n",
        "        WHERE t.id IN $target_ids\n",
        "        RETURN t.id as target_id, t.content as target_text\n",
        "        \"\"\"\n",
        "        with neo4j_client.driver.session() as session:\n",
        "            result = session.run(query, target_ids=target_ids)\n",
        "            target_texts = {record['target_id']: record['target_text'] for record in result}\n",
        "        \n",
        "        logger.debug(f\"Successfully retrieved texts for {len(source_texts)} source and {len(target_texts)} target requirements\")\n",
        "        return source_texts, target_texts\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Failed to retrieve requirement texts\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [8] - False Negative Analysis\n",
        "# Purpose: Analyze false negative cases to understand model limitations and improve recall\n",
        "# Dependencies: numpy, pandas, matplotlib, logger, RandomForestClassifier\n",
        "# Breadcrumbs: Content Analysis -> Error Analysis -> False Negative Investigation\n",
        "\n",
        "def analyze_false_negatives(rf, X_full, y_full, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Analyze all cases where model predicted negative but actual was positive\n",
        "    Args:\n",
        "        rf: Random Forest model\n",
        "        X_full: Full feature set\n",
        "        y_full: Full ground truth labels\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting false negative analysis with threshold {threshold}\")\n",
        "    try:\n",
        "        # Get predictions using the specified threshold\n",
        "        y_prob = rf.predict_proba(X_full)[:, 1]\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        \n",
        "        # Find false negative indices\n",
        "        fn_indices = np.where((y_pred == 0) & (y_full == 1))[0]\n",
        "        \n",
        "        # Create DataFrame with false negatives\n",
        "        fn_data = pd.DataFrame({\n",
        "            'Actual': y_full.iloc[fn_indices],\n",
        "            'Predicted': y_pred[fn_indices],\n",
        "            'Probability': y_prob[fn_indices],\n",
        "            'Source ID': similarity_data.iloc[fn_indices]['source_id'],\n",
        "            'Target ID': similarity_data.iloc[fn_indices]['target_id'],\n",
        "        })\n",
        "        \n",
        "        # Get model columns from X_full\n",
        "        model_columns = X_full.columns\n",
        "        logger.debug(f\"Available model columns: {model_columns}\")\n",
        "        \n",
        "        # Add model scores using actual column names from X_full\n",
        "        for i, model_col in enumerate(model_columns):\n",
        "            fn_data[f'Model {i+1}'] = X_full.iloc[fn_indices][model_col]\n",
        "            logger.debug(f\"Added scores from {model_col} as Model {i+1}\")\n",
        "        \n",
        "        # Sort by probability\n",
        "        fn_data = fn_data.sort_values('Probability', ascending=False)\n",
        "        \n",
        "        # Log statistics\n",
        "        logger.info(\"\\nFalse Negative Analysis Results:\")\n",
        "        logger.info(f\"Total false negatives: {len(fn_indices)}\")\n",
        "        logger.info(f\"False negative rate: {len(fn_indices)/len(y_full):.2%}\")\n",
        "        \n",
        "        # Log probability distribution\n",
        "        logger.info(\"\\nProbability Distribution of False Negatives:\")\n",
        "        prob_ranges = [(0.0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5)]\n",
        "        for low, high in prob_ranges:\n",
        "            count = ((fn_data['Probability'] >= low) & (fn_data['Probability'] < high)).sum()\n",
        "            logger.info(f\"Probability {low:.1f}-{high:.1f}: {count} cases ({count/len(fn_indices):.1%})\")\n",
        "        \n",
        "        # Log model score statistics\n",
        "        logger.info(\"\\nModel Score Statistics for False Negatives:\")\n",
        "        for i, model_col in enumerate(model_columns):\n",
        "            scores = fn_data[f'Model {i+1}']\n",
        "            logger.info(f\"Model {i+1} ({model_col}):\")\n",
        "            logger.info(f\"  Mean: {scores.mean():.3f}\")\n",
        "            logger.info(f\"  Std: {scores.std():.3f}\")\n",
        "            logger.info(f\"  Min: {scores.min():.3f}\")\n",
        "            logger.info(f\"  Max: {scores.max():.3f}\")\n",
        "        \n",
        "        # Plot probability distribution\n",
        "        logger.debug(\"Generating probability distribution plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(fn_data['Probability'], bins=20)\n",
        "        plt.xlabel('Prediction Probability')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of False Negative Probabilities')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        logger.debug(\"False negative analysis completed successfully\")\n",
        "        return fn_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error during false negative analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting false negative analysis\")\n",
        "    fn_analysis = analyze_false_negatives(rf_model, X_full, y_full, threshold=0.3)\n",
        "    \n",
        "    # Get requirement texts for example cases\n",
        "    source_texts, target_texts = get_requirement_texts(\n",
        "        fn_analysis['Source ID'].head().tolist(),\n",
        "        fn_analysis['Target ID'].head().tolist(),\n",
        "        neo4j_client\n",
        "    )\n",
        "    \n",
        "    # Log example cases\n",
        "    logger.info(\"Example False Negative Cases (Top 5 by probability):\")\n",
        "    for idx, row in fn_analysis.head().iterrows():\n",
        "        logger.info(f\"Case {idx+1}:\")\n",
        "        logger.info(f\"Source ID: {row['Source ID']}\")\n",
        "        logger.info(f\"Target ID: {row['Target ID']}\")\n",
        "        logger.info(f\"Prediction Probability: {row['Probability']:.3f}\")\n",
        "        for i in range(len(X_full.columns)):\n",
        "            logger.info(f\"Model {i+1} Score: {row[f'Model {i+1}']:.3f}\")\n",
        "        \n",
        "        # Log requirement texts at DEBUG level\n",
        "        logger.debug(\"Requirement Texts:\")\n",
        "        logger.debug(f\"Source Text: {source_texts.get(row['Source ID'], 'Not found')}\")\n",
        "        logger.debug(f\"Target Text: {target_texts.get(row['Target ID'], 'Not found')}\")\n",
        "    \n",
        "    logger.debug(\"False negative analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete false negative analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [9] - False Positive Analysis\n",
        "# Purpose: Analyze false positive cases to understand precision limitations and reduce false alarms\n",
        "# Dependencies: numpy, pandas, matplotlib, logger, RandomForestClassifier\n",
        "# Breadcrumbs: False Negative Investigation -> Error Analysis -> False Positive Investigation\n",
        "\n",
        "def analyze_false_positives(rf, X_full, y_full, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Analyze all cases where model predicted positive but actual was negative\n",
        "    Args:\n",
        "        rf: Random Forest model\n",
        "        X_full: Full feature set\n",
        "        y_full: Full ground truth labels\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting false positive analysis with threshold {threshold}\")\n",
        "    try:\n",
        "        # Get predictions using the specified threshold\n",
        "        y_prob = rf.predict_proba(X_full)[:, 1]\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        \n",
        "        # Find false positive indices\n",
        "        fp_indices = np.where((y_pred == 1) & (y_full == 0))[0]\n",
        "        \n",
        "        # Create DataFrame with false positives\n",
        "        fp_data = pd.DataFrame({\n",
        "            'Actual': y_full.iloc[fp_indices],\n",
        "            'Predicted': y_pred[fp_indices],\n",
        "            'Probability': y_prob[fp_indices],\n",
        "            'Source ID': similarity_data.iloc[fp_indices]['source_id'],\n",
        "            'Target ID': similarity_data.iloc[fp_indices]['target_id'],\n",
        "        })\n",
        "        \n",
        "        # Get model columns from X_full\n",
        "        model_columns = X_full.columns\n",
        "        logger.debug(f\"Available model columns: {model_columns}\")\n",
        "        \n",
        "        # Add model scores using actual column names from X_full\n",
        "        for i, model_col in enumerate(model_columns):\n",
        "            fp_data[f'Model {i+1}'] = X_full.iloc[fp_indices][model_col]\n",
        "            logger.debug(f\"Added scores from {model_col} as Model {i+1}\")\n",
        "        \n",
        "        # Sort by probability\n",
        "        fp_data = fp_data.sort_values('Probability', ascending=False)\n",
        "        \n",
        "        # Log statistics\n",
        "        logger.info(\"\\nFalse Positive Analysis Results:\")\n",
        "        logger.info(f\"Total false positives: {len(fp_indices)}\")\n",
        "        logger.info(f\"False positive rate: {len(fp_indices)/len(y_full):.2%}\")\n",
        "        \n",
        "        # Log probability distribution\n",
        "        logger.info(\"\\nProbability Distribution of False Positives:\")\n",
        "        prob_ranges = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
        "        for low, high in prob_ranges:\n",
        "            count = ((fp_data['Probability'] >= low) & (fp_data['Probability'] < high)).sum()\n",
        "            logger.info(f\"Probability {low:.1f}-{high:.1f}: {count} cases ({count/len(fp_indices):.1%})\")\n",
        "        \n",
        "        # Log model score statistics\n",
        "        logger.info(\"\\nModel Score Statistics for False Positives:\")\n",
        "        for i, model_col in enumerate(model_columns):\n",
        "            scores = fp_data[f'Model {i+1}']\n",
        "            logger.info(f\"Model {i+1} ({model_col}):\")\n",
        "            logger.info(f\"  Mean: {scores.mean():.3f}\")\n",
        "            logger.info(f\"  Std: {scores.std():.3f}\")\n",
        "            logger.info(f\"  Min: {scores.min():.3f}\")\n",
        "            logger.info(f\"  Max: {scores.max():.3f}\")\n",
        "        \n",
        "        # Plot probability distribution\n",
        "        logger.debug(\"Generating probability distribution plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(fp_data['Probability'], bins=20)\n",
        "        plt.xlabel('Prediction Probability')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of False Positive Probabilities')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        logger.debug(\"False positive analysis completed successfully\")\n",
        "        return fp_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error during false positive analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting false positive analysis\")\n",
        "    fp_analysis = analyze_false_positives(rf_model, X_full, y_full, threshold=0.3)\n",
        "    \n",
        "    # Get requirement texts for example cases\n",
        "    source_texts, target_texts = get_requirement_texts(\n",
        "        fp_analysis['Source ID'].head().tolist(),\n",
        "        fp_analysis['Target ID'].head().tolist(),\n",
        "        neo4j_client\n",
        "    )\n",
        "    \n",
        "    # Log example cases\n",
        "    logger.info(\"Example False Positive Cases (Top 5 by probability):\")\n",
        "    for idx, row in fp_analysis.head().iterrows():\n",
        "        logger.info(f\"Case {idx+1}:\")\n",
        "        logger.info(f\"Source ID: {row['Source ID']}\")\n",
        "        logger.info(f\"Target ID: {row['Target ID']}\")\n",
        "        logger.info(f\"Prediction Probability: {row['Probability']:.3f}\")\n",
        "        for i in range(len(X_full.columns)):\n",
        "            logger.info(f\"Model {i+1} Score: {row[f'Model {i+1}']:.3f}\")\n",
        "            \n",
        "        # Log requirement texts at DEBUG level\n",
        "        logger.debug(\"Requirement Texts:\")\n",
        "        logger.debug(f\"Source Text: {source_texts.get(row['Source ID'], 'Not found')}\")\n",
        "        logger.debug(f\"Target Text: {target_texts.get(row['Target ID'], 'Not found')}\")\n",
        "    \n",
        "    logger.debug(\"False positive analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete false positive analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [10] - True Positive Analysis\n",
        "# Purpose: Analyze true positive cases to understand what makes good predictions and validate model strength\n",
        "# Dependencies: numpy, pandas, matplotlib, logger, RandomForestClassifier\n",
        "# Breadcrumbs: False Positive Investigation -> Success Analysis -> True Positive Investigation\n",
        "\n",
        "def analyze_true_positives(rf, X_full, y_full, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Analyze all cases where model correctly predicted positive matches\n",
        "    Args:\n",
        "        rf: Random Forest model\n",
        "        X_full: Full feature set\n",
        "        y_full: Full ground truth labels\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting true positive analysis with threshold {threshold}\")\n",
        "    try:\n",
        "        # Get predictions using the specified threshold\n",
        "        y_prob = rf.predict_proba(X_full)[:, 1]\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        \n",
        "        # Find true positive indices\n",
        "        tp_indices = np.where((y_pred == 1) & (y_full == 1))[0]\n",
        "        \n",
        "        # Create DataFrame with true positives\n",
        "        tp_data = pd.DataFrame({\n",
        "            'Actual': y_full.iloc[tp_indices],\n",
        "            'Predicted': y_pred[tp_indices],\n",
        "            'Probability': y_prob[tp_indices],\n",
        "            'Source ID': similarity_data.iloc[tp_indices]['source_id'],\n",
        "            'Target ID': similarity_data.iloc[tp_indices]['target_id'],\n",
        "        })\n",
        "        \n",
        "        # Get model columns from X_full\n",
        "        model_columns = X_full.columns\n",
        "        logger.debug(f\"Available model columns: {model_columns}\")\n",
        "        \n",
        "        # Add model scores using actual column names from X_full\n",
        "        for i, model_col in enumerate(model_columns):\n",
        "            tp_data[f'Model {i+1}'] = X_full.iloc[tp_indices][model_col]\n",
        "            logger.debug(f\"Added scores from {model_col} as Model {i+1}\")\n",
        "        \n",
        "        # Sort by probability\n",
        "        tp_data = tp_data.sort_values('Probability', ascending=False)\n",
        "        \n",
        "        # Log statistics\n",
        "        logger.info(\"\\nTrue Positive Analysis Results:\")\n",
        "        logger.info(f\"Total true positives: {len(tp_indices)}\")\n",
        "        logger.info(f\"True positive rate: {len(tp_indices)/len(y_full):.2%}\")\n",
        "        \n",
        "        # Log probability distribution\n",
        "        logger.info(\"\\nProbability Distribution of True Positives:\")\n",
        "        prob_ranges = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
        "        for low, high in prob_ranges:\n",
        "            count = ((tp_data['Probability'] >= low) & (tp_data['Probability'] < high)).sum()\n",
        "            logger.info(f\"Probability {low:.1f}-{high:.1f}: {count} cases ({count/len(tp_indices):.1%})\")\n",
        "        \n",
        "        # Log model score statistics\n",
        "        logger.info(\"\\nModel Score Statistics for True Positives:\")\n",
        "        for i, model_col in enumerate(model_columns):\n",
        "            scores = tp_data[f'Model {i+1}']\n",
        "            logger.info(f\"Model {i+1} ({model_col}):\")\n",
        "            logger.info(f\"  Mean: {scores.mean():.3f}\")\n",
        "            logger.info(f\"  Std: {scores.std():.3f}\")\n",
        "            logger.info(f\"  Min: {scores.min():.3f}\")\n",
        "            logger.info(f\"  Max: {scores.max():.3f}\")\n",
        "        \n",
        "        # Plot probability distribution\n",
        "        logger.debug(\"Generating probability distribution plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(tp_data['Probability'], bins=20)\n",
        "        plt.xlabel('Prediction Probability')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of True Positive Probabilities')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        logger.debug(\"True positive analysis completed successfully\")\n",
        "        return tp_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error during true positive analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting true positive analysis\")\n",
        "    tp_analysis = analyze_true_positives(rf_model, X_full, y_full, threshold=0.3)\n",
        "    \n",
        "    # Get requirement texts for example cases\n",
        "    source_texts, target_texts = get_requirement_texts(\n",
        "        tp_analysis['Source ID'].head().tolist(),\n",
        "        tp_analysis['Target ID'].head().tolist(),\n",
        "        neo4j_client\n",
        "    )\n",
        "    \n",
        "    # Log example cases\n",
        "    logger.info(\"Example True Positive Cases (Top 5 by probability):\")\n",
        "    for idx, row in tp_analysis.head().iterrows():\n",
        "        logger.info(f\"Case {idx+1}:\")\n",
        "        logger.info(f\"Source ID: {row['Source ID']}\")\n",
        "        logger.info(f\"Target ID: {row['Target ID']}\")\n",
        "        logger.info(f\"Prediction Probability: {row['Probability']:.3f}\")\n",
        "        for i in range(len(X_full.columns)):\n",
        "            logger.info(f\"Model {i+1} Score: {row[f'Model {i+1}']:.3f}\")\n",
        "            \n",
        "        # Log requirement texts at DEBUG level\n",
        "        logger.debug(\"Requirement Texts:\")\n",
        "        logger.debug(f\"Source Text: {source_texts.get(row['Source ID'], 'Not found')}\")\n",
        "        logger.debug(f\"Target Text: {target_texts.get(row['Target ID'], 'Not found')}\")\n",
        "    \n",
        "    logger.debug(\"True positive analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete true positive analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [11] - True Negative Analysis\n",
        "# Purpose: Analyze true negative cases to validate model specificity and understand correct rejections\n",
        "# Dependencies: numpy, pandas, matplotlib, logger, RandomForestClassifier\n",
        "# Breadcrumbs: True Positive Investigation -> Specificity Analysis -> True Negative Investigation\n",
        "\n",
        "def analyze_true_negatives(rf, X_full, y_full, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Analyze all cases where model correctly predicted negative matches\n",
        "    Args:\n",
        "        rf: Random Forest model\n",
        "        X_full: Full feature set\n",
        "        y_full: Full ground truth labels\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting true negative analysis with threshold {threshold}\")\n",
        "    try:\n",
        "        # Get predictions using the specified threshold\n",
        "        y_prob = rf.predict_proba(X_full)[:, 1]\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        \n",
        "        # Find true negative indices\n",
        "        tn_indices = np.where((y_pred == 0) & (y_full == 0))[0]\n",
        "        \n",
        "        # Create DataFrame with true negatives\n",
        "        tn_data = pd.DataFrame({\n",
        "            'Actual': y_full.iloc[tn_indices],\n",
        "            'Predicted': y_pred[tn_indices],\n",
        "            'Probability': y_prob[tn_indices],\n",
        "            'Source ID': similarity_data.iloc[tn_indices]['source_id'],\n",
        "            'Target ID': similarity_data.iloc[tn_indices]['target_id'],\n",
        "        })\n",
        "        \n",
        "        # Get model columns from X_full\n",
        "        model_columns = X_full.columns\n",
        "        logger.debug(f\"Available model columns: {model_columns}\")\n",
        "        \n",
        "        # Add model scores using actual column names from X_full\n",
        "        for i, model_col in enumerate(model_columns):\n",
        "            tn_data[f'Model {i+1}'] = X_full.iloc[tn_indices][model_col]\n",
        "            logger.debug(f\"Added scores from {model_col} as Model {i+1}\")\n",
        "        \n",
        "        # Sort by probability\n",
        "        tn_data = tn_data.sort_values('Probability', ascending=False)\n",
        "        \n",
        "        # Log statistics\n",
        "        logger.info(\"\\nTrue Negative Analysis Results:\")\n",
        "        logger.info(f\"Total true negatives: {len(tn_indices)}\")\n",
        "        logger.info(f\"True negative rate: {len(tn_indices)/len(y_full):.2%}\")\n",
        "        \n",
        "        # Log probability distribution\n",
        "        logger.info(\"\\nProbability Distribution of True Negatives:\")\n",
        "        prob_ranges = [(0.0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5)]\n",
        "        for low, high in prob_ranges:\n",
        "            count = ((tn_data['Probability'] >= low) & (tn_data['Probability'] < high)).sum()\n",
        "            logger.info(f\"Probability {low:.1f}-{high:.1f}: {count} cases ({count/len(tn_indices):.1%})\")\n",
        "        \n",
        "        # Log model score statistics\n",
        "        logger.info(\"\\nModel Score Statistics for True Negatives:\")\n",
        "        for i, model_col in enumerate(model_columns):\n",
        "            scores = tn_data[f'Model {i+1}']\n",
        "            logger.info(f\"Model {i+1} ({model_col}):\")\n",
        "            logger.info(f\"  Mean: {scores.mean():.3f}\")\n",
        "            logger.info(f\"  Std: {scores.std():.3f}\")\n",
        "            logger.info(f\"  Min: {scores.min():.3f}\")\n",
        "            logger.info(f\"  Max: {scores.max():.3f}\")\n",
        "        \n",
        "        # Plot probability distribution\n",
        "        logger.debug(\"Generating probability distribution plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(tn_data['Probability'], bins=20)\n",
        "        plt.xlabel('Prediction Probability')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distribution of True Negative Probabilities')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n",
        "        logger.debug(\"True negative analysis completed successfully\")\n",
        "        return tn_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error during true negative analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting true negative analysis\")\n",
        "    tn_analysis = analyze_true_negatives(rf_model, X_full, y_full, threshold=0.3)\n",
        "    \n",
        "    # Get requirement texts for example cases\n",
        "    source_texts, target_texts = get_requirement_texts(\n",
        "        tn_analysis['Source ID'].head().tolist(),\n",
        "        tn_analysis['Target ID'].head().tolist(),\n",
        "        neo4j_client\n",
        "    )\n",
        "    \n",
        "    # Log example cases\n",
        "    logger.info(\"\\nExample True Negative Cases (Top 5 by probability):\")\n",
        "    for idx, row in tn_analysis.head().iterrows():\n",
        "        logger.info(f\"\\nCase {idx+1}:\")\n",
        "        logger.info(f\"Source ID: {row['Source ID']}\")\n",
        "        logger.info(f\"Target ID: {row['Target ID']}\")\n",
        "        logger.info(f\"Prediction Probability: {row['Probability']:.3f}\")\n",
        "        for i in range(len(X_full.columns)):\n",
        "            logger.info(f\"Model {i+1} Score: {row[f'Model {i+1}']:.3f}\")\n",
        "            \n",
        "        # Log requirement texts at DEBUG level\n",
        "        logger.debug(\"\\nRequirement Texts:\")\n",
        "        logger.debug(f\"Source Text: {source_texts.get(row['Source ID'], 'Not found')}\")\n",
        "        logger.debug(f\"Target Text: {target_texts.get(row['Target ID'], 'Not found')}\")\n",
        "    \n",
        "    logger.debug(\"True negative analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete true negative analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [12] - Impact Analysis on Project Effort Estimation\n",
        "# Purpose: Analyze requirement complexity metrics to understand impact on project effort estimation\n",
        "# Dependencies: pandas, logger, get_requirement_texts, neo4j_client\n",
        "# Breadcrumbs: True Negative Investigation -> Impact Assessment -> Complexity Analysis\n",
        "\n",
        "def analyze_requirement_complexity(df_analysis, category):\n",
        "    \"\"\"\n",
        "    Analyze complexity metrics for requirements in different prediction categories\n",
        "    (TP, FP, FN, or TN) to understand potential impact on effort estimation\n",
        "    \n",
        "    Args:\n",
        "        df_analysis: DataFrame containing the analysis results\n",
        "        category: String indicating which category we're analyzing ('TP', 'FP', 'FN', or 'TN')\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting complexity analysis for {category} requirements\")\n",
        "    try:\n",
        "        # Get requirement texts for analysis\n",
        "        source_texts, target_texts = get_requirement_texts(\n",
        "            df_analysis['Source ID'].tolist(),\n",
        "            df_analysis['Target ID'].tolist(),\n",
        "            neo4j_client\n",
        "        )\n",
        "        \n",
        "        # Add texts to dataframe\n",
        "        df_analysis['Source Text'] = df_analysis['Source ID'].map(source_texts)\n",
        "        df_analysis['Target Text'] = df_analysis['Target ID'].map(target_texts)\n",
        "        \n",
        "        # Calculate complexity metrics\n",
        "        metrics = {\n",
        "            'avg_text_length': {\n",
        "                'source': df_analysis['Source Text'].str.len().mean(),\n",
        "                'target': df_analysis['Target Text'].str.len().mean()\n",
        "            },\n",
        "            'requirement_count': len(df_analysis),\n",
        "            'avg_similarity_scores': {\n",
        "                'model1': df_analysis['Model 1'].mean(),\n",
        "                'model2': df_analysis['Model 2'].mean(),\n",
        "                'tfidf': df_analysis['Model 9'].mean()  # TFIDF is Model 9\n",
        "            },\n",
        "            'similarity_score_variance': {\n",
        "                'model1': df_analysis['Model 1'].var(),\n",
        "                'model2': df_analysis['Model 2'].var(),\n",
        "                'tfidf': df_analysis['Model 9'].var()\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Log analysis results\n",
        "        logger.info(f\"=== {category} Impact Analysis ===\")\n",
        "        logger.info(f\"Number of Requirements: {metrics['requirement_count']}\")\n",
        "        \n",
        "        logger.info(\"Text Length Analysis:\")\n",
        "        logger.info(f\"Average Source Text Length: {metrics['avg_text_length']['source']:.1f} characters\")\n",
        "        logger.info(f\"Average Target Text Length: {metrics['avg_text_length']['target']:.1f} characters\")\n",
        "        \n",
        "        logger.info(\"Similarity Score Analysis:\")\n",
        "        logger.info(\"Average Scores:\")\n",
        "        logger.info(f\"  Model 1: {metrics['avg_similarity_scores']['model1']:.3f}\")\n",
        "        logger.info(f\"  Model 2: {metrics['avg_similarity_scores']['model2']:.3f}\")\n",
        "        logger.info(f\"  TF-IDF: {metrics['avg_similarity_scores']['tfidf']:.3f}\")\n",
        "        \n",
        "        logger.info(\"Score Variance:\")\n",
        "        logger.info(f\"  Model 1: {metrics['similarity_score_variance']['model1']:.3f}\")\n",
        "        logger.info(f\"  Model 2: {metrics['similarity_score_variance']['model2']:.3f}\")\n",
        "        logger.info(f\"  TF-IDF: {metrics['similarity_score_variance']['tfidf']:.3f}\")\n",
        "        \n",
        "        # Log example requirements at DEBUG level\n",
        "        logger.debug(\"Example Requirements (first 3):\")\n",
        "        for idx, row in df_analysis.head(3).iterrows():\n",
        "            logger.debug(f\"Requirement Pair {idx+1}:\")\n",
        "            logger.debug(f\"Source ID: {row['Source ID']}\")\n",
        "            logger.debug(f\"Source Text: {row['Source Text']}\")\n",
        "            logger.debug(f\"Target ID: {row['Target ID']}\")\n",
        "            logger.debug(f\"Target Text: {row['Target Text']}\")\n",
        "            logger.debug(f\"Similarity Scores:\")\n",
        "            logger.debug(f\"  Model 1: {row['Model 1']:.3f}\")\n",
        "            logger.debug(f\"  Model 2: {row['Model 2']:.3f}\")\n",
        "            logger.debug(f\"  TF-IDF: {row['Model 9']:.3f}\")\n",
        "        \n",
        "        return metrics\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during complexity analysis for {category}\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting impact analysis on project effort estimation\")\n",
        "    \n",
        "    # Analyze true positives\n",
        "    logger.debug(\"Analyzing true positive cases\")\n",
        "    tp_metrics = analyze_requirement_complexity(tp_analysis, \"True Positives\")\n",
        "    \n",
        "    # Analyze false positives\n",
        "    logger.debug(\"Analyzing false positive cases\")\n",
        "    fp_metrics = analyze_requirement_complexity(fp_analysis, \"False Positives\")\n",
        "    \n",
        "    # Analyze false negatives\n",
        "    logger.debug(\"Analyzing false negative cases\")\n",
        "    fn_metrics = analyze_requirement_complexity(fn_analysis, \"False Negatives\")\n",
        "    \n",
        "    # Calculate and log comparative metrics\n",
        "    logger.info(\"Comparative Analysis:\")\n",
        "    \n",
        "    # Compare text lengths\n",
        "    logger.info(\"Average Text Length Comparison:\")\n",
        "    categories = [\"True Positives\", \"False Positives\", \"False Negatives\"]\n",
        "    metrics = [tp_metrics, fp_metrics, fn_metrics]\n",
        "    \n",
        "    for cat, met in zip(categories, metrics):\n",
        "        logger.info(f\"{cat}:\")\n",
        "        logger.info(f\"  Source Text: {met['avg_text_length']['source']:.1f} characters\")\n",
        "        logger.info(f\"  Target Text: {met['avg_text_length']['target']:.1f} characters\")\n",
        "    \n",
        "    # Compare similarity scores\n",
        "    logger.info(\"Average Similarity Score Comparison:\")\n",
        "    for cat, met in zip(categories, metrics):\n",
        "        logger.info(f\"{cat}:\")\n",
        "        logger.info(f\"  Model 1: {met['avg_similarity_scores']['model1']:.3f}\")\n",
        "        logger.info(f\"  Model 2: {met['avg_similarity_scores']['model2']:.3f}\")\n",
        "        logger.info(f\"  TF-IDF: {met['avg_similarity_scores']['tfidf']:.3f}\")\n",
        "    \n",
        "    logger.debug(\"Impact analysis completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete impact analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [13] - Scatter Plot Analysis\n",
        "# Purpose: Create scatter plots to visualize distribution patterns of TP, FP, FN, and TN cases across models\n",
        "# Dependencies: matplotlib, numpy, seaborn, logger, pandas\n",
        "# Breadcrumbs: Complexity Analysis -> Visualization -> Distribution Pattern Analysis\n",
        "\n",
        "def create_scatter_plots():\n",
        "    \"\"\"\n",
        "    Create scatter plots showing the distribution of TP, FP, FN, and TN cases\n",
        "    with TP and FN on top layers for better visibility\n",
        "    \"\"\"\n",
        "    logger.debug(\"Starting scatter plot analysis\")\n",
        "    try:\n",
        "        # Prepare data for each category - order determines layer position (last items on top)\n",
        "        categories = {\n",
        "            'True Negative': (tn_analysis, 'blue', 'x'),\n",
        "            'False Positive': (fp_analysis, 'grey', '^'),\n",
        "            'False Negative': (fn_analysis, 'red', 's'),\n",
        "            'True Positive': (tp_analysis, 'green', 'o')\n",
        "        }\n",
        "        \n",
        "        # Create subplots for different model combinations\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
        "        fig.suptitle('Similarity Score Distribution by Prediction Category', fontsize=16)\n",
        "        \n",
        "        # Model combinations to plot - using correct column names\n",
        "        plot_configs = [\n",
        "            {\n",
        "                'x': 'Model 1',\n",
        "                'y': 'Model 2',\n",
        "                'title': 'Model1 vs Model2',\n",
        "                'pos': (0, 0)\n",
        "            },\n",
        "            {\n",
        "                'x': 'Model 1',\n",
        "                'y': 'Model 9',  # TFIDF is Model 9\n",
        "                'title': 'Model1 vs TF-IDF',\n",
        "                'pos': (0, 1)\n",
        "            },\n",
        "            {\n",
        "                'x': 'Model 2',\n",
        "                'y': 'Model 9',  # TFIDF is Model 9\n",
        "                'title': 'Model2 vs TF-IDF',\n",
        "                'pos': (1, 0)\n",
        "            },\n",
        "            {\n",
        "                'x': 'Model 3',\n",
        "                'y': 'Model 4',\n",
        "                'title': 'Model3 vs Model4',\n",
        "                'pos': (1, 1)\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        logger.debug(\"Creating scatter plots for model comparisons\")\n",
        "        # Create each subplot\n",
        "        for config in plot_configs:\n",
        "            logger.debug(f\"Creating plot for {config['title']}\")\n",
        "            ax = axes[config['pos'][0], config['pos'][1]]\n",
        "            \n",
        "            # Plot each category (order matters for layering)\n",
        "            for category, (data, color, marker) in categories.items():\n",
        "                # Check for valid data points\n",
        "                valid_mask = ~(np.isnan(data[config['x']]) | np.isnan(data[config['y']]))\n",
        "                if valid_mask.sum() == 0:\n",
        "                    logger.warning(f\"No valid data points for {category} in {config['title']}\")\n",
        "                    continue\n",
        "                \n",
        "                valid_data = data[valid_mask]\n",
        "                logger.debug(f\"Adding {len(valid_data)} {category} data points\")\n",
        "                \n",
        "                ax.scatter(\n",
        "                    valid_data[config['x']],\n",
        "                    valid_data[config['y']],\n",
        "                    c=color,\n",
        "                    marker=marker,\n",
        "                    label=category,\n",
        "                    alpha=0.6,\n",
        "                    s=50  # marker size\n",
        "                )\n",
        "                \n",
        "                # Log statistics for this category and model combination\n",
        "                logger.info(f\"Statistics for {category} in {config['title']}:\")\n",
        "                logger.info(f\"  Number of points: {len(valid_data)}\")\n",
        "                logger.info(f\"  {config['x']} mean: {valid_data[config['x']].mean():.3f}\")\n",
        "                logger.info(f\"  {config['y']} mean: {valid_data[config['y']].mean():.3f}\")\n",
        "            \n",
        "            # Customize plot appearance\n",
        "            ax.set_xlabel(config['x'])\n",
        "            ax.set_ylabel(config['y'])\n",
        "            ax.set_title(config['title'])\n",
        "            ax.grid(True, linestyle='--', alpha=0.7)\n",
        "            ax.legend()\n",
        "            \n",
        "            # Add correlation coefficient to plot\n",
        "            for category, (data, _, _) in categories.items():\n",
        "                valid_mask = ~(np.isnan(data[config['x']]) | np.isnan(data[config['y']]))\n",
        "                if valid_mask.sum() > 1:  # Need at least 2 points for correlation\n",
        "                    valid_data = data[valid_mask]\n",
        "                    correlation = valid_data[config['x']].corr(valid_data[config['y']])\n",
        "                    logger.info(f\"Correlation for {category} between {config['x']} and {config['y']}: {correlation:.3f}\")\n",
        "                else:\n",
        "                    logger.warning(f\"Insufficient data for correlation calculation in {category}\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        logger.debug(\"Scatter plots created successfully\")\n",
        "        \n",
        "        # Save plot with proper path\n",
        "        plot_filename = f\"similarity_scatter_plots_{project_name}_{timestamp}.png\"\n",
        "        plot_path = results['visualizations'] + \"/\" + plot_filename\n",
        "        plt.savefig(plot_path)\n",
        "        logger.info(f\"Scatter plots saved to {plot_path}\")\n",
        "        \n",
        "        plt.close()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating scatter plots\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "def create_density_plots():\n",
        "    \"\"\"\n",
        "    Create density plots for each model's score distribution\n",
        "    \"\"\"\n",
        "    logger.debug(\"Starting density plot analysis\")\n",
        "    try:\n",
        "        fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
        "        fig.suptitle('Score Distribution Density by Model and Category', fontsize=16)\n",
        "        \n",
        "        model_names = [\n",
        "            'Model 1', 'Model 2', 'Model 3',\n",
        "            'Model 4', 'Model 5', 'Model 6',\n",
        "            'Model 7', 'Model 8', 'Model 9'  # Model 9 is TFIDF\n",
        "        ]\n",
        "        \n",
        "        categories = {\n",
        "            'True Positive': (tp_analysis, 'green'),\n",
        "            'False Positive': (fp_analysis, 'grey'),\n",
        "            'False Negative': (fn_analysis, 'red'),\n",
        "            'True Negative': (tn_analysis, 'blue')\n",
        "        }\n",
        "        \n",
        "        for idx, model in enumerate(model_names):\n",
        "            logger.debug(f\"Creating density plot for {model}\")\n",
        "            ax = axes[idx // 3, idx % 3]\n",
        "            \n",
        "            for category, (data, color) in categories.items():\n",
        "                # Filter out invalid values and check for variance\n",
        "                valid_data = data[model].dropna()\n",
        "                if len(valid_data) < 2:\n",
        "                    logger.warning(f\"Insufficient data for {category} in {model}\")\n",
        "                    continue\n",
        "                    \n",
        "                if valid_data.var() == 0:\n",
        "                    logger.warning(f\"Zero variance in {category} for {model}\")\n",
        "                    continue\n",
        "                \n",
        "                try:\n",
        "                    sns.kdeplot(\n",
        "                        data=valid_data,\n",
        "                        ax=ax,\n",
        "                        label=category,\n",
        "                        color=color,\n",
        "                        warn_singular=False  # Suppress singular matrix warning\n",
        "                    )\n",
        "                    \n",
        "                    # Log statistics for this model and category\n",
        "                    logger.info(f\"Statistics for {category} in {model}:\")\n",
        "                    logger.info(f\"  Mean: {valid_data.mean():.3f}\")\n",
        "                    logger.info(f\"  Std: {valid_data.std():.3f}\")\n",
        "                    logger.info(f\"  Min: {valid_data.min():.3f}\")\n",
        "                    logger.info(f\"  Max: {valid_data.max():.3f}\")\n",
        "                except Exception as plot_error:\n",
        "                    logger.warning(f\"Could not create density plot for {category} in {model}: {str(plot_error)}\")\n",
        "            \n",
        "            ax.set_title(f\"{model} Score Distribution\")\n",
        "            ax.grid(True, linestyle='--', alpha=0.7)\n",
        "            ax.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        logger.debug(\"Density plots created successfully\")\n",
        "        \n",
        "        # Save plot with proper path\n",
        "        plot_filename = f\"similarity_density_plots_{project_name}_{timestamp}.png\"\n",
        "        plot_path = results['visualizations'] + \"/\" + plot_filename\n",
        "        plt.savefig(plot_path)\n",
        "        logger.info(f\"Density plots saved to {plot_path}\")\n",
        "        \n",
        "        plt.close()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating density plots\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting visualization analysis\")\n",
        "    logger.info(\"Creating scatter plots...\")\n",
        "    create_scatter_plots()\n",
        "    logger.info(\"Creating density plots...\")\n",
        "    create_density_plots()\n",
        "    logger.info(\"Visualization analysis completed\")\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete visualization analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [14] - Score Transformation Analysis\n",
        "# Purpose: Apply and analyze different mathematical transformations to improve score interpretation\n",
        "# Dependencies: numpy, pandas, matplotlib, seaborn, logger\n",
        "# Breadcrumbs: Distribution Pattern Analysis -> Transformation Analysis -> Score Optimization\n",
        "\n",
        "def apply_transformations(scores):\n",
        "    \"\"\"\n",
        "    Apply different transformations to similarity scores\n",
        "    \n",
        "    Args:\n",
        "        scores: numpy array of similarity scores\n",
        "    Returns:\n",
        "        dict: Dictionary of transformed scores\n",
        "    \"\"\"\n",
        "    logger.debug(\"Applying score transformations\")\n",
        "    try:\n",
        "        transformations = {\n",
        "            'log': np.log1p(scores),  # log1p to handle zeros\n",
        "            'exp': np.exp(scores) - 1,  # subtract 1 to maintain 0 baseline\n",
        "            'squared': np.square(scores),\n",
        "            'cubic': np.power(scores, 3),\n",
        "            'sqrt': np.sqrt(scores),\n",
        "            'sigmoid': 1 / (1 + np.exp(-10 * (scores - 0.5)))  # scaled sigmoid\n",
        "        }\n",
        "        \n",
        "        # Log transformation statistics\n",
        "        for name, transformed in transformations.items():\n",
        "            logger.debug(f\"{name.capitalize()} transformation stats:\")\n",
        "            logger.debug(f\"  Mean: {transformed.mean():.3f}\")\n",
        "            logger.debug(f\"  Std: {transformed.std():.3f}\")\n",
        "            logger.debug(f\"  Range: [{transformed.min():.3f}, {transformed.max():.3f}]\")\n",
        "            \n",
        "        return transformations\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error applying transformations\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "def plot_transformed_scores():\n",
        "    \"\"\"\n",
        "    Plot original vs transformed scores for different categories\n",
        "    \"\"\"\n",
        "    logger.debug(\"Starting transformed scores visualization\")\n",
        "    try:\n",
        "        # Combine all datasets with their categories\n",
        "        data_sources = {\n",
        "            'True Positive': tp_analysis['Model 1'],\n",
        "            'False Positive': fp_analysis['Model 1'],\n",
        "            'False Negative': fn_analysis['Model 1'],\n",
        "            'True Negative': tn_analysis['Model 1']\n",
        "        }\n",
        "        \n",
        "        all_data = []\n",
        "        for category, scores in data_sources.items():\n",
        "            valid_scores = scores.dropna()\n",
        "            if len(valid_scores) > 0:\n",
        "                df = pd.DataFrame({'score': valid_scores, 'category': category})\n",
        "                all_data.append(df)\n",
        "            else:\n",
        "                logger.warning(f\"No valid scores for {category}\")\n",
        "        \n",
        "        all_data = pd.concat(all_data)\n",
        "        logger.info(f\"Total samples for transformation analysis: {len(all_data)}\")\n",
        "        \n",
        "        # Create subplots for each transformation\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 15))\n",
        "        fig.suptitle('Score Transformations by Category', fontsize=16)\n",
        "        \n",
        "        # Color mapping for categories\n",
        "        colors = {\n",
        "            'True Positive': 'green',\n",
        "            'False Positive': 'grey',\n",
        "            'False Negative': 'red',\n",
        "            'True Negative': 'blue'\n",
        "        }\n",
        "        \n",
        "        # Plot each transformation\n",
        "        transformations = apply_transformations(all_data['score'].values)\n",
        "        for (name, transformed), ax in zip(transformations.items(), axes.flat):\n",
        "            logger.debug(f\"Creating plot for {name} transformation\")\n",
        "            \n",
        "            for category in colors.keys():\n",
        "                category_mask = all_data['category'] == category\n",
        "                if category_mask.sum() > 0:\n",
        "                    original = all_data.loc[category_mask, 'score']\n",
        "                    transformed_scores = transformed[category_mask]\n",
        "                    \n",
        "                    ax.scatter(\n",
        "                        original,\n",
        "                        transformed_scores,\n",
        "                        c=colors[category],\n",
        "                        label=category,\n",
        "                        alpha=0.6,\n",
        "                        s=50\n",
        "                    )\n",
        "                    \n",
        "                    # Log correlation between original and transformed scores\n",
        "                    correlation = np.corrcoef(original, transformed_scores)[0, 1]\n",
        "                    logger.info(f\"Correlation for {category} with {name} transformation: {correlation:.3f}\")\n",
        "            \n",
        "            ax.set_xlabel('Original Score')\n",
        "            ax.set_ylabel(f'{name.capitalize()} Score')\n",
        "            ax.set_title(f'{name.capitalize()} Transformation')\n",
        "            ax.grid(True, linestyle='--', alpha=0.7)\n",
        "            ax.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        logger.debug(\"Transformation plots created successfully\")\n",
        "        \n",
        "        # Save plot with proper path\n",
        "        plot_filename = f\"score_transformations_{project_name}_{timestamp}.png\"\n",
        "        plot_path = results['visualizations'] + \"/\" + plot_filename\n",
        "        plt.savefig(plot_path)\n",
        "        logger.info(f\"Transformation plots saved to {plot_path}\")\n",
        "        \n",
        "        # Save transformation statistics to CSV\n",
        "        stats_data = []\n",
        "        for category in colors.keys():\n",
        "            category_mask = all_data['category'] == category\n",
        "            if category_mask.sum() > 0:\n",
        "                original_scores = all_data.loc[category_mask, 'score']\n",
        "                \n",
        "                for name, transformed in transformations.items():\n",
        "                    transformed_scores = transformed[category_mask]\n",
        "                    stats = {\n",
        "                        'Category': category,\n",
        "                        'Transformation': name,\n",
        "                        'Original_Mean': original_scores.mean(),\n",
        "                        'Original_Std': original_scores.std(),\n",
        "                        'Transformed_Mean': transformed_scores.mean(),\n",
        "                        'Transformed_Std': transformed_scores.std(),\n",
        "                        'Correlation': np.corrcoef(original_scores, transformed_scores)[0, 1]\n",
        "                    }\n",
        "                    stats_data.append(stats)\n",
        "        \n",
        "        stats_df = pd.DataFrame(stats_data)\n",
        "        stats_filename = f\"transformation_statistics_{project_name}_{timestamp}.csv\"\n",
        "        stats_path = results['visualizations'] + \"/\" + stats_filename\n",
        "        stats_df.to_csv(stats_path, index=False)\n",
        "        logger.info(f\"Transformation statistics saved to {stats_path}\")\n",
        "        \n",
        "        plt.close()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating transformation plots\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting score transformation analysis\")\n",
        "    plot_transformed_scores()\n",
        "    logger.info(\"Score transformation analysis completed\")\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete transformation analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [15] - Log Scale Model Comparison\n",
        "# Purpose: Create log-scale visualizations to better understand model score distributions and relationships\n",
        "# Dependencies: matplotlib, numpy, seaborn, logger\n",
        "# Breadcrumbs: Score Optimization -> Scale Analysis -> Logarithmic Transformation\n",
        "\n",
        "def create_log_scale_comparisons():\n",
        "    \"\"\"\n",
        "    Create log-scale visualizations comparing different models\n",
        "    \"\"\"\n",
        "    logger.debug(\"Starting log scale comparison analysis\")\n",
        "    try:\n",
        "        # Create figure with multiple subplots\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "        fig.suptitle('Model Comparisons with Different Scale Transformations', fontsize=16)\n",
        "        \n",
        "        categories = {\n",
        "            'True Negative': (tn_analysis, 'blue', 'x'),\n",
        "            'False Positive': (fp_analysis, 'grey', '^'),\n",
        "            'False Negative': (fn_analysis, 'red', 's'),\n",
        "            'True Positive': (tp_analysis, 'green', 'o')\n",
        "        }\n",
        "        \n",
        "        logger.debug(\"Creating log10 scale comparison plot\")\n",
        "        # Plot 1: Log scale scatter\n",
        "        for category, (data, color, marker) in categories.items():\n",
        "            # Add validation for data\n",
        "            if len(data) == 0:\n",
        "                logger.warning(f\"No data available for {category}\")\n",
        "                continue\n",
        "                \n",
        "            valid_mask = (data['Model 1'] > 0) & (data['Model 2'] > 0)\n",
        "            if not valid_mask.any():\n",
        "                logger.warning(f\"No valid positive scores for {category}\")\n",
        "                continue\n",
        "                \n",
        "            valid_data = data[valid_mask]\n",
        "            \n",
        "            ax1.scatter(\n",
        "                np.log10(valid_data['Model 1']),\n",
        "                np.log10(valid_data['Model 2']),\n",
        "                c=color,\n",
        "                marker=marker,\n",
        "                label=category,\n",
        "                alpha=0.6\n",
        "            )\n",
        "            logger.info(f\"Log10 scale statistics for {category}:\")\n",
        "            logger.info(f\"  Points plotted: {len(valid_data)}\")\n",
        "            logger.info(f\"  Model 1 log10 range: [{np.log10(valid_data['Model 1']).min():.3f}, {np.log10(valid_data['Model 1']).max():.3f}]\")\n",
        "            logger.info(f\"  Model 2 log10 range: [{np.log10(valid_data['Model 2']).min():.3f}, {np.log10(valid_data['Model 2']).max():.3f}]\")\n",
        "            \n",
        "        ax1.set_title('Log10 Scale Comparison')\n",
        "        ax1.set_xlabel('Log10(Model 1 Score)')\n",
        "        ax1.set_ylabel('Log10(Model 2 Score)')\n",
        "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax1.legend()\n",
        "\n",
        "        logger.debug(\"Creating natural log scale comparison plot\")\n",
        "        # Plot 2: Natural log scale scatter\n",
        "        for category, (data, color, marker) in categories.items():\n",
        "            valid_mask = (data['Model 1'] > 0) & (data['Model 2'] > 0)\n",
        "            if not valid_mask.any():\n",
        "                continue\n",
        "                \n",
        "            valid_data = data[valid_mask]\n",
        "            \n",
        "            ax2.scatter(\n",
        "                np.log(valid_data['Model 1']),\n",
        "                np.log(valid_data['Model 2']),\n",
        "                c=color,\n",
        "                marker=marker,\n",
        "                label=category,\n",
        "                alpha=0.6\n",
        "            )\n",
        "            logger.info(f\"Natural log scale statistics for {category}:\")\n",
        "            logger.info(f\"  Points plotted: {len(valid_data)}\")\n",
        "            logger.info(f\"  Model 1 ln range: [{np.log(valid_data['Model 1']).min():.3f}, {np.log(valid_data['Model 1']).max():.3f}]\")\n",
        "            logger.info(f\"  Model 2 ln range: [{np.log(valid_data['Model 2']).min():.3f}, {np.log(valid_data['Model 2']).max():.3f}]\")\n",
        "            \n",
        "        ax2.set_title('Natural Log Scale Comparison')\n",
        "        ax2.set_xlabel('ln(Model 1 Score)')\n",
        "        ax2.set_ylabel('ln(Model 2 Score)')\n",
        "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax2.legend()\n",
        "\n",
        "        logger.debug(\"Creating density plots\")\n",
        "        # Plot 3: Log-scaled density plot for Model 1\n",
        "        for category, (data, color, _) in categories.items():\n",
        "            valid_data = data[data['Model 1'] > 0]\n",
        "            if len(valid_data) > 0:\n",
        "                try:\n",
        "                    sns.kdeplot(\n",
        "                        data=np.log10(valid_data['Model 1']),\n",
        "                        ax=ax3,\n",
        "                        label=category,\n",
        "                        color=color\n",
        "                    )\n",
        "                    logger.debug(f\"Created density plot for {category} Model 1\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Could not create density plot for {category} Model 1: {str(e)}\")\n",
        "\n",
        "        ax3.set_title('Log10 Score Distribution - Model 1')\n",
        "        ax3.set_xlabel('Log10(Score)')\n",
        "        ax3.set_ylabel('Density')\n",
        "        ax3.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax3.legend()\n",
        "\n",
        "        # Plot 4: Log-scaled density plot for Model 2\n",
        "        for category, (data, color, _) in categories.items():\n",
        "            valid_data = data[data['Model 2'] > 0]\n",
        "            if len(valid_data) > 0:\n",
        "                try:\n",
        "                    sns.kdeplot(\n",
        "                        data=np.log10(valid_data['Model 2']),\n",
        "                        ax=ax4,\n",
        "                        label=category,\n",
        "                        color=color\n",
        "                    )\n",
        "                    logger.debug(f\"Created density plot for {category} Model 2\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Could not create density plot for {category} Model 2: {str(e)}\")\n",
        "\n",
        "        ax4.set_title('Log10 Score Distribution - Model 2')\n",
        "        ax4.set_xlabel('Log10(Score)')\n",
        "        ax4.set_ylabel('Density')\n",
        "        ax4.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax4.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        logger.debug(\"All plots created successfully\")\n",
        "        \n",
        "        # Save plot with proper path\n",
        "        plot_filename = f\"log_scale_comparisons_{project_name}_{timestamp}.png\"\n",
        "        plot_path = results['visualizations'] + \"/\" + plot_filename\n",
        "        plt.savefig(plot_path)\n",
        "        logger.info(f\"Log scale comparison plots saved to {plot_path}\")\n",
        "        \n",
        "        # Save statistics to CSV\n",
        "        stats_data = []\n",
        "        for category, (data, _, _) in categories.items():\n",
        "            valid_data = data[(data['Model 1'] > 0) & (data['Model 2'] > 0)]\n",
        "            if len(valid_data) > 0:\n",
        "                stats = {\n",
        "                    'Category': category,\n",
        "                    'Count': len(valid_data),\n",
        "                    'Model1_Log10_Mean': np.log10(valid_data['Model 1']).mean(),\n",
        "                    'Model1_Log10_Std': np.log10(valid_data['Model 1']).std(),\n",
        "                    'Model2_Log10_Mean': np.log10(valid_data['Model 2']).mean(),\n",
        "                    'Model2_Log10_Std': np.log10(valid_data['Model 2']).std(),\n",
        "                    'Log10_Correlation': np.corrcoef(\n",
        "                        np.log10(valid_data['Model 1']),\n",
        "                        np.log10(valid_data['Model 2'])\n",
        "                    )[0, 1]\n",
        "                }\n",
        "                stats_data.append(stats)\n",
        "        \n",
        "        stats_df = pd.DataFrame(stats_data)\n",
        "        stats_filename = f\"log_scale_statistics_{project_name}_{timestamp}.csv\"\n",
        "        stats_path = results['visualizations'] + \"/\" + stats_filename\n",
        "        stats_df.to_csv(stats_path, index=False)\n",
        "        logger.info(f\"Log scale statistics saved to {stats_path}\")\n",
        "        \n",
        "        plt.close()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error creating log scale comparisons\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting log scale model comparison analysis\")\n",
        "    create_log_scale_comparisons()\n",
        "    logger.info(\"Log scale model comparison analysis completed\")\n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete log scale comparison analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [16] - Optimized Aggregate Score Calculation\n",
        "# Purpose: Create optimized aggregate scoring system using feature importance weights for LLM analysis\n",
        "# Dependencies: numpy, pandas, logger, feature_weights\n",
        "# Breadcrumbs: Logarithmic Transformation -> Aggregation Strategy -> Optimized Scoring\n",
        "\n",
        "def create_optimized_aggregate(source_text, target_text, model_dict=None, feature_weights=None):\n",
        "    \"\"\"\n",
        "    Create an optimized aggregate of similarity scores for LLM analysis\n",
        "    \n",
        "    Args:\n",
        "        source_text: Source requirement text\n",
        "        target_text: Target requirement text\n",
        "        model_dict: Dictionary of model objects with compute_similarity methods\n",
        "        feature_weights: DataFrame containing feature importance values from random forest\n",
        "    \n",
        "    Returns:\n",
        "        dict: Structured similarity analysis\n",
        "    \"\"\"\n",
        "    # Log the available feature importance values\n",
        "    if feature_weights is not None:\n",
        "        logger.debug(\"Available feature importance values:\")\n",
        "        for _, row in feature_weights.iterrows():\n",
        "            logger.debug(f\"Model: {row['feature']}, Importance: {row['importance']:.4f}\")\n",
        "    \n",
        "    # If no feature weights provided, use default weights\n",
        "    if feature_weights is None:\n",
        "        logger.warning(\"No feature weights provided, using default weights\")\n",
        "        weights = {\n",
        "            \"miniLM_similarity\": 0.1393,\n",
        "            \"distilbert_qa_similarity\": 0.1254,\n",
        "            \"tfidf_similarity\": 0.0998,\n",
        "            \"distilroberta_similarity\": 0.0967\n",
        "        }\n",
        "    else:\n",
        "        # Initialize weights with defaults\n",
        "        weights = {\n",
        "            \"miniLM_similarity\": 0.1393,\n",
        "            \"distilbert_qa_similarity\": 0.1254,\n",
        "            \"tfidf_similarity\": 0.0998,\n",
        "            \"distilroberta_similarity\": 0.0967\n",
        "        }\n",
        "        \n",
        "        # Map feature importance values to weights\n",
        "        model_name_mapping = {\n",
        "            'minilm': 'miniLM_similarity',\n",
        "            'qa-distilbert': 'distilbert_qa_similarity',\n",
        "            'tfidf': 'tfidf_similarity',\n",
        "            'distilroberta': 'distilroberta_similarity'\n",
        "        }\n",
        "        \n",
        "        # Update weights with actual values from feature importance\n",
        "        for _, row in feature_weights.iterrows():\n",
        "            model_name = row['feature'].lower()\n",
        "            for key, weight_key in model_name_mapping.items():\n",
        "                if key in model_name:\n",
        "                    weights[weight_key] = row['importance']\n",
        "                    logger.debug(f\"Updated weight for {weight_key}: {row['importance']:.4f}\")\n",
        "\n",
        "        logger.debug(f\"Final weights after mapping: {weights}\")\n",
        "\n",
        "    # For testing without models, generate random similarities\n",
        "    if model_dict is None:\n",
        "        logger.warning(\"No models provided, using random similarities for testing\")\n",
        "        aggregate = {\n",
        "            \"primary_signals\": {\n",
        "                \"miniLM_similarity\": np.random.uniform(0, 1),\n",
        "                \"distilbert_qa_similarity\": np.random.uniform(0, 1)\n",
        "            },\n",
        "            \"secondary_signals\": {\n",
        "                \"tfidf_similarity\": np.random.uniform(0, 1),\n",
        "                \"distilroberta_similarity\": np.random.uniform(0, 1)\n",
        "            },\n",
        "            \"confidence_metrics\": {\n",
        "                \"weighted_score\": None,\n",
        "                \"agreement_score\": None\n",
        "            }\n",
        "        }\n",
        "    else:\n",
        "        try:\n",
        "            aggregate = {\n",
        "                \"primary_signals\": {\n",
        "                    \"miniLM_similarity\": model_dict.compute_similarity(source_text, target_text, model_name=\"all-MiniLM-L6-v2\"),\n",
        "                    \"distilbert_qa_similarity\": model_dict.compute_similarity(source_text, target_text, model_name=\"multi-qa-distilbert-cos-v1\")\n",
        "                },\n",
        "                \"secondary_signals\": {\n",
        "                    \"tfidf_similarity\": model_dict.compute_similarity(source_text, target_text, model_name=\"tfidf\"),\n",
        "                    \"distilroberta_similarity\": model_dict.compute_similarity(source_text, target_text, model_name=\"all-distilroberta-v1\")\n",
        "                },\n",
        "                \"confidence_metrics\": {\n",
        "                    \"weighted_score\": None,\n",
        "                    \"agreement_score\": None\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error computing similarities: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    # Calculate weighted primary score using actual feature importances\n",
        "    primary_weight = weights['miniLM_similarity'] + weights['distilbert_qa_similarity']\n",
        "    weighted_primary = (\n",
        "        (aggregate[\"primary_signals\"][\"miniLM_similarity\"] * weights['miniLM_similarity'] + \n",
        "         aggregate[\"primary_signals\"][\"distilbert_qa_similarity\"] * weights['distilbert_qa_similarity']) \n",
        "        / primary_weight\n",
        "    )\n",
        "    \n",
        "    # Calculate weighted secondary score using actual feature importances\n",
        "    secondary_weight = weights['tfidf_similarity'] + weights['distilroberta_similarity']\n",
        "    weighted_secondary = (\n",
        "        (aggregate[\"secondary_signals\"][\"tfidf_similarity\"] * weights['tfidf_similarity'] + \n",
        "         aggregate[\"secondary_signals\"][\"distilroberta_similarity\"] * weights['distilroberta_similarity'])\n",
        "        / secondary_weight\n",
        "    )\n",
        "    \n",
        "    # Calculate final weighted score using the ratio of primary to secondary importance\n",
        "    total_primary = primary_weight\n",
        "    total_secondary = secondary_weight\n",
        "    total_weight = total_primary + total_secondary\n",
        "    \n",
        "    primary_ratio = total_primary / total_weight\n",
        "    secondary_ratio = total_secondary / total_weight\n",
        "    \n",
        "    aggregate[\"confidence_metrics\"][\"weighted_score\"] = (\n",
        "        weighted_primary * primary_ratio +\n",
        "        weighted_secondary * secondary_ratio\n",
        "    )\n",
        "    \n",
        "    # Calculate agreement score (how much models agree with each other)\n",
        "    scores = [\n",
        "        aggregate[\"primary_signals\"][\"miniLM_similarity\"],\n",
        "        aggregate[\"primary_signals\"][\"distilbert_qa_similarity\"],\n",
        "        aggregate[\"secondary_signals\"][\"tfidf_similarity\"],\n",
        "        aggregate[\"secondary_signals\"][\"distilroberta_similarity\"]\n",
        "    ]\n",
        "    aggregate[\"confidence_metrics\"][\"agreement_score\"] = 1 - np.std(scores)\n",
        "    \n",
        "    # Log the weights used\n",
        "    logger.debug(f\"Primary weight ratio: {primary_ratio:.3f}\")\n",
        "    logger.debug(f\"Secondary weight ratio: {secondary_ratio:.3f}\")\n",
        "    logger.debug(f\"Final weighted score: {aggregate['confidence_metrics']['weighted_score']:.3f}\")\n",
        "    \n",
        "    return aggregate\n",
        "\n",
        "def create_llm_prompt(aggregate, source_text, target_text, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Create an optimized prompt for LLM analysis\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Analyze the similarity between these two software requirements:\n",
        "\n",
        "Source Requirement: {source_text}\n",
        "\n",
        "Target Requirement: {target_text}\n",
        "\n",
        "Similarity Analysis:\n",
        "- Primary Similarity (MiniLM): {aggregate['primary_signals']['miniLM_similarity']:.3f}\n",
        "- QA-Focused Similarity (DistilBERT): {aggregate['primary_signals']['distilbert_qa_similarity']:.3f}\n",
        "- Text Similarity (TF-IDF): {aggregate['secondary_signals']['tfidf_similarity']:.3f}\n",
        "- Semantic Similarity (DistilRoBERTa): {aggregate['secondary_signals']['distilroberta_similarity']:.3f}\n",
        "\n",
        "Confidence Metrics:\n",
        "- Weighted Score: {aggregate['confidence_metrics']['weighted_score']:.3f}\n",
        "- Model Agreement: {aggregate['confidence_metrics']['agreement_score']:.3f}\n",
        "\n",
        "Based on these similarity scores and your analysis of the actual text content:\n",
        "1. Are these requirements related? Consider both semantic meaning and functional implications.\n",
        "2. What specific aspects make them similar or different?\n",
        "3. Confidence level in your assessment (high/medium/low)?\n",
        "\n",
        "Note: Our goal is to minimize false negatives (missing actual relationships) while maintaining reasonable precision.\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def evaluate_relationship(aggregate, llm_response, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Make final decision about requirement relationship\n",
        "    \"\"\"\n",
        "    weighted_score = aggregate[\"confidence_metrics\"][\"weighted_score\"]\n",
        "    agreement_score = aggregate[\"confidence_metrics\"][\"agreement_score\"]\n",
        "    \n",
        "    # High confidence positive match\n",
        "    if weighted_score > 0.6 and agreement_score > 0.8:\n",
        "        return True, \"high_confidence\"\n",
        "        \n",
        "    # Likely match needing review\n",
        "    elif weighted_score > threshold:\n",
        "        if \"high\" in llm_response[\"confidence\"].lower():\n",
        "            return True, \"llm_confirmed\"\n",
        "        else:\n",
        "            return True, \"needs_review\"\n",
        "            \n",
        "    # Potential false negative check\n",
        "    elif weighted_score > 0.2 and \"related\" in llm_response[\"decision\"].lower():\n",
        "        return True, \"llm_rescued\"\n",
        "        \n",
        "    return False, \"low_confidence\"\n",
        "\n",
        "try:\n",
        "    logger.info(\"Testing optimized aggregate score calculation\")\n",
        "    \n",
        "    # Log feature importance from Cell 3\n",
        "    logger.info(\"\\nFeature Importance from Random Forest:\")\n",
        "    for _, row in feature_importance.iterrows():\n",
        "        logger.info(f\"Model: {row['feature']:<30} Importance: {row['importance']:.4f}\")\n",
        "    \n",
        "    # Get a sample source and target text from our analysis results\n",
        "    sample_source_id = tp_analysis['Source ID'].iloc[0]\n",
        "    sample_target_id = tp_analysis['Target ID'].iloc[0]\n",
        "    \n",
        "    # Get the actual texts using our existing function\n",
        "    source_texts, target_texts = get_requirement_texts(\n",
        "        [sample_source_id], \n",
        "        [sample_target_id], \n",
        "        neo4j_client\n",
        "    )\n",
        "    \n",
        "    sample_source_text = source_texts[sample_source_id]\n",
        "    sample_target_text = target_texts[sample_target_id]\n",
        "    \n",
        "    logger.debug(\"Sample texts retrieved:\")\n",
        "    logger.debug(f\"Source: {sample_source_text[:100]}...\")\n",
        "    logger.debug(f\"Target: {sample_target_text[:100]}...\")\n",
        "    \n",
        "    # Create a mock model dictionary for testing\n",
        "    class MockModelDict:\n",
        "        def compute_similarity(self, source, target, model_name=None):\n",
        "            return np.random.uniform(0, 1)\n",
        "    \n",
        "    mock_model = MockModelDict()\n",
        "    \n",
        "    logger.info(\"Testing with mock model\")\n",
        "    aggregate_result = create_optimized_aggregate(\n",
        "        source_text=sample_source_text,\n",
        "        target_text=sample_target_text,\n",
        "        model_dict=mock_model,  # Use mock model instead of None\n",
        "        feature_weights=feature_importance\n",
        "    )\n",
        "    \n",
        "    # Log the results\n",
        "    logger.info(\"\\nAggregate Score Analysis Results:\")\n",
        "    logger.info(\"Primary Signals:\")\n",
        "    for signal, value in aggregate_result[\"primary_signals\"].items():\n",
        "        logger.info(f\"- {signal}: {value:.3f}\")\n",
        "    \n",
        "    logger.info(\"\\nSecondary Signals:\")\n",
        "    for signal, value in aggregate_result[\"secondary_signals\"].items():\n",
        "        logger.info(f\"- {signal}: {value:.3f}\")\n",
        "    \n",
        "    logger.info(\"\\nConfidence Metrics:\")\n",
        "    logger.info(f\"- Weighted Score: {aggregate_result['confidence_metrics']['weighted_score']:.3f}\")\n",
        "    logger.info(f\"- Agreement Score: {aggregate_result['confidence_metrics']['agreement_score']:.3f}\")\n",
        "    \n",
        "    # Create and log the LLM prompt\n",
        "    prompt = create_llm_prompt(aggregate_result, sample_source_text, sample_target_text)\n",
        "    logger.debug(\"\\nGenerated LLM Prompt:\")\n",
        "    logger.debug(prompt)\n",
        "    \n",
        "    # Simulate an LLM response for demonstration\n",
        "    mock_llm_response = {\n",
        "        \"decision\": \"related\",\n",
        "        \"confidence\": \"high\"\n",
        "    }\n",
        "    \n",
        "    # Evaluate the relationship\n",
        "    is_related, confidence_level = evaluate_relationship(\n",
        "        aggregate_result, \n",
        "        mock_llm_response\n",
        "    )\n",
        "    \n",
        "    logger.info(\"\\nFinal Evaluation:\")\n",
        "    logger.info(f\"Related: {is_related}\")\n",
        "    logger.info(f\"Confidence Level: {confidence_level}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Error testing optimized aggregate calculation\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [17] - Aggregate Score Analysis and Validation\n",
        "# Purpose: Analyze and validate the optimized aggregate scoring approach across all prediction categories\n",
        "# Dependencies: pandas, matplotlib, seaborn, tqdm, numpy, logger\n",
        "# Breadcrumbs: Optimized Scoring -> Validation Analysis -> Comprehensive Evaluation\n",
        "\n",
        "def analyze_aggregate_scores(tp_data, fp_data, fn_data, tn_data, feature_importance):\n",
        "    \"\"\"\n",
        "    Analyze and visualize how well the aggregate scoring approach differentiates between different cases\n",
        "    \n",
        "    Args:\n",
        "        tp_data: DataFrame containing true positive cases\n",
        "        fp_data: DataFrame containing false positive cases\n",
        "        fn_data: DataFrame containing false negative cases\n",
        "        tn_data: DataFrame containing true negative cases\n",
        "        feature_importance: DataFrame containing feature importance values\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting aggregate score analysis\")\n",
        "    \n",
        "    try:\n",
        "        # Create mock model for consistent testing\n",
        "        class MockModelDict:\n",
        "            def compute_similarity(self, source, target, model_name=None):\n",
        "                return np.random.uniform(0, 1)\n",
        "        \n",
        "        mock_model = MockModelDict()\n",
        "        \n",
        "        # Function to process a batch of cases\n",
        "        def process_cases(data, case_type):\n",
        "            logger.info(f\"Processing {len(data)} {case_type} cases\")\n",
        "            results = []\n",
        "            for idx, row in tqdm(data.iterrows(), total=len(data), desc=f\"Processing {case_type}\"):\n",
        "                source_id = row['Source ID']\n",
        "                target_id = row['Target ID']\n",
        "                \n",
        "                # Get texts\n",
        "                source_texts, target_texts = get_requirement_texts(\n",
        "                    [source_id], \n",
        "                    [target_id], \n",
        "                    neo4j_client\n",
        "                )\n",
        "                \n",
        "                # Calculate aggregate scores\n",
        "                aggregate = create_optimized_aggregate(\n",
        "                    source_text=source_texts[source_id],\n",
        "                    target_text=target_texts[target_id],\n",
        "                    model_dict=mock_model,\n",
        "                    feature_weights=feature_importance\n",
        "                )\n",
        "                \n",
        "                results.append({\n",
        "                    'case_type': case_type,\n",
        "                    'source_id': source_id,\n",
        "                    'target_id': target_id,\n",
        "                    'weighted_score': aggregate['confidence_metrics']['weighted_score'],\n",
        "                    'agreement_score': aggregate['confidence_metrics']['agreement_score'],\n",
        "                    'primary_avg': np.mean([\n",
        "                        aggregate['primary_signals']['miniLM_similarity'],\n",
        "                        aggregate['primary_signals']['distilbert_qa_similarity']\n",
        "                    ]),\n",
        "                    'secondary_avg': np.mean([\n",
        "                        aggregate['secondary_signals']['tfidf_similarity'],\n",
        "                        aggregate['secondary_signals']['distilroberta_similarity']\n",
        "                    ])\n",
        "                })\n",
        "                \n",
        "            return pd.DataFrame(results)\n",
        "        \n",
        "        # Process all cases\n",
        "        logger.info(\"Processing all cases:\")\n",
        "        logger.info(f\"True Positives: {len(tp_data)}\")\n",
        "        logger.info(f\"False Positives: {len(fp_data)}\")\n",
        "        logger.info(f\"False Negatives: {len(fn_data)}\")\n",
        "        logger.info(f\"True Negatives: {len(tn_data)}\")\n",
        "        \n",
        "        results_tp = process_cases(tp_data, 'True Positive')\n",
        "        results_fp = process_cases(fp_data, 'False Positive')\n",
        "        results_fn = process_cases(fn_data, 'False Negative')\n",
        "        results_tn = process_cases(tn_data, 'True Negative')\n",
        "        \n",
        "        # Combine results\n",
        "        all_results = pd.concat([results_tp, results_fp, results_fn, results_tn])\n",
        "        logger.info(f\"Total processed cases: {len(all_results)}\")\n",
        "        \n",
        "        # Create visualizations\n",
        "        plt.figure(figsize=(20, 15))\n",
        "        \n",
        "        # Define colors and markers for consistency\n",
        "        case_styles = {\n",
        "            'True Positive': ('green', 'o'),\n",
        "            'False Positive': ('red', '^'),\n",
        "            'False Negative': ('orange', 's'),\n",
        "            'True Negative': ('blue', 'x')\n",
        "        }\n",
        "        \n",
        "        # 1. Scatter plot of weighted score vs agreement score\n",
        "        plt.subplot(2, 2, 1)\n",
        "        for case_type, (color, marker) in case_styles.items():\n",
        "            mask = all_results['case_type'] == case_type\n",
        "            plt.scatter(\n",
        "                all_results[mask]['weighted_score'],\n",
        "                all_results[mask]['agreement_score'],\n",
        "                label=f\"{case_type} (n={sum(mask)})\",\n",
        "                color=color,\n",
        "                marker=marker,\n",
        "                alpha=0.6\n",
        "            )\n",
        "        plt.xlabel('Weighted Score')\n",
        "        plt.ylabel('Agreement Score')\n",
        "        plt.title('Weighted Score vs Agreement Score')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # 2. Primary vs Secondary Signal Scatter\n",
        "        plt.subplot(2, 2, 2)\n",
        "        for case_type, (color, marker) in case_styles.items():\n",
        "            mask = all_results['case_type'] == case_type\n",
        "            plt.scatter(\n",
        "                all_results[mask]['primary_avg'],\n",
        "                all_results[mask]['secondary_avg'],\n",
        "                label=f\"{case_type} (n={sum(mask)})\",\n",
        "                color=color,\n",
        "                marker=marker,\n",
        "                alpha=0.6\n",
        "            )\n",
        "        plt.xlabel('Average Primary Signal')\n",
        "        plt.ylabel('Average Secondary Signal')\n",
        "        plt.title('Primary vs Secondary Signals')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # 3. Score Distribution\n",
        "        plt.subplot(2, 2, 3)\n",
        "        for case_type, (color, _) in case_styles.items():\n",
        "            mask = all_results['case_type'] == case_type\n",
        "            sns.kdeplot(\n",
        "                data=all_results[mask]['weighted_score'],\n",
        "                label=f\"{case_type} (n={sum(mask)})\",\n",
        "                color=color,\n",
        "                fill=True,\n",
        "                alpha=0.3\n",
        "            )\n",
        "        plt.xlabel('Weighted Score')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Distribution of Weighted Scores')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # 4. Agreement Score Distribution\n",
        "        plt.subplot(2, 2, 4)\n",
        "        for case_type, (color, _) in case_styles.items():\n",
        "            mask = all_results['case_type'] == case_type\n",
        "            sns.kdeplot(\n",
        "                data=all_results[mask]['agreement_score'],\n",
        "                label=f\"{case_type} (n={sum(mask)})\",\n",
        "                color=color,\n",
        "                fill=True,\n",
        "                alpha=0.3\n",
        "            )\n",
        "        plt.xlabel('Agreement Score')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Distribution of Agreement Scores')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save plot\n",
        "        plot_filename = f\"aggregate_score_analysis_{project_name}_{timestamp}.png\"\n",
        "        plot_path = results['visualizations'] + \"/\" + plot_filename\n",
        "        plt.savefig(plot_path)\n",
        "        logger.info(f\"Analysis plots saved to {plot_path}\")\n",
        "        \n",
        "        # Calculate and log statistics\n",
        "        logger.info(\"\\nScore Statistics by Case Type:\")\n",
        "        stats_data = []\n",
        "        for case_type in case_styles.keys():\n",
        "            mask = all_results['case_type'] == case_type\n",
        "            case_data = all_results[mask]\n",
        "            stats = {\n",
        "                'Case Type': case_type,\n",
        "                'Count': len(case_data),\n",
        "                'Weighted Score Mean': case_data['weighted_score'].mean(),\n",
        "                'Weighted Score Std': case_data['weighted_score'].std(),\n",
        "                'Agreement Score Mean': case_data['agreement_score'].mean(),\n",
        "                'Agreement Score Std': case_data['agreement_score'].std(),\n",
        "                'Primary Signals Mean': case_data['primary_avg'].mean(),\n",
        "                'Primary Signals Std': case_data['primary_avg'].std(),\n",
        "                'Secondary Signals Mean': case_data['secondary_avg'].mean(),\n",
        "                'Secondary Signals Std': case_data['secondary_avg'].std()\n",
        "            }\n",
        "            stats_data.append(stats)\n",
        "            \n",
        "            logger.info(f\"\\n{case_type} (n={len(case_data)}):\")\n",
        "            logger.info(f\"Weighted Score: mean={stats['Weighted Score Mean']:.3f}, std={stats['Weighted Score Std']:.3f}\")\n",
        "            logger.info(f\"Agreement Score: mean={stats['Agreement Score Mean']:.3f}, std={stats['Agreement Score Std']:.3f}\")\n",
        "            logger.info(f\"Primary Signals: mean={stats['Primary Signals Mean']:.3f}, std={stats['Primary Signals Std']:.3f}\")\n",
        "            logger.info(f\"Secondary Signals: mean={stats['Secondary Signals Mean']:.3f}, std={stats['Secondary Signals Std']:.3f}\")\n",
        "        \n",
        "        # Save statistics to CSV\n",
        "        stats_df = pd.DataFrame(stats_data)\n",
        "        stats_filename = f\"aggregate_score_statistics_{project_name}_{timestamp}.csv\"\n",
        "        stats_path = results['visualizations'] + \"/\" + stats_filename\n",
        "        stats_df.to_csv(stats_path, index=False)\n",
        "        logger.info(f\"\\nDetailed statistics saved to {stats_path}\")\n",
        "        \n",
        "        return all_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(\"Error in aggregate score analysis\", exc_info=True)\n",
        "        handle_exception(e)\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    logger.info(\"Starting aggregate score analysis visualization\")\n",
        "    analysis_results = analyze_aggregate_scores(\n",
        "        tp_analysis,\n",
        "        fp_analysis,\n",
        "        fn_analysis,\n",
        "        tn_analysis,\n",
        "        feature_importance\n",
        "    )\n",
        "    logger.info(\"Aggregate score analysis visualization completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(\"Failed to complete aggregate score analysis\", exc_info=True)\n",
        "    handle_exception(e)\n",
        "    raise"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}