{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Multi-Modal Requirements Analysis Pipeline\n",
        "**Combines sentence transformer similarity scores with machine learning classifiers and Claude AI analysis for advanced requirement association detection.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [0] - Setup and Imports\n",
        "# Purpose: Import all required libraries and configure environment settings for Multi-LLM testing\n",
        "# Dependencies: os, sys, json, pathlib, dotenv, datetime, pandas, praxis_sentence_transformer\n",
        "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, fbeta_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Import from praxis_sentence_transformer package (installed via pip)\n",
        "from praxis_sentence_transformer.neo4j_operations import Neo4jClient\n",
        "from praxis_sentence_transformer.analysis.analyzer import RequirementsAnalyzer\n",
        "from praxis_sentence_transformer.clients.claude import ClaudeRequirementAnalyzer\n",
        "from praxis_sentence_transformer.logger import setup_logging, DebugTimer\n",
        "from praxis_sentence_transformer.visualization import RequirementsVisualizer\n",
        "\n",
        "# Set up logging\n",
        "logger = setup_logging(\"neo4j-notebook\", logging.ERROR)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [1] - Environment Variables Validation and Neo4j Client Initialization\n",
        "# Purpose: Validate environment variables and establish Neo4j database connection\n",
        "# Dependencies: os, logger, Neo4jClient, sys\n",
        "# Breadcrumbs: Environment Configuration -> Validation -> Database Connection\n",
        "\n",
        "# Display loaded environment variables (without showing sensitive values)\n",
        "env_vars = {\n",
        "    'NEO4J_URI': '✓' if os.getenv('NEO4J_URI') else '✗',\n",
        "    'NEO4J_USER': '✓' if os.getenv('NEO4J_USER') else '✗',\n",
        "    'NEO4J_PASSWORD': '✓' if os.getenv('NEO4J_PASSWORD') else '✗',\n",
        "    'ANTHROPIC_API_KEY': '✓' if os.getenv('ANTHROPIC_API_KEY') else '✗',\n",
        "    'HF_TOKEN': '✓' if os.getenv('HF_TOKEN') else '✗'\n",
        "}\n",
        "\n",
        "logger.info(\"Environment variables loaded:\")\n",
        "for var, status in env_vars.items():\n",
        "    logger.info(f\"{var}: {status}\")\n",
        "\n",
        "# Get Neo4j credentials from environment variables\n",
        "neo4j_uri = os.getenv('NEO4J_URI')\n",
        "neo4j_user = os.getenv('NEO4J_USER')\n",
        "neo4j_password = os.getenv('NEO4J_PASSWORD')\n",
        "\n",
        "# Verify all required credentials are present\n",
        "if not all([neo4j_uri, neo4j_user, neo4j_password]):\n",
        "    logger.error(\"Missing required Neo4j credentials in environment variables\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Initialize Neo4j Client with credentials\n",
        "neo4j_client = Neo4jClient(\n",
        "    uri=neo4j_uri,\n",
        "    username=neo4j_user,\n",
        "    password=neo4j_password\n",
        ")\n",
        "\n",
        "# Test connection\n",
        "if not neo4j_client.connect():\n",
        "    logger.error(\"Failed to connect to Neo4j database\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [2] - Model Configuration and Results Directory Setup\n",
        "# Purpose: Configure model parameters and create output directories for analysis results\n",
        "# Dependencies: Path, logger, os\n",
        "# Breadcrumbs: Database Connection -> Configuration -> Parameter Setup\n",
        "\n",
        "# Model configuration\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "alpha = 0.6\n",
        "threshold = 0.4\n",
        "project_name = os.getenv('PROJECT_NAME')\n",
        "\n",
        "# Create results directory\n",
        "results_dir = Path(\"../results/neo4j_analysis\")\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logger.info(f\"Using model: {model_name}\")\n",
        "logger.info(f\"Alpha: {alpha}\")\n",
        "logger.info(f\"Threshold: {threshold}\")\n",
        "logger.info(f\"Project Name: {project_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [3] - Similarity Data Retrieval and Analysis\n",
        "# Purpose: Retrieve and analyze similarity data from Neo4j database for requirement pairs\n",
        "# Dependencies: neo4j_client, logger, pandas, json\n",
        "# Breadcrumbs: Parameter Setup -> Data Retrieval -> Similarity Analysis\n",
        "\n",
        "# Get similarity data\n",
        "similarity_results = neo4j_client.get_requirement_similarity_data(project_name=project_name)\n",
        "\n",
        "# Log summary statistics\n",
        "logger.info(f\"Project: {similarity_results['metadata']['project_name']}\")\n",
        "logger.info(f\"Total pairs analyzed: {similarity_results['metadata']['total_pairs']}\")\n",
        "logger.info(f\"Ground truth pairs: {similarity_results['metadata']['ground_truth_pairs']}\")\n",
        "logger.info(f\"Models found: {similarity_results['metadata']['models']}\")\n",
        "\n",
        "# Show sample data for first few requirement pairs\n",
        "df = similarity_results['data']\n",
        "for _, row in df.head(3).iterrows():  # First 3 pairs\n",
        "    logger.info(f\"\\nSource {row['source_id']}:\")\n",
        "    logger.info(f\"Content: {row['source_content'][:100]}...\")\n",
        "    logger.info(f\"Target {row['target_id']}:\")\n",
        "    logger.info(f\"Content: {row['target_content'][:100]}...\")\n",
        "    logger.info(f\"Ground Truth: {row['is_ground_truth']}\")\n",
        "    \n",
        "    # Log similarity scores for each model\n",
        "    for model in similarity_results['metadata']['models']:\n",
        "        if model in row:  # Check if model exists in row\n",
        "            logger.info(f\"{model} similarity: {row[model]:.3f}\")\n",
        "\n",
        "# Create sample structure with first 3 pairs\n",
        "sample_data = {\n",
        "    \"metadata\": similarity_results[\"metadata\"],\n",
        "    \"pairs\": [\n",
        "        {\n",
        "            \"source\": {\n",
        "                \"id\": row['source_id'],\n",
        "                \"content\": row['source_content']\n",
        "            },\n",
        "            \"target\": {\n",
        "                \"id\": row['target_id'],\n",
        "                \"content\": row['target_content']\n",
        "            },\n",
        "            \"is_ground_truth\": row['is_ground_truth'],\n",
        "            \"model_scores\": {\n",
        "                model: row[model] \n",
        "                for model in similarity_results['metadata']['models']\n",
        "                if model in row and not pd.isna(row[model])\n",
        "            }\n",
        "        }\n",
        "        for _, row in df.head(3).iterrows()\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Log sample structure\n",
        "logger.debug(\"Sample data structure:\")\n",
        "logger.debug(json.dumps(sample_data, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [4] - Similarity Results Display\n",
        "# Purpose: Display the retrieved similarity results for inspection and validation\n",
        "# Dependencies: similarity_results\n",
        "# Breadcrumbs: Similarity Analysis -> Data Inspection -> Results Validation\n",
        "\n",
        "similarity_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [5] - DataFrame Head Inspection\n",
        "# Purpose: Inspect the first few rows of the DataFrame for data structure validation\n",
        "# Dependencies: df (DataFrame from similarity_results)\n",
        "# Breadcrumbs: Results Validation -> Data Structure -> DataFrame Inspection\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [6] - DataFrame Size Check\n",
        "# Purpose: Check the total number of records in the DataFrame for dataset size validation\n",
        "# Dependencies: df\n",
        "# Breadcrumbs: DataFrame Inspection -> Size Validation -> Data Quality Check\n",
        "\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [7] - Correlation Analysis\n",
        "# Purpose: Analyze correlations between model scores and ground truth labels for feature evaluation\n",
        "# Dependencies: pandas, df\n",
        "# Breadcrumbs: Data Quality Check -> Statistical Analysis -> Correlation Assessment\n",
        "\n",
        "# Assuming df is your DataFrame containing the data\n",
        "\n",
        "# Drop non-numeric columns\n",
        "numeric_df = df.drop(columns=['source_id', 'target_id', 'source_content', 'target_content'])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "# Get the correlation with 'is_ground_truth'\n",
        "ground_truth_correlation = correlation_matrix['is_ground_truth'].drop('is_ground_truth')\n",
        "print(ground_truth_correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [8] - Logistic Regression Model Training and Evaluation\n",
        "# Purpose: Train and evaluate logistic regression model for requirement similarity classification\n",
        "# Dependencies: sklearn, matplotlib, pandas, logger\n",
        "# Breadcrumbs: Correlation Assessment -> Model Training -> Logistic Regression Analysis\n",
        "\n",
        "# Prepare the data\n",
        "X = df.drop(columns=['is_ground_truth', 'source_id', 'target_id', 'source_content', 'target_content'])\n",
        "y = df['is_ground_truth'].astype(int)\n",
        "\n",
        "# Calculate and log the correlation matrix\n",
        "correlation_matrix = X.corr()\n",
        "logger.info(\"Correlation matrix:\")\n",
        "logger.debug(correlation_matrix.to_string())\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the sizes of the training and testing sets\n",
        "logger.info(f\"Training set size: {X_train.shape[0]}, Testing set size: {X_test.shape[0]}\")\n",
        "\n",
        "# Train the model with class weights\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate F2 score\n",
        "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
        "print(f\"F2 Score: {f2_score:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "logger.info(\"Confusion Matrix:\")\n",
        "logger.debug(conf_matrix)\n",
        "\n",
        "# Extract TP, FN, FP, TN from the confusion matrix\n",
        "TN, FP, FN, TP = conf_matrix.ravel()\n",
        "logger.info(f\"True Positives (TP): {TP}\")\n",
        "logger.info(f\"False Negatives (FN): {FN}\")\n",
        "logger.info(f\"False Positives (FP): {FP}\")\n",
        "logger.info(f\"True Negatives (TN): {TN}\")\n",
        "\n",
        "# Verify the total count in the confusion matrix\n",
        "total_count = conf_matrix.sum()\n",
        "logger.info(f\"Total count in confusion matrix: {total_count}, Expected: {y_test.shape[0]}\")\n",
        "\n",
        "# Feature importance analysis\n",
        "coefficients = model.coef_[0]  # Get the coefficients for the logistic regression model\n",
        "feature_importance = pd.Series(coefficients, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Display the feature importance\n",
        "print(\"Feature Importance (Sentence Transformers):\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Optionally, plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importance.plot(kind='bar')\n",
        "plt.title('Feature Importance of Sentence Transformers')\n",
        "plt.xlabel('Sentence Transformers')\n",
        "plt.ylabel('Coefficient Value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [9] - Random Forest Model Training and Feature Importance\n",
        "# Purpose: Train Random Forest classifier and analyze feature importance for model interpretation\n",
        "# Dependencies: sklearn, pandas, matplotlib\n",
        "# Breadcrumbs: Logistic Regression Analysis -> Ensemble Methods -> Random Forest Analysis\n",
        "\n",
        "# Prepare the data\n",
        "X = df.drop(columns=['is_ground_truth', 'source_id', 'target_id', 'source_content', 'target_content'])\n",
        "y = df['is_ground_truth'].astype(int)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate F2 score\n",
        "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
        "print(f\"F2 Score: {f2_score:.4f}\")\n",
        "\n",
        "# Feature importance analysis\n",
        "importances = rf_model.feature_importances_\n",
        "feature_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Display the feature importance\n",
        "print(\"Feature Importance (Sentence Transformers):\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Optionally, plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importance.plot(kind='bar')\n",
        "plt.title('Feature Importance of Sentence Transformers')\n",
        "plt.xlabel('Sentence Transformers')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [10] - Feature Selection and Model Evaluation with Top Features\n",
        "# Purpose: Evaluate model performance using only top-ranked features for optimization\n",
        "# Dependencies: sklearn, pandas, matplotlib\n",
        "# Breadcrumbs: Random Forest Analysis -> Feature Selection -> Top Features Evaluation\n",
        "\n",
        "# Prepare the data\n",
        "X = df.drop(columns=['is_ground_truth', 'source_id', 'target_id', 'source_content', 'target_content'])\n",
        "y = df['is_ground_truth'].astype(int)\n",
        "\n",
        "# Train the initial Random Forest model to get feature importances\n",
        "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# Feature importance analysis\n",
        "importances = rf_model.feature_importances_\n",
        "feature_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Select top N features (e.g., top 5)\n",
        "top_n = 5\n",
        "top_features = feature_importance.head(top_n).index.tolist()\n",
        "\n",
        "# Create a new DataFrame with only the top features\n",
        "X_top = X[top_features]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Random Forest model with top features\n",
        "rf_model_top = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf_model_top.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_top = rf_model_top.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification report for model with top features:\")\n",
        "print(classification_report(y_test, y_pred_top))\n",
        "\n",
        "# Calculate F2 score\n",
        "f2_score_top = fbeta_score(y_test, y_pred_top, beta=2)\n",
        "print(f\"F2 Score with top features: {f2_score_top:.4f}\")\n",
        "\n",
        "# Display feature importance for the top features\n",
        "print(\"Feature Importance (Top Features):\")\n",
        "print(feature_importance[top_features])\n",
        "\n",
        "# Optionally, plot the feature importance for the top features\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importance[top_features].plot(kind='bar')\n",
        "plt.title('Feature Importance of Top Sentence Transformers')\n",
        "plt.xlabel('Sentence Transformers')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [11] - F1 and F2 Score Evaluation for Top N Features\n",
        "# Purpose: Systematically evaluate F1 and F2 scores across different numbers of top features\n",
        "# Dependencies: sklearn, pandas, matplotlib\n",
        "# Breadcrumbs: Top Features Evaluation -> Performance Metrics -> Score Optimization\n",
        "\n",
        "# Prepare the data\n",
        "X = df.drop(columns=['is_ground_truth', 'source_id', 'target_id', 'source_content', 'target_content'])\n",
        "y = df['is_ground_truth'].astype(int)\n",
        "\n",
        "# Train an initial Random Forest model to get feature importances\n",
        "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# Feature importance analysis\n",
        "importances = rf_model.feature_importances_\n",
        "feature_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Evaluate models for top N features\n",
        "for n in range(1, len(feature_importance) + 1):\n",
        "    # Select top N features\n",
        "    top_features = feature_importance.head(n).index.tolist()\n",
        "    X_top = X[top_features]\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the Random Forest model with top features\n",
        "    rf_model_top = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "    rf_model_top.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_top = rf_model_top.predict(X_test)\n",
        "\n",
        "    # Calculate F1 and F2 scores\n",
        "    f1_score = fbeta_score(y_test, y_pred_top, beta=1)\n",
        "    f2_score = fbeta_score(y_test, y_pred_top, beta=2)\n",
        "\n",
        "    # Store the results\n",
        "    results.append({'N': n, 'F1 Score': f1_score, 'F2 Score': f2_score})\n",
        "\n",
        "# Convert results to DataFrame for easier analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display results\n",
        "print(results_df)\n",
        "\n",
        "# Plot F1 and F2 scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(results_df['N'], results_df['F1 Score'], marker='o', label='F1 Score', color='blue')\n",
        "plt.plot(results_df['N'], results_df['F2 Score'], marker='o', label='F2 Score', color='orange')\n",
        "plt.title('F1 and F2 Scores for Top N Features')\n",
        "plt.xlabel('Number of Top Features (N)')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(results_df['N'])\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [12] - Principal Component Analysis Visualization\n",
        "# Purpose: Apply PCA for dimensionality reduction and visualize data distribution patterns\n",
        "# Dependencies: sklearn.decomposition, matplotlib\n",
        "# Breadcrumbs: Score Optimization -> Dimensionality Reduction -> PCA Visualization\n",
        "\n",
        "# Prepare the data\n",
        "X = df.drop(columns=['is_ground_truth', 'source_id', 'target_id', 'source_content', 'target_content'])\n",
        "y = df['is_ground_truth'].astype(int)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
        "plt.title('PCA of Sentence Transformers')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(label='is_ground_truth')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell [13] - Claude AI Integration and Mapping Processing\n",
        "# Purpose: Initialize Claude AI analyzer and process requirement mappings for enhanced analysis\n",
        "# Dependencies: os, ClaudeRequirementAnalyzer, DebugTimer, logger, pandas\n",
        "# Breadcrumbs: PCA Visualization -> AI Enhancement -> Claude Integration\n",
        "\n",
        "# Get configuration from environment\n",
        "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "claude_model = os.getenv(\"CLAUDE_3_5_MODEL\")\n",
        "min_association_prob = float(os.getenv(\"MIN_ASSOCIATION_PROBABILITY\", 0.6))\n",
        "\n",
        "# Validate environment variables\n",
        "if not anthropic_api_key:\n",
        "    raise ValueError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
        "if not claude_model:\n",
        "    raise ValueError(\"CLAUDE_3_5_MODEL not found in environment variables\")\n",
        "\n",
        "logger.debug(\"Retrieved configuration from environment variables\")\n",
        "\n",
        "# Initialize Claude analyzer\n",
        "try:\n",
        "    claude_analyzer = ClaudeRequirementAnalyzer(\n",
        "        api_key=anthropic_api_key,\n",
        "        model_name=claude_model,\n",
        "        min_association_probability=min_association_prob\n",
        "    )\n",
        "    logger.info(f\"Successfully initialized Claude analyzer with model: {claude_model}\")\n",
        "    \n",
        "    # Log the system prompt being used\n",
        "    logger.debug(f\"Using system prompt:\\n{claude_analyzer.system_prompt}\")\n",
        "    \n",
        "    # Process mappings with Claude\n",
        "    logger.info(\"Starting mapping processing with Claude...\")\n",
        "    timer = DebugTimer(\"Processing mappings with Claude\")\n",
        "    timer.start()\n",
        "    \n",
        "    test_mode = os.getenv(\"TEST_MODE\", \"false\").strip().lower() in [\"true\", \"1\", \"t\", \"yes\"]\n",
        "    if test_mode:\n",
        "        logger.info(\"Test mode is enabled. Processing first 2 source requirements with their first 12 targets each.\")\n",
        "        # Get unique source IDs\n",
        "        unique_sources = similarity_results['data']['source_id'].unique()\n",
        "        # Take first 2 source IDs\n",
        "        test_sources = unique_sources[:2]\n",
        "        \n",
        "        # Filter for first 2 sources and their first 12 targets each\n",
        "        filtered_data = []\n",
        "        for source in test_sources:\n",
        "            source_data = similarity_results['data'][similarity_results['data']['source_id'] == source]\n",
        "            filtered_data.append(source_data.head(12))\n",
        "        \n",
        "        # Combine the filtered data\n",
        "        test_data = pd.concat(filtered_data)\n",
        "        \n",
        "        # Create test similarity results with original metadata\n",
        "        test_similarity_results = {\n",
        "            'metadata': similarity_results['metadata'],\n",
        "            'data': test_data\n",
        "        }\n",
        "        \n",
        "        similarity_results = test_similarity_results\n",
        "        logger.info(f\"Filtered to {len(test_data)} pairs for testing\")\n",
        "    else:\n",
        "        logger.info(\"Test mode is disabled. Processing all source requirements.\")\n",
        "    \n",
        "    # Pass Neo4j client to process_mappings\n",
        "    claude_requirements_results_set = claude_analyzer.process_mappings(\n",
        "        mappings=similarity_results,  # This contains the model names in metadata\n",
        "        neo4j_client=neo4j_client\n",
        "    )\n",
        "    \n",
        "    timer.end()\n",
        "    logger.info(f\"Processing completed in {timer.duration:.2f} seconds\")\n",
        "    \n",
        "    # Log summary statistics\n",
        "    logger.info(f\"Processed {claude_requirements_results_set.total_target_matches} requirement matches\")\n",
        "    logger.info(f\"Found {claude_requirements_results_set.total_associated_matches} associated matches\")\n",
        "    \n",
        "    # Show detailed debug info about some matches\n",
        "    if claude_requirements_results_set.processed_matches:\n",
        "        logger.info(\"Sample of processed matches:\")\n",
        "        for i, match in enumerate(claude_requirements_results_set.processed_matches[:2]):\n",
        "            similarity_score = f\"{match.similarity_score:.3f}\" if match.similarity_score is not None else \"N/A\"\n",
        "            logger.debug(f\"\"\"\n",
        "            Match {i+1}:\n",
        "            Source ID: {match.source_id}\n",
        "            Source Content: {match.source_content[:100]}...\n",
        "            Target ID: {match.target_id}\n",
        "            Target Content: {match.target_content[:100]}...\n",
        "            Similarity Score: {similarity_score}\n",
        "            Association Probability: {match.association_probability:.3f}\n",
        "            Is Associated: {match.is_associated}\n",
        "            Explanation: {match.explanation}\n",
        "            \"\"\")\n",
        "        # Print summary of results\n",
        "        logger.info(\"\\nClaude Analysis Summary:\")\n",
        "        logger.info(f\"Total source requirements processed: {claude_requirements_results_set.total_source_requirements}\")\n",
        "        logger.info(f\"Total target matches analyzed: {claude_requirements_results_set.total_target_matches}\")\n",
        "        logger.info(f\"Total associated matches found: {claude_requirements_results_set.total_associated_matches}\")\n",
        "\n",
        "        # Print breakdown by source requirement\n",
        "        logger.info(\"\\nBreakdown by source requirement:\")\n",
        "        source_counts = {}\n",
        "        associated_counts = {}\n",
        "\n",
        "        for match in claude_requirements_results_set.processed_matches:\n",
        "            source_counts[match.source_id] = source_counts.get(match.source_id, 0) + 1\n",
        "            if match.is_associated:\n",
        "                associated_counts[match.source_id] = associated_counts.get(match.source_id, 0) + 1\n",
        "\n",
        "        for source_id, count in source_counts.items():\n",
        "            associated = associated_counts.get(source_id, 0)\n",
        "            logger.info(f\"Source {source_id}:\")\n",
        "            logger.info(f\"  - Total target matches: {count}\")\n",
        "            logger.info(f\"  - Associated matches: {associated}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error in Claude analysis: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [NEW CELL] - Select Top Transformers and Process with Claude\n",
        "\n",
        "logger = setup_logging(__name__, logging.INFO)\n",
        "\n",
        "# First, identify top performing transformers using Random Forest\n",
        "logger.info(\"Analyzing transformer performance with Random Forest...\")\n",
        "\n",
        "# Prepare the data - exclude non-transformer columns\n",
        "X = df.drop(columns=['is_ground_truth', 'source_id', 'target_id', 'source_content', 'target_content'])\n",
        "y = df['is_ground_truth'].astype(int)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# Get feature importance and select top 5 transformers\n",
        "feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "top_5_transformers = feature_importance.head(5).index.tolist()\n",
        "\n",
        "logger.info(f\"Selected top 5 transformers based on importance:\")\n",
        "for i, transformer in enumerate(top_5_transformers, 1):\n",
        "    logger.info(f\"{i}. {transformer} (importance: {feature_importance[transformer]:.4f})\")\n",
        "\n",
        "# Get configuration from environment\n",
        "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "claude_model = os.getenv(\"CLAUDE_3_5_MODEL\")\n",
        "min_association_prob = float(os.getenv(\"MIN_ASSOCIATION_PROBABILITY\", 0.6))\n",
        "\n",
        "# Validate environment variables\n",
        "if not anthropic_api_key:\n",
        "    raise ValueError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
        "if not claude_model:\n",
        "    raise ValueError(\"CLAUDE_3_5_MODEL not found in environment variables\")\n",
        "\n",
        "logger.debug(\"Retrieved configuration from environment variables\")\n",
        "\n",
        "# Initialize Claude analyzer with top transformers\n",
        "try:\n",
        "    claude_analyzer = ClaudeRequirementAnalyzer(\n",
        "        api_key=anthropic_api_key,\n",
        "        model_name=claude_model,\n",
        "        min_association_probability=min_association_prob,\n",
        "        transformer_names=top_5_transformers  # Pass the top transformers\n",
        "    )\n",
        "    logger.info(f\"Successfully initialized Claude analyzer with model: {claude_model}\")\n",
        "    \n",
        "    # Log the system prompt being used\n",
        "    logger.debug(f\"Using system prompt:\\n{claude_analyzer.system_prompt}\")\n",
        "    \n",
        "    # Process mappings with Claude using selected transformers\n",
        "    logger.info(\"Starting mapping processing with selected transformers...\")\n",
        "    timer = DebugTimer(\"Processing mappings with selected transformers\")\n",
        "    timer.start()\n",
        "    \n",
        "    test_mode = os.getenv(\"TEST_MODE\", \"false\").strip().lower() in [\"true\", \"1\", \"t\", \"yes\"]\n",
        "    if test_mode:\n",
        "        logger.info(\"Test mode is enabled. Processing first 2 source requirements with their first 12 targets each.\")\n",
        "        # Get unique source IDs\n",
        "        unique_sources = similarity_results['data']['source_id'].unique()\n",
        "        # Take first 2 source IDs\n",
        "        test_sources = unique_sources[:2]\n",
        "        \n",
        "        # Filter for first 2 sources and their first 12 targets each\n",
        "        filtered_data = []\n",
        "        for source in test_sources:\n",
        "            source_data = similarity_results['data'][similarity_results['data']['source_id'] == source]\n",
        "            filtered_data.append(source_data.head(12))\n",
        "        \n",
        "        # Combine the filtered data\n",
        "        test_data = pd.concat(filtered_data)\n",
        "        \n",
        "        # Create test similarity results with original metadata\n",
        "        test_similarity_results = {\n",
        "            'metadata': similarity_results['metadata'],\n",
        "            'data': test_data\n",
        "        }\n",
        "        \n",
        "        similarity_results = test_similarity_results\n",
        "        logger.info(f\"Filtered to {len(test_data)} pairs for testing\")\n",
        "    else:\n",
        "        logger.info(\"Test mode is disabled. Processing all source requirements.\")\n",
        "    \n",
        "    # Use the new process_mappings_with_sentence_transformers function\n",
        "    claude_requirements_results_set = claude_analyzer.process_mappings_with_sentence_transformers(\n",
        "        mappings=similarity_results,\n",
        "        sentence_transformers=top_5_transformers,\n",
        "        neo4j_client=neo4j_client\n",
        "    )\n",
        "    \n",
        "    timer.end()\n",
        "    logger.info(f\"Processing completed in {timer.duration:.2f} seconds\")\n",
        "    \n",
        "    # Log summary statistics\n",
        "    logger.info(f\"Processed {claude_requirements_results_set.total_target_matches} requirement matches\")\n",
        "    logger.info(f\"Found {claude_requirements_results_set.total_associated_matches} associated matches\")\n",
        "    \n",
        "    # Show detailed debug info about some matches\n",
        "    if claude_requirements_results_set.processed_matches:\n",
        "        logger.info(\"Sample of processed matches:\")\n",
        "        for i, match in enumerate(claude_requirements_results_set.processed_matches[:2]):\n",
        "            logger.debug(f\"\"\"\n",
        "            Match {i+1}:\n",
        "            Source ID: {match.source_id}\n",
        "            Source Content: {match.source_content[:100]}...\n",
        "            Target ID: {match.target_id}\n",
        "            Target Content: {match.target_content[:100]}...\n",
        "            Association Probability: {match.association_probability:.3f}\n",
        "            Is Associated: {match.is_associated}\n",
        "            Explanation: {match.explanation}\n",
        "            \"\"\")\n",
        "        \n",
        "        # Print summary of results\n",
        "        logger.info(\"\\nClaude Analysis Summary:\")\n",
        "        logger.info(f\"Total source requirements processed: {claude_requirements_results_set.total_source_requirements}\")\n",
        "        logger.info(f\"Total target matches analyzed: {claude_requirements_results_set.total_target_matches}\")\n",
        "        logger.info(f\"Total associated matches found: {claude_requirements_results_set.total_associated_matches}\")\n",
        "\n",
        "        # Print breakdown by source requirement\n",
        "        logger.info(\"\\nBreakdown by source requirement:\")\n",
        "        source_counts = {}\n",
        "        associated_counts = {}\n",
        "\n",
        "        for match in claude_requirements_results_set.processed_matches:\n",
        "            source_counts[match.source_id] = source_counts.get(match.source_id, 0) + 1\n",
        "            if match.is_associated:\n",
        "                associated_counts[match.source_id] = associated_counts.get(match.source_id, 0) + 1\n",
        "\n",
        "        for source_id, count in source_counts.items():\n",
        "            associated = associated_counts.get(source_id, 0)\n",
        "            logger.info(f\"Source {source_id}:\")\n",
        "            logger.info(f\"  - Total target matches: {count}\")\n",
        "            logger.info(f\"  - Associated matches: {associated}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error in Claude analysis: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.getenv(\"TEST_MODE\", \"false\").strip().lower() in [\"true\", \"1\", \"t\", \"yes\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CELL 6] - Create LLM Requirement Traces\n",
        "#if claude_requirements_results_set.processed_matches:\n",
        "    # Configuration\n",
        "#    sentence_transformer_model = model_name\n",
        "#    llm_model = claude_model  # Using the Claude model name from environment\n",
        "\n",
        "#    logger.info(\"Creating LLM requirement traces...\")\n",
        "#    logger.info(f\"Sentence Transformer Model: {sentence_transformer_model}\")\n",
        "#    logger.info(f\"LLM Model: {llm_model}\")\n",
        "\n",
        "    # Create traces\n",
        "#    success_count, failure_count = neo4j_client.create_llm_traces_from_results(\n",
        "#        results_set=claude_requirements_results_set,\n",
        "#        model_name=sentence_transformer_model,\n",
        "#        llm_model_name=llm_model\n",
        "#    )\n",
        "\n",
        "    # Log results\n",
        " #   logger.info(f\"Successfully created {success_count} LLM requirement traces\")\n",
        " #   if failure_count > 0:\n",
        " #       logger.warning(f\"Failed to create {failure_count} traces\")\n",
        "\n",
        "    # Verify creation\n",
        " #   verification_query = f\"\"\"\n",
        " #   MATCH ()-[r:LLM_REQUIREMENT_TRACE]->()\n",
        " #   WHERE r.llm_model_name = '{llm_model}'\n",
        " #   RETURN count(r) as count\n",
        "#    \"\"\"\n",
        "\n",
        " #   with neo4j_client.driver.session(database=neo4j_client.database) as session:\n",
        " #       result = session.run(verification_query)\n",
        "#        trace_count = result.single()[\"count\"]\n",
        "#        logger.info(f\"Total LLM requirement traces in database for {llm_model}: {trace_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CELL 7] - Statistical Analysis\n",
        "logger.info(\"Performing statistical analysis of results\")\n",
        "\n",
        "claude_model = os.getenv(\"CLAUDE_2_MODEL\")\n",
        "\n",
        "# Initialize analyzer if not already initialized\n",
        "analyzer = RequirementsAnalyzer(\n",
        "    neo4j_client=neo4j_client,\n",
        "    sentence_transformer_model_name=model_name,  # Updated variable name\n",
        "    llm_model_name=claude_model\n",
        ")\n",
        "\n",
        "# Calculate basic metrics\n",
        "metrics = analyzer.calculate_metrics(\n",
        "    llm_model_name=claude_model,\n",
        "    sentence_transformer_model_name=model_name\n",
        ")\n",
        "\n",
        "# Print detailed results\n",
        "logger.info(\"\\nDetailed Metrics:\")\n",
        "logger.info(f\"True Positives: {metrics['true_positives']}\")\n",
        "logger.info(f\"False Positives: {metrics['false_positives']}\")\n",
        "logger.info(f\"True Negatives: {metrics['true_negatives']}\")\n",
        "logger.info(f\"False Negatives: {metrics['false_negatives']}\")\n",
        "logger.info(f\"\\nPrecision: {metrics['precision']:.4f}\")\n",
        "logger.info(f\"Recall: {metrics['recall']:.4f}\")\n",
        "logger.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "logger.info(f\"Specificity: {metrics['specificity']:.4f}\")\n",
        "logger.info(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
        "logger.info(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "# Get confusion matrix\n",
        "conf_matrix = analyzer.get_confusion_matrix()\n",
        "logger.info(\"\\nConfusion Matrix:\")\n",
        "logger.info(\"[[TN, FP]\")\n",
        "logger.info(\" [FN, TP]]\")\n",
        "logger.info(f\"\\n{conf_matrix}\")\n",
        "\n",
        "# Get detailed classification information\n",
        "logger.info(\"\\nDetailed Classification Analysis:\")\n",
        "classification_details = analyzer.get_all_classification_details()\n",
        "\n",
        "# True Positives Analysis\n",
        "tp_details = pd.DataFrame(classification_details['true_positives'])\n",
        "if not tp_details.empty:\n",
        "    logger.info(\"\\nTrue Positives Details:\")\n",
        "    logger.info(f\"Total Count: {len(tp_details)}\")\n",
        "    logger.info(\"\\nSample of True Positive matches:\")\n",
        "    logger.info(tp_details.head().to_string())\n",
        "    logger.info(f\"\\nAverage LLM Confidence: {tp_details['llm_confidence'].mean():.4f}\")\n",
        "    logger.info(f\"Average Ground Truth Confidence: {tp_details['ground_truth_confidence'].mean():.4f}\")\n",
        "\n",
        "# False Positives Analysis\n",
        "fp_details = pd.DataFrame(classification_details['false_positives'])\n",
        "if not fp_details.empty:\n",
        "    logger.info(\"\\nFalse Positives Details:\")\n",
        "    logger.info(f\"Total Count: {len(fp_details)}\")\n",
        "    logger.info(\"\\nSample of False Positive matches:\")\n",
        "    logger.info(fp_details.head().to_string())\n",
        "    logger.info(f\"\\nAverage LLM Confidence: {fp_details['llm_confidence'].mean():.4f}\")\n",
        "\n",
        "# False Negatives Analysis\n",
        "fn_details = pd.DataFrame(classification_details['false_negatives'])\n",
        "if not fn_details.empty:\n",
        "    logger.info(\"\\nFalse Negatives Details:\")\n",
        "    logger.info(f\"Total Count: {len(fn_details)}\")\n",
        "    logger.info(\"\\nSample of False Negative matches:\")\n",
        "    logger.info(fn_details.head().to_string())\n",
        "    logger.info(f\"\\nAverage Ground Truth Confidence: {fn_details['ground_truth_confidence'].mean():.4f}\")\n",
        "\n",
        "# True Negatives Analysis\n",
        "tn_details = pd.DataFrame(classification_details['true_negatives'])\n",
        "if not tn_details.empty:\n",
        "    logger.info(\"\\nTrue Negatives Details:\")\n",
        "    logger.info(f\"Total Count: {len(tn_details)}\")\n",
        "    logger.info(\"\\nSample of True Negative pairs:\")\n",
        "    logger.info(tn_details.head().to_string())\n",
        "\n",
        "# Save detailed results to file\n",
        "results_dir = Path(\"results/analysis\")\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_file = results_dir / f\"detailed_analysis_{timestamp}.json\"\n",
        "\n",
        "detailed_results = {\n",
        "    'metrics': metrics,\n",
        "    'confusion_matrix': conf_matrix.tolist(),\n",
        "    'classification_details': classification_details\n",
        "}\n",
        "\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(detailed_results, f, indent=2)\n",
        "\n",
        "logger.info(f\"\\nDetailed analysis results saved to {results_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CELL 8] - Visualize Analysis Results\n",
        "\n",
        "# Initialize visualizer with model information\n",
        "visualizer = RequirementsVisualizer(\n",
        "    figsize=(8, 6),  # Reduced figure size\n",
        "    st_model=model_name,\n",
        "    llm_model=claude_model\n",
        ")\n",
        "\n",
        "# Get current timestamp for filenames\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Create model-specific results directory\n",
        "st_name = model_name.replace('/', '_').replace('-', '_')\n",
        "llm_name = claude_model.replace('-', '_')\n",
        "results_dir = Path(f\"results/analysis_plots/{llm_name}/{st_name}\")\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Get metrics for Claude model\n",
        "metrics = analyzer.calculate_metrics()\n",
        "\n",
        "# Plot and display confusion matrix\n",
        "plt.figure(figsize=(8, 6))  # Controlled figure size\n",
        "conf_matrix = analyzer.get_confusion_matrix()\n",
        "visualizer.plot_confusion_matrix(\n",
        "    conf_matrix,\n",
        "    title=\"Model Confusion Matrix\",\n",
        "    save_path=str(results_dir / f\"confusion_{timestamp}.png\"),\n",
        "    dpi=100\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# Create and display metrics summary table\n",
        "metrics_to_display = [\n",
        "    'precision', 'recall', 'f1_score', \n",
        "    'accuracy', 'balanced_accuracy'\n",
        "]\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': metrics_to_display,\n",
        "    'Value': [f\"{metrics[metric]:.4f}\" for metric in metrics_to_display]\n",
        "})\n",
        "\n",
        "# Set pandas display options for better formatting\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Display the metrics table\n",
        "display(metrics_df)\n",
        "\n",
        "# Log paths to saved plots\n",
        "logger.info(f\"\\nPlots saved to:\")\n",
        "logger.info(f\"- Confusion Matrix: {results_dir}/confusion_{timestamp}.png\")\n",
        "\n",
        "# Close any remaining matplotlib figures\n",
        "plt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [CELL 9] - Cleanup\n",
        "neo4j_client.close()\n",
        "logger.info(\"Analysis complete\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}